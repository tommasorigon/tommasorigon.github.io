[
  {
    "objectID": "bio.html",
    "href": "bio.html",
    "title": "Tommaso Rigon",
    "section": "",
    "text": "University of Milano-Bicocca, Department of Economics, Management and Statistics (DEMS), Milan, Italy.\n\nSenior Assistant Professor (RTD-B), 2023 - Present\nJunior Assistant Professor (RTD-A), 2020 - 2023\n\n\nDuke University, Department of Statistical Science, Durham (NC), U.S.A.\n\nPostdoctoral Associate, 2020 - 2020\nResearch Associate, 2019 - 2020\n\nI worked with prof. Amy Herring and prof. David Dunson. My research focused on Bayesian methods for robust clustering, dimensionality reduction, and sequential species discovery.\nCollegio Carlo Alberto, Fondazione “de Castro” and Collegio Carlo Alberto, Turin, Italy.\n\nResearch Affiliate, 2017 - 2020\n\nI have been a Research affiliate at the Statistics initiative."
  },
  {
    "objectID": "bio.html#academic-positions",
    "href": "bio.html#academic-positions",
    "title": "Tommaso Rigon",
    "section": "",
    "text": "University of Milano-Bicocca, Department of Economics, Management and Statistics (DEMS), Milan, Italy.\n\nSenior Assistant Professor (RTD-B), 2023 - Present\nJunior Assistant Professor (RTD-A), 2020 - 2023\n\n\nDuke University, Department of Statistical Science, Durham (NC), U.S.A.\n\nPostdoctoral Associate, 2020 - 2020\nResearch Associate, 2019 - 2020\n\nI worked with prof. Amy Herring and prof. David Dunson. My research focused on Bayesian methods for robust clustering, dimensionality reduction, and sequential species discovery.\nCollegio Carlo Alberto, Fondazione “de Castro” and Collegio Carlo Alberto, Turin, Italy.\n\nResearch Affiliate, 2017 - 2020\n\nI have been a Research affiliate at the Statistics initiative."
  },
  {
    "objectID": "bio.html#education",
    "href": "bio.html#education",
    "title": "Tommaso Rigon",
    "section": "Education",
    "text": "Education\nPh.D. in Statistical Sciences, Bocconi University, Milan, Italy.\n\nThesis title: Finite-dimensional nonparametric priors: theory and applications.\nI worked under the joint supervision of prof. Antonio Lijoi and prof. Igor Prünster.\n\nPeriod 2015–2020. Ph.D. awarded with honors.\nM.Sc. in Statistical Sciences, University of Padova, Padua, Italy.\n\nThesis title: Functional telecommunication data: a Bayesian nonparametric approach.\nAdvisor: Bruno Scarpa.\n\nPeriod 2013 - 2015. Final mark: 110/110 with laude.\nB.Sc. in Statistics, Economics & Finance, University of Padova, Padua, Italy.\n\nThesis title: Box-Cox transformation: an analysis based on the likelihood.\nAdvisor: Nicola Sartori.\n\nPeriod 2010–2013. Final mark: 110/110 with laude."
  },
  {
    "objectID": "index.html#about-me",
    "href": "index.html#about-me",
    "title": "Tommaso Rigon",
    "section": "About me",
    "text": "About me\nI am an Assistant Professor of Statistical Science (RTD-B, SECS-S/01) at the department of Economics, Management and Statistics (DEMS) of University of Milano-Bicocca. I currently live in Milan, Italy.\nI am a member of the BayesLab at the Bocconi Institute for Data Science and Analytics (BIDSA), and the MIDAS Complex Data Modeling Research Network.\nMy curriculum vitae is available here. A complete list of publications is available here.\nResearch interests: applied Bayesian modeling; Bayesian clustering; Bayesian nonparametrics; computational statistics; generalized Bayes; functional data analysis; mixture models; species sampling models.\nE-mail: tommaso.rigon@unimib.it"
  },
  {
    "objectID": "index.html#news-and-awards",
    "href": "index.html#news-and-awards",
    "title": "Tommaso Rigon",
    "section": "News and awards",
    "text": "News and awards\n[2025] Bnews Bicocca (ITA). Statistica, tripletta di premi internazionali per il ricercatore di Bicocca Tommaso Rigon\n[2024] Blackwell and Rosenbluth Award 2024. The award aims at recognizing outstanding junior Bayesian researchers based on their overall contribution to the field and to the community. Junior Section of International Society for Bayesian Analysis (j-ISBA). [Award] [News]\n\n[2024] Mitchell Prize 2023 for the paper “Extended Stochastic Block Models with Application to Criminal Networks”. International Society for Bayesian Analysis (ISBA). [Award]\n\n\n[2024] Daniele Durante was awarded an ERC Starting Grant for the project NEMESIS, focusing on the sociogenesis of criminal networks. I am very happy to be involved in the research group!"
  },
  {
    "objectID": "papers.html",
    "href": "papers.html",
    "title": "Tommaso Rigon",
    "section": "",
    "text": "Preprints\n\nGhilotti, L., Camerlenghi, F., Rigon, T., and Guindani, M. (2026+). Bayesian nonparametric modeling of multivariate count data with an unknown number of traits. Submitted. [ArXiv]\nAgnoletto, D., Rigon, T., and Dunson D.B. (2026+). Nonparametric predictive inference for discrete data via Metropolis-adjusted Dirichlet sequences. Submitted. [ArXiv].\nRigon, T., Hsu, C., and Dunson D.B. (2026+). A Bayesian theory for estimation of biodiversity. Submitted. [ArXiv].\nAnceschi, N., Castiglione, C., Rigon, T., Zanella, G., and Durante D. (2026+). Optimal and computationally tractable lower bounds for logistic log-likelihoods. Submitted. [ArXiv].\n\nArticles in refereed journals\n\nGhilotti, L., Camerlenghi, F., and Rigon, T. (2026+). Bayesian analysis of product feature allocation models. Journal of the Royal Statistical Society Series B: Statistical Methodology, to appear. [Link] [Github repository].\nZito, A., Rigon, T., and Dunson, D. B. (2026+). Bayesian nonparametric modeling of latent partitions via Stirling-gamma priors. Bayesian Analysis, to appear. [Link].\nAgnoletto, D., Rigon, T., and Dunson D.B. (2025). Bayesian inference for generalized linear models via quasi-posteriors. Biometrika, 112(2). [Link] [Slides].\nRigon, T., Scarpa, B. and Petrone, S. (2025). Enriched Pitman-Yor processes. Scandinavian Journal of Statistics, 52(2), 631-657. [Link].\nLijoi, A., Prünster, I. and Rigon, T. (2024). Finite-dimensional discrete random structures and Bayesian clustering. Journal of the American Statistical Association (T&M), 119(546), 929–941. [Link].\nCatalano, M., Lijoi, A., Prünster, I. and Rigon, T. (2023). Bayesian modeling via discrete nonparametric priors. Japanese Journal of Statistics and Data Science, 6, 607–624 [Link].\nRigon, T. (2023). An enriched mixture model for functional clustering. Applied Stochastic Models in Business and Industry, 39, 232–250 [Link].\nRigon, T. and Aliverti E. (2023) Conjugate priors and bias reduction for logistic regression models. Statistics and Probability Letters, 202, 109901. [Link] [Github repository].\nRigon, T., Herring, A. H. and Dunson, D. B. (2023). A generalized Bayes framework for probabilistic clustering. Biometrika, 110(3), 559–578. [Link] [Github repository].\nZito, A., Rigon, T. and Dunson, D. B. (2023). Inferring taxonomic placement from DNA barcoding aiding in discovery of new taxa. Methods in Ecology and Evolution, 14, 529–542 [Link] [GitHub Repository].\nZito, A., Rigon, T., Ovaskainen, O. and Dunson, D. B. (2023). Bayesian modelling of sequential discoveries. Journal of the American Statistical Association (T&M), 118(544), 2521–2532. [Link] [GitHub Repository].\nLegramanti, S., Rigon, T., Durante, D. and Dunson D. B. (2022). Extended stochastic block models with application to criminal networks. Annals of Applied Statistics 16(4), 2369–2395. [Link] [GitHub repository].\nReverberi, C., Rigon, T., Solari, A., Hassan, C., Cherubini, P., GI Genius CADx Study Group and A. Cherubini (2022). Experimental evidence of effective human-AI collaboration in medical decision‐making. Scientific Reports, 12(14952) [Link].\nFavaro, S., Panero, F. and Rigon, T. (2021). Bayesian nonparametric disclosure risk assessment. Electronic Journal of Statistics 15(2), 5626–5651. [Link].\nRigon, T. and Durante, D., (2021), Tractable Bayesian density regression via logit stick-breaking priors. Journal of Statistical Planning and inference 211, 131–142. [Link] [GitHub Repository].\nLegramanti, S., Rigon, T. and Durante, D. (2020). Bayesian testing for exogenous partition structures in stochastic block models. Sankhya A: The Indian Journal of Statistics, 84, 108–126. [Link] [GitHub repository].\nLijoi, A., Prünster, I. and Rigon, T. (2020). Sampling hierarchies of discrete random structures. Statistics and Computing 30, 1591–1607. [Link].\nLijoi, A., Prünster, I. and Rigon, T. (2020). The Pitman–Yor multinomial process for mixture modeling. Biometrika 107(4), 891–906. [Link].\nDurante, D. and Rigon, T. (2019). Conditionally conjugate mean-field variational Bayes for logistic models. Statistical Science 34(3), 472–485. [Link] [GitHub Repository].\nDurante, D., Canale, A. and Rigon, T. (2019). A nested expectation-maximization algorithm for latent class models with covariates. Statistics and Probability Letters 146, 97–103. [Link] [GitHub Repository].\nRigon, T., Durante, D. and Torelli, N. (2019). Bayesian semiparametric modelling of contraceptive behavior in India via sequential logistic regressions. Journal of the Royal Statistical Society Series A: Statistics in Society 182(1), 225–247. [Link] [GitHub Repository].\n\nPublications in monographs, volumes, and discussions\n\nAgnoletto, D., Rigon, T. and Dunson, D.B. (2025). Bayesian inference for generalized linear models via quasi-posteriors: an application to Eurasian Chaffinch abundance in Finland. In Methodological and Applied Statistics and Demography III (Pollice, A. and Mariani, P., editors). Springer. [Link]\nRigon, T., Aliverti, E., Russo, M., and Scarpa, B. (2021). A discussion on: “Centered partition processes: Informative priors for clustering” Paganin, S., Herring, A. H., Olshan, A. F., Dunson, D. B., et al. (2021) in Bayesian Analysis 16(1) 301–370. [Link].\nAliverti, E., Paganin, S., Rigon, T. and Russo, M. (2019). A discussion on: “Latent nested nonparametric priors” by Camerlenghi, F., Dunson, D.B., Lijoi, A., Prünster, I. and Rodriguez, A. in Bayesian Analysis 14(4), 1303–1356. [Link].\nCaponera, A., Denti, F., Rigon, T., Sottosanti, A. and Gelfand, A. (2018). Hierarchical Spatio-Temporal Modeling of Resting State fMRI Data. In Studies in Neural Data Science (Canale, A., Durante, D., Paci, L. And Scarpa, B., editors). Springer. [Link] [GitHub Repository].\n\nNational conference proceedings\n\nAgnoletto, D., Rigon, T. and Scarpa, B. (2023). Bayesian density estimation for modeling age-at-death distribution. In Book of Short Papers of the Italian Statistical Society (Chelli, F. M., Ciommi, M., Ingrassia, S., Mariani, F., Recchioni, M.C.) 2023. ISBN: 9788891935618. [Link].\nCogo, R., Camerlenghi, F. and Rigon, T. (2023). Hierarchical processes in survival analysis. In Book of Short Papers of the Italian Statistical Society (Chelli, F. M., Ciommi, M., Ingrassia, S., Mariani, F., Recchioni, M.C.) 2023. ISBN: 9788891935618. [Link].\nPresicce, L., Rigon, T. and Aliverti, E. (2023). Bias-reduction methods for Poisson regression models. In Book of Short Papers of the Italian Statistical Society (Chelli, F. M., Ciommi, M., Ingrassia, S., Mariani, F., Recchioni, M.C.) 2023. ISBN: 9788891935618. [Link].\nLegramanti S., Rigon, T. and Durante, D. (2022). Bayesian clustering of brain regions via extended stochastic block models. In Book of Short Papers of the Italian Statistical Society (Balzanella, A., Bini, M., Cavicchia, C., Verde, R.) 2022. ISBN: 9788891932310. [Link].\nZito, A., Rigon, T. and Dunson, D. B. (2021). Modelling of accumulation curves through Weibull survival functions. In Book of Short Papers of the Italian Statistical Society 2021 (Perna, C., Salvati, N. and Schirripa Spagnolo, F., editors). ISBN: 9788891927361. [Link - Part 1] [Link - Part 2].\nRigon, T. (2018). Logit stick-breaking priors for partially exchangeable count data. In Book of Short Papers SIS 2018 (Abbruzzo, A., Piacentino, D., Chiodi, M. and Brentari, E., editors). ISBN: 9788891910233. [Link]."
  },
  {
    "objectID": "papers.html#papers",
    "href": "papers.html#papers",
    "title": "Tommaso Rigon",
    "section": "",
    "text": "Preprints\n\nGhilotti, L., Camerlenghi, F., Rigon, T., and Guindani, M. (2026+). Bayesian nonparametric modeling of multivariate count data with an unknown number of traits. Submitted. [ArXiv]\nAgnoletto, D., Rigon, T., and Dunson D.B. (2026+). Nonparametric predictive inference for discrete data via Metropolis-adjusted Dirichlet sequences. Submitted. [ArXiv].\nRigon, T., Hsu, C., and Dunson D.B. (2026+). A Bayesian theory for estimation of biodiversity. Submitted. [ArXiv].\nAnceschi, N., Castiglione, C., Rigon, T., Zanella, G., and Durante D. (2026+). Optimal and computationally tractable lower bounds for logistic log-likelihoods. Submitted. [ArXiv].\n\nArticles in refereed journals\n\nGhilotti, L., Camerlenghi, F., and Rigon, T. (2026+). Bayesian analysis of product feature allocation models. Journal of the Royal Statistical Society Series B: Statistical Methodology, to appear. [Link] [Github repository].\nZito, A., Rigon, T., and Dunson, D. B. (2026+). Bayesian nonparametric modeling of latent partitions via Stirling-gamma priors. Bayesian Analysis, to appear. [Link].\nAgnoletto, D., Rigon, T., and Dunson D.B. (2025). Bayesian inference for generalized linear models via quasi-posteriors. Biometrika, 112(2). [Link] [Slides].\nRigon, T., Scarpa, B. and Petrone, S. (2025). Enriched Pitman-Yor processes. Scandinavian Journal of Statistics, 52(2), 631-657. [Link].\nLijoi, A., Prünster, I. and Rigon, T. (2024). Finite-dimensional discrete random structures and Bayesian clustering. Journal of the American Statistical Association (T&M), 119(546), 929–941. [Link].\nCatalano, M., Lijoi, A., Prünster, I. and Rigon, T. (2023). Bayesian modeling via discrete nonparametric priors. Japanese Journal of Statistics and Data Science, 6, 607–624 [Link].\nRigon, T. (2023). An enriched mixture model for functional clustering. Applied Stochastic Models in Business and Industry, 39, 232–250 [Link].\nRigon, T. and Aliverti E. (2023) Conjugate priors and bias reduction for logistic regression models. Statistics and Probability Letters, 202, 109901. [Link] [Github repository].\nRigon, T., Herring, A. H. and Dunson, D. B. (2023). A generalized Bayes framework for probabilistic clustering. Biometrika, 110(3), 559–578. [Link] [Github repository].\nZito, A., Rigon, T. and Dunson, D. B. (2023). Inferring taxonomic placement from DNA barcoding aiding in discovery of new taxa. Methods in Ecology and Evolution, 14, 529–542 [Link] [GitHub Repository].\nZito, A., Rigon, T., Ovaskainen, O. and Dunson, D. B. (2023). Bayesian modelling of sequential discoveries. Journal of the American Statistical Association (T&M), 118(544), 2521–2532. [Link] [GitHub Repository].\nLegramanti, S., Rigon, T., Durante, D. and Dunson D. B. (2022). Extended stochastic block models with application to criminal networks. Annals of Applied Statistics 16(4), 2369–2395. [Link] [GitHub repository].\nReverberi, C., Rigon, T., Solari, A., Hassan, C., Cherubini, P., GI Genius CADx Study Group and A. Cherubini (2022). Experimental evidence of effective human-AI collaboration in medical decision‐making. Scientific Reports, 12(14952) [Link].\nFavaro, S., Panero, F. and Rigon, T. (2021). Bayesian nonparametric disclosure risk assessment. Electronic Journal of Statistics 15(2), 5626–5651. [Link].\nRigon, T. and Durante, D., (2021), Tractable Bayesian density regression via logit stick-breaking priors. Journal of Statistical Planning and inference 211, 131–142. [Link] [GitHub Repository].\nLegramanti, S., Rigon, T. and Durante, D. (2020). Bayesian testing for exogenous partition structures in stochastic block models. Sankhya A: The Indian Journal of Statistics, 84, 108–126. [Link] [GitHub repository].\nLijoi, A., Prünster, I. and Rigon, T. (2020). Sampling hierarchies of discrete random structures. Statistics and Computing 30, 1591–1607. [Link].\nLijoi, A., Prünster, I. and Rigon, T. (2020). The Pitman–Yor multinomial process for mixture modeling. Biometrika 107(4), 891–906. [Link].\nDurante, D. and Rigon, T. (2019). Conditionally conjugate mean-field variational Bayes for logistic models. Statistical Science 34(3), 472–485. [Link] [GitHub Repository].\nDurante, D., Canale, A. and Rigon, T. (2019). A nested expectation-maximization algorithm for latent class models with covariates. Statistics and Probability Letters 146, 97–103. [Link] [GitHub Repository].\nRigon, T., Durante, D. and Torelli, N. (2019). Bayesian semiparametric modelling of contraceptive behavior in India via sequential logistic regressions. Journal of the Royal Statistical Society Series A: Statistics in Society 182(1), 225–247. [Link] [GitHub Repository].\n\nPublications in monographs, volumes, and discussions\n\nAgnoletto, D., Rigon, T. and Dunson, D.B. (2025). Bayesian inference for generalized linear models via quasi-posteriors: an application to Eurasian Chaffinch abundance in Finland. In Methodological and Applied Statistics and Demography III (Pollice, A. and Mariani, P., editors). Springer. [Link]\nRigon, T., Aliverti, E., Russo, M., and Scarpa, B. (2021). A discussion on: “Centered partition processes: Informative priors for clustering” Paganin, S., Herring, A. H., Olshan, A. F., Dunson, D. B., et al. (2021) in Bayesian Analysis 16(1) 301–370. [Link].\nAliverti, E., Paganin, S., Rigon, T. and Russo, M. (2019). A discussion on: “Latent nested nonparametric priors” by Camerlenghi, F., Dunson, D.B., Lijoi, A., Prünster, I. and Rodriguez, A. in Bayesian Analysis 14(4), 1303–1356. [Link].\nCaponera, A., Denti, F., Rigon, T., Sottosanti, A. and Gelfand, A. (2018). Hierarchical Spatio-Temporal Modeling of Resting State fMRI Data. In Studies in Neural Data Science (Canale, A., Durante, D., Paci, L. And Scarpa, B., editors). Springer. [Link] [GitHub Repository].\n\nNational conference proceedings\n\nAgnoletto, D., Rigon, T. and Scarpa, B. (2023). Bayesian density estimation for modeling age-at-death distribution. In Book of Short Papers of the Italian Statistical Society (Chelli, F. M., Ciommi, M., Ingrassia, S., Mariani, F., Recchioni, M.C.) 2023. ISBN: 9788891935618. [Link].\nCogo, R., Camerlenghi, F. and Rigon, T. (2023). Hierarchical processes in survival analysis. In Book of Short Papers of the Italian Statistical Society (Chelli, F. M., Ciommi, M., Ingrassia, S., Mariani, F., Recchioni, M.C.) 2023. ISBN: 9788891935618. [Link].\nPresicce, L., Rigon, T. and Aliverti, E. (2023). Bias-reduction methods for Poisson regression models. In Book of Short Papers of the Italian Statistical Society (Chelli, F. M., Ciommi, M., Ingrassia, S., Mariani, F., Recchioni, M.C.) 2023. ISBN: 9788891935618. [Link].\nLegramanti S., Rigon, T. and Durante, D. (2022). Bayesian clustering of brain regions via extended stochastic block models. In Book of Short Papers of the Italian Statistical Society (Balzanella, A., Bini, M., Cavicchia, C., Verde, R.) 2022. ISBN: 9788891932310. [Link].\nZito, A., Rigon, T. and Dunson, D. B. (2021). Modelling of accumulation curves through Weibull survival functions. In Book of Short Papers of the Italian Statistical Society 2021 (Perna, C., Salvati, N. and Schirripa Spagnolo, F., editors). ISBN: 9788891927361. [Link - Part 1] [Link - Part 2].\nRigon, T. (2018). Logit stick-breaking priors for partially exchangeable count data. In Book of Short Papers SIS 2018 (Abbruzzo, A., Piacentino, D., Chiodi, M. and Brentari, E., editors). ISBN: 9788891910233. [Link]."
  },
  {
    "objectID": "post/champions/champions2025.html#la-classifica",
    "href": "post/champions/champions2025.html#la-classifica",
    "title": "Previsioni Champions League",
    "section": "La classifica",
    "text": "La classifica\nNella Champions League del 2025, 36 squadre competono in una prima fase con un formato “all’italiana”, disputando un totale di 8 partite ciascuna. Ad oggi, 29 gennaio 2024, sono state giocate 7 delle 8 giornate previste. Siamo quindi interessati a formulare una previsione sulla classifica finale. Alla settima giornata, la classifica della Champions League era la seguente.\n\n\n\n\n\n\nClassifica Champions League alla 7a giornata\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPosizione\nSquadra\nPunti\nPosizione\nSquadra\nPunti\n\n\n\n\n1\nLiverpool\n21\n19\nClub Brugge\n11\n\n\n2\nBarcellona\n18\n20\nPSV Eindhoven\n11\n\n\n3\nArsenal\n16\n21\nBenfica\n10\n\n\n4\nInter\n16\n22\nPSG\n10\n\n\n5\nAtletico Madrid\n15\n23\nSporting\n10\n\n\n6\nMilan\n15\n24\nStoccarda\n10\n\n\n7\nAtalanta\n14\n25\nDinamo Zagabria\n8\n\n\n8\nAston Villa\n13\n26\nManchester City\n8\n\n\n9\nBayer Leverkusen\n13\n27\nShaktar Donetsk\n7\n\n\n10\nBrest\n13\n28\nBologna\n5\n\n\n11\nFeyenoord\n13\n29\nSparta Praga\n4\n\n\n12\nLille\n13\n30\nGirona\n3\n\n\n13\nMonaco\n13\n31\nLipsia\n3\n\n\n14\nBayern Monaco\n12\n32\nSalisburgo\n3\n\n\n15\nBorussia Dortmund\n12\n33\nStella Rossa\n3\n\n\n16\nCeltic FC\n12\n34\nSturm\n3\n\n\n17\nJuventus\n12\n35\nSlovan Bratislava\n0\n\n\n18\nReal Madrid\n12\n36\nYoung Boys\n0\n\n\n\n\n\n\nDopo le prime 8 giornate, il regolamento della Champions League stabilisce quanto segue:\n\nLe prime 8 squadre accedono direttamente agli ottavi di finale;\nLe squadre classificate dalla 9a alla 24a posizione partecipano ai playoff;\nLe squadre dalla 25a posizione in poi vengono eliminate."
  },
  {
    "objectID": "post/champions/champions2025.html#quotazioni-delle-partite",
    "href": "post/champions/champions2025.html#quotazioni-delle-partite",
    "title": "Previsioni Champions League",
    "section": "Quotazioni delle partite",
    "text": "Quotazioni delle partite\nPer poter formulare delle previsioni, è innanzitutto necessario stimare in modo ragionevole l’esito di ciascuno dei 18 incontri dell’ultima giornata. Una possibilità consiste nell’utilizzare modelli avanzati, basati sullo storico delle squadre. Tuttavia, per semplicità, sia espositiva che computazionale, adotteremo qui un approccio molto più semplice, basato sulle quote rese pubbliche da alcuni bookmaker (facendo la media delle quote di alcuni tra i bookmaker più popolari).\nSiano q_1, q_X, q_2 le quote associate, rispettivamente, alla vittoria della squadra 1, al pareggio e alla vittoria della squadra 2. Le probabilità corrispondenti p_1, p_X, p_2 possono essere calcolate come segue:\n\np_1 = \\frac{1/q_1}{1/q_1 + 1/q_X + 1/q_2}, \\quad p_X = \\frac{1/q_X}{1/q_1 + 1/q_X + 1/q_2}, \\quad p_2 = \\frac{1/q_2}{1/q_1 + 1/q_X + 1/q_2},\n\nSi noti che per costruzione si ha che p_1 + p_X + p_2 = 1. Si noti inoltre che le scommesse non sono eque, ecco perché è necessario “normalizzare” le quote tramite il termine 1/q_1 + 1/q_X + 1/q_2. Infatti, se i bookmaker fossero equi, si avrebbe 1/q_1 + 1/q_X + 1/q_2 = 1.\nEcco il risultato di questa operazione, per i 18 incontri dell’ultima giornata."
  },
  {
    "objectID": "post/champions/champions2025.html#il-metodo-monte-carlo",
    "href": "post/champions/champions2025.html#il-metodo-monte-carlo",
    "title": "Previsioni Champions League",
    "section": "Il metodo Monte Carlo",
    "text": "Il metodo Monte Carlo\nA partire dalle probabilità di vittoria/sconfitta di ciascuna partita, è possibile simulare tramite Monte Carlo la classifica finale. In altri termini, simuliamo per 100,000 volte le 18 partite della 8a giornata e valutiamo la frequenza con cui un certo evento si è verificato.\n\n\n\n\n\n\nIn caso di parità di punteggio, il regolamento prevede che si guardi in prima battuta la differenza reti e poi ad ulteriori indicatori. Non avendo elementi per prevedere questi aspetti, la posizione in classifica delle squadre a pari merito viene assegnata casualmente.\n\n\n\nL’evento che più ci interessa è la probabilità che ha ciascuna squadra di finire agli ottavi, ai playoff o di essere eliminata."
  },
  {
    "objectID": "post/champions/champions2025.html#commento-ai-risultati-e-focus-su-squadre-italiane",
    "href": "post/champions/champions2025.html#commento-ai-risultati-e-focus-su-squadre-italiane",
    "title": "Previsioni Champions League",
    "section": "Commento ai risultati e focus su squadre italiane",
    "text": "Commento ai risultati e focus su squadre italiane\nSulle base dei risultati precedenti, possiamo fare alcune considerazioni.\n\nSalvo grosse sorprese, Inter dovrebbe andare direttamente agli ottavi di finale.\nAnche il Milan ha un’alta probabilità di andare direttamente agli ottavi, anche se meno schiacciante di quella dell’Inter.\nAtalanta è in bilico: il passaggio agli ottavi è dato soltanto al 24%, principalmente a causa dello scontro difficile contro il Barcelona, in cui quest’ultima è data per favorita.\nÈ davvero molto difficile che Juventus passi agli ottavi (2%), ma è anche impossibile che venga eliminata. Al 98% andrà ai playoff.\n\n\n\n\n\n\n\n\n\nPossiamo anche calcolare la distribuzione della posizione in classifica di Inter, Milan, Atalanta e Juventus, che è riportata qui di seguito. La variabilità è notevole, perchè le squadre hanno punteggi molto simili tra loro."
  },
  {
    "objectID": "post/karmic_dice/karmic.html",
    "href": "post/karmic_dice/karmic.html",
    "title": "Karmic dice for Baldur’s Gate III",
    "section": "",
    "text": "I played the game “Baldur’s Gate” when I was a kid, on my old-fashioned computer and I thought it was a great game. When Baldur’s Gate III was released a few weeks ago, it caught my attention.\nThe central dynamic of the game is based on Dungeons & Dragons, meaning that players need to roll the famous 20 sided dice several times. On a laptop/console, these “random” numbers (actually, pseudo-random) are automatically generated by the software.\nA specific feature of the game, the karmic dice, was very much discussed by the online community.\nThe karmic dice avoid subsequent successes and failures. In other terms, the results of the dice are not independent anymore.\nI am an Assistant Professor of Statistical Science, so I had a few questions: how does this algorithm work, exactly? Will it preserve basic properties that do not screw up the balance of the game?\nPerhaps more importantly: is it possible to find a simple way to generate “karmic” rolls?\n\n\nLet us step back for a second. Why do we need a karmic dice in the first place?\nMost people find “true randomness” (independent trials, in the language of probability) quite counter-intuitive.\nSuppose we toss a fair dice with 20 sides, say, 100 times. A compatible sequence of results could be:\n\n\n  [1] 15  5 11 14  1  8 13  6 17  2 13 15  7 12  7  8  9  5  3  8  8  6  2 20 15\n [26] 20 13  8 15 11 12  4  3 15  3 11 14 20 14 14 11  7 11  1 11 14 17  1  2  7\n [51] 11 13  1  1 17 18  4  2  2  9 13 11 11  6  2  8 17  2 11  4 11  6  1 15 11\n [76]  5  8 17  3 14  3 18 14  8 18 17  7 10  1 10 18 19 14 20 10 19  8 12  3 20\n\n\nA surprising amount of people would find this sequence “non-random” or would suspect that something is off with the random number generator (RNG). There is indeed a sequence of bad rolls: 7, 8, 9, 5, 3, 8, 8, 6, and 2. Is this some mistake?\nLong story short: nothing fishy is going on, this is the normal behavior of a regular dice. Yet, a non-trained eye would expect something like the following karmic sequence (dependent trials):\n\n\n  [1] 16 12  6 11  1  4 19  9  5 16  7 12 18  4 10 19  5 16  6 15  7  8 16 14 12\n [26] 13  8  2 20 15  7 20  5  8 14  2  9  5 15  7 20 14 12 13 14 16  1 17 20 12\n [51] 19 17 15  8 15  2 12 10  7 12 10  3  7  2 19 10  7  8 17 15 15 14  9 15 10\n [76] 15 10  3 10  4 14 19 19 16  7 15  4  5 13  5 11 12 19  8 14  9 20  8  9  3\n\n\nIn this sequence, there are fewer streaks of negative results, and many players find this kind of pattern more enjoyable.\n\n\n\nThere are many ways to obtain a sequence with fewer unlucky rolls. A simple idea could be using a weighted dice, e.g. an hypothetical dice with non-uniform probabilities.\nSuch a weighted dice accomplishes the goal of reducing extreme events, but it is modifying a fundamental aspect of Dungeons & Dragons and therefore affecting the balance of the game.\nChanging the rules is not necessarily bad, but it has consequences. Indeed, most role players are well aware that there is a difference between 1d20 and 2d10.\n\n\n\nI think a more gentle approach would be tweaking the algorithm in such a way:\n\nThe long-rung proportions are identical to those of a regular dice, i.e., each side of the dice has a 5\\% probability of being picked if we were to roll the dice a considerable amount of times.\nThe rolls are dependent, meaning the values obtained at the previous step influence future behavior.\n\nMoreover, the algorithm should be easy to implement and tunable to account for players’ preferences (e.g., weak vs strong karma effect)."
  },
  {
    "objectID": "post/karmic_dice/karmic.html#karmic-dice-what-do-they-do",
    "href": "post/karmic_dice/karmic.html#karmic-dice-what-do-they-do",
    "title": "Karmic dice for Baldur’s Gate III",
    "section": "",
    "text": "I played the game “Baldur’s Gate” when I was a kid, on my old-fashioned computer and I thought it was a great game. When Baldur’s Gate III was released a few weeks ago, it caught my attention.\nThe central dynamic of the game is based on Dungeons & Dragons, meaning that players need to roll the famous 20 sided dice several times. On a laptop/console, these “random” numbers (actually, pseudo-random) are automatically generated by the software.\nA specific feature of the game, the karmic dice, was very much discussed by the online community.\nThe karmic dice avoid subsequent successes and failures. In other terms, the results of the dice are not independent anymore.\nI am an Assistant Professor of Statistical Science, so I had a few questions: how does this algorithm work, exactly? Will it preserve basic properties that do not screw up the balance of the game?\nPerhaps more importantly: is it possible to find a simple way to generate “karmic” rolls?\n\n\nLet us step back for a second. Why do we need a karmic dice in the first place?\nMost people find “true randomness” (independent trials, in the language of probability) quite counter-intuitive.\nSuppose we toss a fair dice with 20 sides, say, 100 times. A compatible sequence of results could be:\n\n\n  [1] 15  5 11 14  1  8 13  6 17  2 13 15  7 12  7  8  9  5  3  8  8  6  2 20 15\n [26] 20 13  8 15 11 12  4  3 15  3 11 14 20 14 14 11  7 11  1 11 14 17  1  2  7\n [51] 11 13  1  1 17 18  4  2  2  9 13 11 11  6  2  8 17  2 11  4 11  6  1 15 11\n [76]  5  8 17  3 14  3 18 14  8 18 17  7 10  1 10 18 19 14 20 10 19  8 12  3 20\n\n\nA surprising amount of people would find this sequence “non-random” or would suspect that something is off with the random number generator (RNG). There is indeed a sequence of bad rolls: 7, 8, 9, 5, 3, 8, 8, 6, and 2. Is this some mistake?\nLong story short: nothing fishy is going on, this is the normal behavior of a regular dice. Yet, a non-trained eye would expect something like the following karmic sequence (dependent trials):\n\n\n  [1] 16 12  6 11  1  4 19  9  5 16  7 12 18  4 10 19  5 16  6 15  7  8 16 14 12\n [26] 13  8  2 20 15  7 20  5  8 14  2  9  5 15  7 20 14 12 13 14 16  1 17 20 12\n [51] 19 17 15  8 15  2 12 10  7 12 10  3  7  2 19 10  7  8 17 15 15 14  9 15 10\n [76] 15 10  3 10  4 14 19 19 16  7 15  4  5 13  5 11 12 19  8 14  9 20  8  9  3\n\n\nIn this sequence, there are fewer streaks of negative results, and many players find this kind of pattern more enjoyable.\n\n\n\nThere are many ways to obtain a sequence with fewer unlucky rolls. A simple idea could be using a weighted dice, e.g. an hypothetical dice with non-uniform probabilities.\nSuch a weighted dice accomplishes the goal of reducing extreme events, but it is modifying a fundamental aspect of Dungeons & Dragons and therefore affecting the balance of the game.\nChanging the rules is not necessarily bad, but it has consequences. Indeed, most role players are well aware that there is a difference between 1d20 and 2d10.\n\n\n\nI think a more gentle approach would be tweaking the algorithm in such a way:\n\nThe long-rung proportions are identical to those of a regular dice, i.e., each side of the dice has a 5\\% probability of being picked if we were to roll the dice a considerable amount of times.\nThe rolls are dependent, meaning the values obtained at the previous step influence future behavior.\n\nMoreover, the algorithm should be easy to implement and tunable to account for players’ preferences (e.g., weak vs strong karma effect)."
  },
  {
    "objectID": "post/karmic_dice/karmic.html#a-simple-algorithm-for-the-karmic-dice",
    "href": "post/karmic_dice/karmic.html#a-simple-algorithm-for-the-karmic-dice",
    "title": "Karmic dice for Baldur’s Gate III",
    "section": "A simple algorithm for the karmic dice",
    "text": "A simple algorithm for the karmic dice\nI do not know which approach is implemented in Baldur’s Gate III, I can only make educated guesses. However, it turns out that a simple idea, based on the notion of latent karma, has all the above desiderata.\nLet us focus on the generic tth roll. The latent karma Y_t of the is a real number that identifies how lucky we have been, say the positive values 1.5 or 3.2 (lucky events) or the negative value -4.3 (unlucky event).\nLet 0 &lt; \\kappa &lt; 1 be a constant determining the “karma effect.” The first karma score Y_1 does not depend on the past (because there is no past), and we generate it according to a Gaussian random variable:\n\nY_1 \\sim \\text{N}\\left(0, \\frac{1}{1 - \\kappa^2}\\right).\n\nThe subsequent karma scores are obtained by adjusting the previous karma score as follows:\n\nY_t = - \\kappa \\:Y_{t-1} + \\epsilon_t, \\qquad \\epsilon_t \\sim \\text{N}(0, 1),\n for t = 2,\\dots,n. In other words, if we were unlucky at step t, we had better chances to be lucky at step t+1 (and vice-versa). The amount of this effect is regulated by the parameter \\kappa.\nIn R / python code, the generation of the latent karma scores is straightforward:\n\nR codePython code\n\n\n\nkarmic_score &lt;- function(n, kappa) {\n  sigma &lt;- sqrt(1 / (1 - kappa^2))\n  latent_score &lt;- numeric(n)\n  \n  # First latent karma Y_1\n  latent_score[1] &lt;- rnorm(1, 0, sigma)\n  \n  # Generation of the latent karma values Y_2,...,Y_n\n  for (t in 1:(n - 1)) {\n    latent_score[t + 1] &lt;- -kappa * latent_score[t] + rnorm(1, 0, 1)\n  }\n  \n  return(latent_score)\n}\n\n\n\n\nimport numpy as np\n\ndef karmic_score(n, kappa):\n    sigma = np.sqrt(1 / (1 - kappa**2))\n    latent_score = np.zeros(n)\n    \n    # First latent karma Y_1\n    latent_score[0] = np.random.normal(0, sigma)\n    \n    # Generation of the latent karma values Y_2,...,Y_n\n    for t in range(1, n):\n        latent_score[t] = -kappa * latent_score[t - 1] + np.random.normal(0, 1)\n    \n    return latent_score\n\n\n\n\nOnce we have obtained the karma scores Y_t, we simply convert them into integers D_t, belonging to 1,\\dots,20, using the following formula\n\nD_t = \\text{ceiling}\\{20 \\Phi_\\kappa(Y_t)\\}, \\qquad t = 1,\\dots,n,\n where \\Phi_\\kappa(y) is the cumulative distribution function of a Gaussian with zero mean and variance 1 / (1 - \\kappa^2).\nThe \\text{ceiling} operation rounds up its argument, producing an integer. Once again, this operation can be performed with a few lines of R / Python code.\n\nR codePython code\n\n\n\nkarmic_dice &lt;- function(n, kappa) {\n  sigma &lt;- sqrt(1 / (1 - kappa^2))\n  \n  # Generate the latent scores\n  latent_score &lt;- karmic_score(n, kappa)\n  # Convert the latent scores into integers between 1 and 20\n  dice_results &lt;- ceiling(20 * pnorm(latent_score, 0, sigma))\n  return(dice_results)\n}\n\n\n\n\nfrom scipy.stats import norm\n\ndef karmic_dice(n, kappa):\n    sigma = np.sqrt(1 / (1 - kappa**2))\n    \n    # Generate the latent scores\n    latent_score = karmic_score(n, kappa)\n    # Convert the latent scores into integers between 1 and 20\n    dice_results = np.ceil(20 * norm.cdf(latent_score, loc=0, scale=sigma))\n    return dice_results\n\n\n\n\nThe usage of this function is also straightforward. In R, we can get a bunch of values for the latent score and the dice as follows:\n\nkarmic_dice(n = 30, kappa = 0.35)\n\n [1] 17 17  1 11 19  1  8  2  7  6 17  1  9  9  7  2  5 14  8 10 11  5 15  4 13\n[26]  5 17 18 17 20"
  },
  {
    "objectID": "post/karmic_dice/karmic.html#analyzing-the-algorithm",
    "href": "post/karmic_dice/karmic.html#analyzing-the-algorithm",
    "title": "Karmic dice for Baldur’s Gate III",
    "section": "Analyzing the algorithm",
    "text": "Analyzing the algorithm\nThe most interesting property of this karmic dice is that it preserves the long-run proportions. If you have a solid background on the theory of auto-regressive processes and the so-called inversion theorem for generating random variables, this should be immediately obvious to you.\nIf you are not familiar with probability theory, the following empirical demonstration should give you an intuition of why this idea works nicely.\nSuppose I were to get one million rolls, then I could check how many times I got each of the values 1,2,\\dots, 20. With a computer, this can be quickly done (using \\kappa = 0.35), and these are the proportions we get:\n\n\n\n       1        2        3        4        5        6        7        8 \n0.049854 0.049871 0.049799 0.049926 0.050014 0.050330 0.050038 0.050022 \n       9       10       11       12       13       14       15       16 \n0.049772 0.049845 0.050407 0.050344 0.049760 0.050350 0.049981 0.049673 \n      17       18       19       20 \n0.050130 0.049961 0.049937 0.049986 \n\n\nEach number is roughly appearing in the sequence 5\\% of the times, as it should!\nThe take-home message is that this algorithm does not affect the overall balance of the game because the long-term behavior (called stationary distribution) coincides with that of the regular dice.\n\nThe karmic effect\nAnother aspect we would like to understand is the dependence between the current roll and the following. These two rolls are independent in the case of a regular dice, but the situation is quite different in a karmic dice.\nLet us start noticing that when \\kappa = 0, the above karmic dice algorithm is just a convoluted way of sampling from a regular dice! It can be “easily” proved using standard probability tools.\nWhen \\kappa &gt; 0, the karmic dice tweaks the probability and induces a negative correlation between subsequent rolls. Let us visualize what happens when \\kappa = 0.35.\n\n\n\n\n\n\n\n\n\nBlue squares are more likely values (compared to the regular dice), whereas orange squares are less likely values. This graph reveals a few interesting facts:\n\nIf the current roll is a 1, then the most likely outcome of the following will be in the range of 15-20, with a peak in 20.\nIf the current roll is 20, then the most likely outcome of the following will be in the range of 1-5, with a peak in 1.\nIf the current roll is either 10 or 11, the following roll will roughly behave like a regular dice.\n\nThis version of the karmic effect therefore compensate positive values with negative values in a balanced manner so that the long-run proportions are correct.\n\n\nThe choice of \\kappa\nThe amount of compensation is regulated by \\kappa, a crucial tuning parameter that can be used to tune the dynamic of the karma dice.\nIf \\kappa is close to 0, we get a regular dice. Vice versa, if \\kappa is close to 1, i.e., the theoretical maximum, we would get sequences like this:\n\nkarmic_dice(n = 100, kappa = 0.999)\n\n  [1]  7 13  7 14  7 14  7 14  7 13  8 13  8 13  7 13  8 13  8 13  8 13  7 14  7\n [26] 14  7 14  7 14  7 14  8 13  7 14  7 14  8 14  8 13  8 13  8 13  8 13  9 13\n [51]  8 13  8 13  8 12  8 12  9 12 10 11 10 11 10 11 10 11 10 11  9 12  9 12  9\n [76] 12  9 12 10 11 10 12 10 11 10 11 10 10 10 11 10 11 10 11 10 11 11 10 12 10\n\n\nEven though, in the (very) long run, the correct proportions are still preserved for any \\kappa &lt; 1, such an extreme choice of \\kappa = 0.999 leads to almost deterministic compensations, which are not appropriate for a game, because it would become too predictable.\nThere is no “statistical” optimal choice for \\kappa, which instead should be selected based on the player preferences. Here I picked \\kappa = 0.35 as it seems like a reasonable default, being a middle-ground solution between the independence case (regular dice) and an almost determinist pattern."
  },
  {
    "objectID": "post/lotteria_AI/lotteria_truccata.html",
    "href": "post/lotteria_AI/lotteria_truccata.html",
    "title": "La lotteria nazionale non è difettosa",
    "section": "",
    "text": "Dopo la pubblicazione di un mio post di qualche giorno fa riguardante la lotteria nazionale, alcuni commentatori social (si, forse dovrei smettere di leggerli) hanno messo in discussione la validità delle ipotesi su cui si regge le lotteria, ed in particolare l’equiprobabilità dei numeri del lotto. In altri termini, secondo loro, alcuni numeri avrebbero maggiore probabilità di essere estratti rispetto ad altri.\nSe ciò fosse vero, significherebbe che i dipendenti della lotteria hanno per anni commesso un grave errore, fatto che probabilmente avrebbero ripercussioni legali importanti. In buona sostanza, l’ipotesi alternativa di alcuni commentatori è che il macchinario con cui vengono estratti i numeri sia “difettoso” e che questo favorisca alcuni numeri rispetto ad altri.\nI numeri del lotto vengono estratti tramite un macchinario che utilizza palline numerate. Il video seguente mostra come avviene l’estrazione dei numeri (estrazione del 6 Marzo 2025).\n\n\n\n\n\n\n\n\n\n\n\n\n\nSebbene l’onere della prova dovrebbe spettare a chi muove questa accusa, ho deciso ugualmente di prendere le parti della difesa e di analizzare i dati a disposizione.\nDopo aver studiato i dati della lotteria nazionale degli ultimi 20 anni, non ho registrato alcun tipo di anomalia statistica. In altri termini, non c’è evidenza di alcun macchinario difettoso."
  },
  {
    "objectID": "post/lotteria_AI/lotteria_truccata.html#unanalisi-delle-estrazioni-della-lotteria-nazionale",
    "href": "post/lotteria_AI/lotteria_truccata.html#unanalisi-delle-estrazioni-della-lotteria-nazionale",
    "title": "La lotteria nazionale non è difettosa",
    "section": "",
    "text": "Dopo la pubblicazione di un mio post di qualche giorno fa riguardante la lotteria nazionale, alcuni commentatori social (si, forse dovrei smettere di leggerli) hanno messo in discussione la validità delle ipotesi su cui si regge le lotteria, ed in particolare l’equiprobabilità dei numeri del lotto. In altri termini, secondo loro, alcuni numeri avrebbero maggiore probabilità di essere estratti rispetto ad altri.\nSe ciò fosse vero, significherebbe che i dipendenti della lotteria hanno per anni commesso un grave errore, fatto che probabilmente avrebbero ripercussioni legali importanti. In buona sostanza, l’ipotesi alternativa di alcuni commentatori è che il macchinario con cui vengono estratti i numeri sia “difettoso” e che questo favorisca alcuni numeri rispetto ad altri.\nI numeri del lotto vengono estratti tramite un macchinario che utilizza palline numerate. Il video seguente mostra come avviene l’estrazione dei numeri (estrazione del 6 Marzo 2025).\n\n\n\n\n\n\n\n\n\n\n\n\n\nSebbene l’onere della prova dovrebbe spettare a chi muove questa accusa, ho deciso ugualmente di prendere le parti della difesa e di analizzare i dati a disposizione.\nDopo aver studiato i dati della lotteria nazionale degli ultimi 20 anni, non ho registrato alcun tipo di anomalia statistica. In altri termini, non c’è evidenza di alcun macchinario difettoso."
  },
  {
    "objectID": "post/lotteria_AI/lotteria_truccata.html#un-problema-storico",
    "href": "post/lotteria_AI/lotteria_truccata.html#un-problema-storico",
    "title": "La lotteria nazionale non è difettosa",
    "section": "Un problema storico",
    "text": "Un problema storico\nIl problema dell’equità di una lotteria ha una lunga e importante storia, che è elegantemente raccontata nell’articolo di storia della statistica di Stigler (2003)1:\n\nStigler S. M. (2003), Casanova, «Bonaparte», and the loterie de France, Journal de la société française de statistique, 144(1-2), 5–34.\n\nGli strumenti statistici che qui presenteremo sono simili a quelli usati da Stigler per analizzare la lotteria francese del 18esimo secolo. L’articolo di Stigler è molto più completo e dettagliato di questo post, e ne consiglio caldamente la lettura. Anzitutto, è una testimonianza importante di quanta intelligenza sia già stata dedicata alla questione della lotteria, oltre che esempio di come la statistica possa risolvere problemi concreti."
  },
  {
    "objectID": "post/lotteria_AI/lotteria_truccata.html#i-dati-della-lotteria-nazionale",
    "href": "post/lotteria_AI/lotteria_truccata.html#i-dati-della-lotteria-nazionale",
    "title": "La lotteria nazionale non è difettosa",
    "section": "I dati della lotteria nazionale",
    "text": "I dati della lotteria nazionale\nI dati delle estrazioni del Lotto sono a disposizione sul sito ufficiale del Lotto a partire dalle estrazioni del 1939. La lotteria nazionale inizia invece dal 5 Maggio 2005, cioè la data della prima estrazione nazionale. Questa analisi è replicabile da chiunque avesse interesse a farlo.\nPer dare un po’ di contesto, i 3 studenti dell’Università del Salento hanno scommesso sui numeri 21 e 48 della lotteria nazionale in data 1 Marzo 2025, vincendo, come si può vedere nella fotografia allegata.\n\n\n\nImmagine tratta dall’articolo originale di Repubblica.\n\n\nA titolo di esempio, le ultime cinque estrazioni sono state:\n\n\n\n\n\nData\nN1\nN2\nN3\nN4\nN5\n\n\n\n\n2025-02-28\n14\n78\n18\n40\n8\n\n\n2025-03-01\n21\n48\n3\n17\n62\n\n\n2025-03-04\n57\n54\n79\n44\n40\n\n\n2025-03-06\n34\n83\n53\n81\n14\n\n\n2025-03-07\n45\n81\n88\n30\n28\n\n\n\n\n\nA detta stessa dei 3 studenti e di alcuni commentatori dei social, ci sarebbero delle anomalie nelle frequenze dei numeri del lotto. Ho calcolato le frequenze di ciascun numero, per verificare questa affermazione e non ho riscontrato anomalie. Per semplicità, qui di seguito riporto le frequenze del primo numero estratto (N1); si veda la fine di questo post per un approccio leggermente più sofisticato, che tenga conto di tutti i numeri.\n\n\n\n\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n\n\n\n\n25\n43\n32\n41\n32\n40\n42\n30\n31\n42\n31\n35\n36\n34\n51\n\n\n\n\n\n\n\n\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n\n\n\n\n41\n36\n28\n25\n33\n32\n37\n41\n23\n33\n29\n40\n39\n39\n31\n\n\n\n\n\n\n\n\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n\n\n\n\n43\n28\n31\n40\n32\n26\n30\n26\n32\n41\n30\n41\n32\n34\n37\n\n\n\n\n\n\n\n\n46\n47\n48\n49\n50\n51\n52\n53\n54\n55\n56\n57\n58\n59\n60\n\n\n\n\n42\n40\n34\n35\n43\n39\n40\n41\n34\n36\n39\n39\n39\n32\n35\n\n\n\n\n\n\n\n\n61\n62\n63\n64\n65\n66\n67\n68\n69\n70\n71\n72\n73\n74\n75\n\n\n\n\n38\n39\n37\n46\n29\n35\n22\n30\n33\n43\n28\n30\n36\n37\n26\n\n\n\n\n\n\n\n\n76\n77\n78\n79\n80\n81\n82\n83\n84\n85\n86\n87\n88\n89\n90\n\n\n\n\n27\n40\n47\n32\n50\n30\n22\n43\n31\n41\n45\n25\n33\n44\n29\n\n\n\n\n\n\nIn totale sono state effettuate n = 3171 estrazioni. Ad un occhio poco esperto, potrebbe sembrare che i numeri 15 ed 80 compaiano con maggiore frequenza rispetto agli altri (51 volte e 50 volte, rispettivamente). È anche ironico notare che non si tratta dei numeri (21 e 48) usati nella famosa scommessa – non saprei proprio dire quale criterio abbiano seguito i tre studenti salentini. In ogni caso, si tratta di oscillazioni casuali, come vedremo.\nPossiamo anche rappresentare graficamente queste frequenze, ottenendo il seguente diagramma a bastoncini.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLa tabella ed il grafico confermano che ci sono delle oscillazioni nelle frequenze: alcuni numeri sono comparsi di più, altri di meno. Questo è un fenomeno assolutamente normale!"
  },
  {
    "objectID": "post/lotteria_AI/lotteria_truccata.html#oscillazioni-casuali-o-lotteria-difettosa",
    "href": "post/lotteria_AI/lotteria_truccata.html#oscillazioni-casuali-o-lotteria-difettosa",
    "title": "La lotteria nazionale non è difettosa",
    "section": "Oscillazioni casuali o lotteria difettosa?",
    "text": "Oscillazioni casuali o lotteria difettosa?\nPer stabilire se le oscillazioni che abbiamo registrato sono frutto del caso o meno, dobbiamo usare degli strumenti statistici un po’ più complicati. Sotto l’ipotesi di indipendenza delle estrazioni, ma non quella di equidistribuzione, la frequenze che abbiano riportato sopra, che indichiamo con n_1,\\dots,n_{90}, seguono una distribuzione multinomiale di parametro n = 3171 e probabilità p_1,\\dots,p_{90}.\nVogliamo verificare se le probabilità p_1,\\dots,p_{90} sono diverse tra loro, cosa che indicherebbe la presenza di un macchinario “difettoso”, oppure se sono uguali tra loro e pari a 1/90. Per far questo, facciamo uso del celeberrimo test del Chi-quadrato2. In pratica, confrontiamo le frequenze che abbiamo osservato n_1,\\dots,n_{90} con quelle “teoriche”, pari a n_\\text{teo} = n / 90 = 35.233. Otteniamo: \nX^2 = \\frac{(n_1 - n_\\text{teo})^2 + \\cdots + (n_{90} - n_\\text{teo})^2}{n_\\text{teo}} = 102.12.\n Il numero X^2 = 102.12 va confrontato con i valori che ci aspetteremmo di osservare se la lotteria fosse regolare, che sono compresi tra circa 65 e 117. Più rigorosamente, calcoliamo il cosiddetto p-value, un concetto tecnico un po’ difficile da raccontare, ma che in buona sostanza ci conferma che non ci sono anomalie statistiche. Per gli statistici in ascolto, ecco i risultati completi:\n\n\n\n    Chi-squared test for given probabilities\n\ndata:  freq\nX-squared = 102.12, df = 89, p-value = 0.1615"
  },
  {
    "objectID": "post/lotteria_AI/lotteria_truccata.html#un-cenno-a-metodi-statistici-più-complessi",
    "href": "post/lotteria_AI/lotteria_truccata.html#un-cenno-a-metodi-statistici-più-complessi",
    "title": "La lotteria nazionale non è difettosa",
    "section": "Un cenno a metodi statistici più complessi",
    "text": "Un cenno a metodi statistici più complessi\n\n\n\n\n\n\nE gli altri numeri?\n\n\n\nIn effetti, i numeri della lotteria sono 5 mentre noi abbiamo analizzato soltanto il primo (N1). Ho pertanto eseguito la stessa analisi per tutti i numeri N1, N2, N3, N4 ed N5, applicando una correzione di Bonferroni, senza registrare alcuna anomalia. A dirla tutta, nessun p-value risultava inferiore a 0.1, rendendo superflua la correzione di Bonferroni.\nAncor meglio, si potrebbe modellare la distribuzione congiunta dei 5 numeri della lotteria e confrontarla quindi con una distribuzione ipergeometrica multivariata. Questra strada è forse la migliore ed è anche tecnicamente percorribile, ma richiede un po’ troppo lavoro aggiuntivo che mi sembrava eccessivo ai fini di questo post.\n\n\n\n\n\n\n\n\nE l’ipotesi di indipendenza?\n\n\n\nIn questa analisi abbiamo fatto uso dell’ipotesi di indipendenza tra le diverse estrazioni. È un’ipotesi molto ragionevole, anche perchè la sua violazione è molto probabilmente legata ad una frode.\nIn linea di principio, anche l’ipotesi di indipendenza può essere messa sotto attento scrutinio statistico, al costo di un po’ di lavoro aggiuntivo. Mi sembra, però, che l’accusa di frode allo stato italiano sfoci nel complottismo, soprattutto se non supportata da ulteriori prove indiziali di altra natura.\n\n\n\nUn aggiornamento ed un’elegante soluzione\nProprio nel paragrafo precedente, ho accennato alla possibilità di modellare la distribuzione congiunta dei 5 numeri della lotteria e confrontarla con una distribuzione ipergeometrica multivariata. L’approccio è corretto ma un po’ complicato. Fortunamente, esiste una elegante soluzione che consente di semplificare notevolmente il problema.\nNell’articolo Stigler (2003), che quando ho scritto questo post non avevo letto, si fa riferimento ad un test del chi-quadrato simile a quello che ho eseguito qui sopra. Stigler considera le frequenze N_1,\\dots, N_{90} di tutti e 5 i numeri della lotteria e considera quindi una correzione di X^2 che tenga conto del fatto che le cinquine sono state estratte senza reinserimento. Stigler, a sua volta, rimanda all’autorevole libro McCullagh & Nelder (1986, pag. 191-192) per i dettagli formali, in cui il problema è dato come esercizio (!).\nIn questo caso, avremo che N = 15855 e che N_\\text{teo} = N / 90 = 176.167. Il test del chi-quadrato, aggiustato per tener conto dei reinserimenti, è \n(X^2 \\text{ corretto}) = \\frac{89}{85}X^2 = \\frac{89}{85}\\frac{(N_1 - N_\\text{teo})^2 + \\cdots + (N_{90} - N_\\text{teo})^2}{N_\\text{teo}} = 87.33.\n I risultati completi sono riportati nel seguito, incluso il p-value, che nuovamente non mostra alcuna evidenza di uno sbilanciamento della lotteria.\n\n\n\nX^2\nX^2 corretto\nGradi di libertà\np-value\n\n\n\n\n83.40114\n87.32589\n89\n0.5303704\n\n\n\nL’articolo di Stigler inoltre menziona anche a test utili a verificare l’ipotesi di indipendenza delle estrazioni, che tuttavia qui non approfondiremo."
  },
  {
    "objectID": "post/lotteria_AI/lotteria_truccata.html#footnotes",
    "href": "post/lotteria_AI/lotteria_truccata.html#footnotes",
    "title": "La lotteria nazionale non è difettosa",
    "section": "Note",
    "text": "Note\n\n\nQuesto bell’articolo mi è stato segnalato dal prof. Aldo Solari, che ha letto una versione iniziale di questo post e che ringrazio molto.↩︎\nIl test del chi-quadrato è un famoso test statistico introdotto da Karl Pearson all’inizio del secolo scorso. Una trattazione più dettagliata si può trovare in Sezione 1.5.2 del libro Agresti (2013).↩︎"
  },
  {
    "objectID": "teaching.html",
    "href": "teaching.html",
    "title": "Tommaso Rigon",
    "section": "",
    "text": "Ph.D. courses (Dottorato)\n\nStatistical Inference, Ph.D. in Economics, Statistics and Data Science, University of Milano-Bicocca.\nBayesian Computations, Ph.D. in Economics, Statistics and Data Science, University of Milano-Bicocca.\n\nM.Sc. courses (Laurea magistrale)\n\nData Mining, CdL Magistrale in Scienze Statistiche ed Economiche, University of Milano-Bicocca.\n\nB.Sc. courses (Laurea triennale)\n\nStatistics III: Generalized Linear Models, CdL in Scienze Statistiche ed Economiche, University of Milano-Bicocca.\nStatistics I (ITA), CdL in Scienze Statistiche ed Economiche, University of Milano-Bicocca.\nR for the multivariate statistical analysis (ITA), CdL in Scienze Statistiche ed Economiche, University of Milano-Bicocca."
  },
  {
    "objectID": "teaching.html#teaching-unimib",
    "href": "teaching.html#teaching-unimib",
    "title": "Tommaso Rigon",
    "section": "",
    "text": "Ph.D. courses (Dottorato)\n\nStatistical Inference, Ph.D. in Economics, Statistics and Data Science, University of Milano-Bicocca.\nBayesian Computations, Ph.D. in Economics, Statistics and Data Science, University of Milano-Bicocca.\n\nM.Sc. courses (Laurea magistrale)\n\nData Mining, CdL Magistrale in Scienze Statistiche ed Economiche, University of Milano-Bicocca.\n\nB.Sc. courses (Laurea triennale)\n\nStatistics III: Generalized Linear Models, CdL in Scienze Statistiche ed Economiche, University of Milano-Bicocca.\nStatistics I (ITA), CdL in Scienze Statistiche ed Economiche, University of Milano-Bicocca.\nR for the multivariate statistical analysis (ITA), CdL in Scienze Statistiche ed Economiche, University of Milano-Bicocca."
  },
  {
    "objectID": "teaching.html#slides-of-talks-and-seminars",
    "href": "teaching.html#slides-of-talks-and-seminars",
    "title": "Tommaso Rigon",
    "section": "Slides of talks and seminars",
    "text": "Slides of talks and seminars\n\nSlides. Bayesian nonparametric prediction of the taxonomic affiliation of DNA sequences, UCLA, 2023.\nSlides. Finite-dimensional discrete random structures and Bayesian clustering, Duke University, 2023.\nSlides. The Stirling-gamma process and its application to Bayesian nonparametrics, BISP13, 2023.\nSlides. An enriched mixture model for functional clustering, PwC, 2022.\nSlides. Statistical learning via Bayesian nonparametrics, University of Milano-Bicocca, 2020.\nSlides. A generalized Bayes framework for probabilistic clustering, Collegio Carlo Alberto, 2020."
  },
  {
    "objectID": "teaching.html#open-day-science-statistiche-ed-economiche-sse",
    "href": "teaching.html#open-day-science-statistiche-ed-economiche-sse",
    "title": "Tommaso Rigon",
    "section": "Open Day, Science Statistiche ed Economiche (SSE)",
    "text": "Open Day, Science Statistiche ed Economiche (SSE)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSlides. Open Day, Science Statistiche ed Economiche (SSE), Università Milano-Bicocca, Dicembre 2022."
  },
  {
    "objectID": "tesi.html",
    "href": "tesi.html",
    "title": "Informazioni per la tesi di laurea",
    "section": "",
    "text": "Sebbene talvolta vi sia una certa sovrapposizione, di norma la didattica, soprattutto quella “di base”, non riflette gli interessi di ricerca di un docente.\nLe tematiche di cui mi occupo sono: applicazioni e metodi di statistica bayesiana (nonparametrica) & statistica computazionale. Per maggiori informazioni, si consiglia di consultare la lista delle pubblicazioni, oppure la lista di tesi passate che trovate in questa pagina.\nNel caso di tesi triennali, ho talvolta seguito anche progetti di carattere più applicativo (ad es. progetti legati ad uno stage esterno), oppure di statistica metodologica classica (test d’ipotesi, metodi boostrap, etc.).\nPer informazioni riguardanti la tesi, si prega di scrivere una mail a tommaso.rigon@unimib.it e venire quindi a ricevimento Martedì alle ore 17.30."
  },
  {
    "objectID": "tesi.html#il-software-latex",
    "href": "tesi.html#il-software-latex",
    "title": "Informazioni per la tesi di laurea",
    "section": "Il software LaTeX",
    "text": "Il software LaTeX\nL’utilizzo del software LaTeX per la scrittura della tesi è caldamente raccomandato.\nLaTeX può essere scaricato gratuitamente da internet. In alternativa, è possibile usare Overleaf. Per una guida in italiano all’uso di LaTeX si consiglia LaTeX per l’impaziente oppure l’Arte di scrivere con LaTeX.\nViene inoltre reso disponibile un template LaTeX. Se compilato, il risultato finale dovrebbe coincidere con questo file, in formato .pdf."
  },
  {
    "objectID": "tesi.html#tesi-passate-triennali-magistrali-e-dottorato",
    "href": "tesi.html#tesi-passate-triennali-magistrali-e-dottorato",
    "title": "Informazioni per la tesi di laurea",
    "section": "Tesi passate (triennali, magistrali e dottorato)",
    "text": "Tesi passate (triennali, magistrali e dottorato)\nVengono riportate nel seguito alcune tesi triennali svolte negli anni passati di cui sono stato relatore, correlatore, oppure in cui sono stato attivamente coinvolto per altri motivi. Non è una lista esaustiva.\n\nThesis\n\n\n\n\n\n\n\n\nA.A.\nLaurea\nCandidata/o\nTitolo\n\n\n\n\n\n\n\nLaurea Triennale\n\n\n2021/2022\nTriennale\nMarco Carrettoni\nAnalisi dei consumi energetici degli edifici dell’Università degli studi di Milano-Bicocca\n\n\n2021/2022\nTriennale\nErica Delvino\nRegressione nonparametrica tramite processi gaussiani\n\n\n2021/2022\nTriennale\nAnna Petranzan\nAn introduction to Bayesian mixture models and applications\n\n\n2022/2023\nTriennale\nGiulia De Innocentiis\nCriteri di selezione di un modello in ambito bayesiano\n\n\n2022/2023\nTriennale\nTommaso Menghini\nApproccio bayesiano applicato a modelli di regressione probit\n\n\n2023/2024\nTriennale\nStefano Andreoli\nMetodi di stima intervallare in un modello binomiale\n\n\n2023/2024\nTriennale\nRiccardo Cicuttin\nMetodi di campionamento per distribuzioni univariate\n\n\n2024/2025\nTriennale\nFederico Boiocchi\nRobust variable selection: an approach based on knockoffs\n\n\n\n\n\nLaurea Magistrale\n\n\n2020/2021\nMagistrale (Unipd)\nDavide Agnoletto\nAnalisi delle curve di mortalità: stima non parametrica della densità con approccio bayesiano\n\n\n2022/2023\nMagistrale\nGabriele Tinè\nPrevisione della sopravvivenza di pazienti affetti da sarcomi tramite un modello bayesiano per dati ad elevata dimensionalità\n\n\n2023/2024\nMagistrale\nAnna Petranzan\nThe Gnedin model: recent developments and future perspectives\n\n\n2023/2024\nMagistrale\nLuna Cantaroni\nAnalisi della struttura interna de La Lombardia, attraverso l’utilizzo di modelli per blocchi stocastici estesi\n\n\n2024/2025\nMagistrale\nAlessandro Colello\nBayesian nonparametric clustering with variational inference and fusing of localized densities\n\n\n2024/2025\nMagistrale\nMaria Regina Mucilli\nDistribuzione secante iperbolica: caratterizzazione, generalizzazione e applicazioni in modelli lineari generalizzati - Slides\n\n\n2024/2025\nMagistrale\nTommaso Menghini\nSondaggi elettorali: studio sull’affidabilità statistica degli istituti demoscopici e delle loro distorsioni sistematiche - Slides\n\n\n\n\n\nDottorato di ricerca\n\n\n2023\nPh.D. (Duke)\nAlessandro Zito\nEcological modeling via Bayesian nonparametric species sampling priors - Slides\n\n\n2024\nPh.D. (Unipd)\nDavide Agnoletto\nStatistical modeling within the generalized Bayesian paradigm"
  },
  {
    "objectID": "post/BISP2025/BISP2025.html",
    "href": "post/BISP2025/BISP2025.html",
    "title": "BISP14",
    "section": "",
    "text": "Davide Agnoletto (Duke University)\n\n\n\n\n\n\n\n\nDavid Dunson (Duke University)"
  },
  {
    "objectID": "post/BISP2025/BISP2025.html#warm-thanks",
    "href": "post/BISP2025/BISP2025.html#warm-thanks",
    "title": "BISP14",
    "section": "",
    "text": "Davide Agnoletto (Duke University)\n\n\n\n\n\n\n\n\nDavid Dunson (Duke University)"
  },
  {
    "objectID": "post/BISP2025/BISP2025.html#foundations",
    "href": "post/BISP2025/BISP2025.html#foundations",
    "title": "BISP14",
    "section": "Foundations",
    "text": "Foundations\n\nDe Finetti’s representation Theorem (De Finetti 1937) has a central role in Bayesian statistics because it provides the fundamental justification to the two approaches to Bayesian statistics:\n\nthe hypothetical approach;\nthe predictive approach.\n\n\n\n\n\n\n\n\nDe Finetti’s representation theorem\n\n\n\nLet (Y_n)_{n\\ge 1}, Y_n\\in\\mathcal{Y}, be a sequence of exchangeable random variables with probability law P. Then there exists a unique probability measure \\Pi such that, for any n\\ge 1,\n\nP(y_1,\\ldots,y_n) = \\int_{\\mathcal{F}} \\prod_{i=1}^n F(y_i)\\,\\Pi(\\mathrm{d}F).\n\n\n\n\nWhile representing opposite interpretations of the Theorem, the two approaches are intrinsically connected."
  },
  {
    "objectID": "post/BISP2025/BISP2025.html#hypothetical-approach",
    "href": "post/BISP2025/BISP2025.html#hypothetical-approach",
    "title": "BISP14",
    "section": "Hypothetical approach",
    "text": "Hypothetical approach\n\nThe hypothetical approach represents the the most common way to operate within the Bayesian community.\nIn a parametric setting, \\Pi has support on a class \\Theta\\subseteq\\mathbb{R}^p, p&lt;\\infty, such that \\boldsymbol{\\theta}\\in\\Theta indexes the class of distributions \\mathcal{F}_{\\boldsymbol{\\theta}}=\\{F_{\\boldsymbol{\\theta}} : \\boldsymbol{\\theta} \\in \\Theta\\subseteq\\mathbb{R}^p\\}.\nBayes’ rule takes the well-known formulation: \n\\pi(\\boldsymbol{\\theta}\\mid y_1,\\ldots,y_n) \\propto \\pi(\\boldsymbol{\\theta}) \\prod_{i=1}^n f_{\\boldsymbol{\\theta}}(y_i),\n where \\pi and f_{\\boldsymbol{\\theta}} denote the probability density functions associated with \\Pi and F_{\\boldsymbol{\\theta}}, respectively.\nHowever, when the link between observations and parameter of interest cannot be expressed through a distribution function, the traditional hypothetical approach fails.\n\n\nSolution: generalized posterior distributions, sometimes called Gibbs-posteriors.\nThis is a lively recent topic, see for instance: Chernozhukov and Hong (2003); Bissiri et al. (2016) Heide et al. (2020); Grünwald and Mehta (2020); Knoblauch et al. (2022); Matsubara et al. (2022); Matsubara et al. (2023); Jewson and Rossell (2022); Rigon et al. (2023)."
  },
  {
    "objectID": "post/BISP2025/BISP2025.html#generalizations-of-the-hypothetical-approach",
    "href": "post/BISP2025/BISP2025.html#generalizations-of-the-hypothetical-approach",
    "title": "BISP14",
    "section": "Generalizations of the hypothetical approach",
    "text": "Generalizations of the hypothetical approach\n\nBissiri et al. (2016) showed that the generalized posterior \n\\pi_\\omega(\\boldsymbol{\\theta} \\mid \\mathbf{y}_{1:n}) \\propto \\pi(\\boldsymbol{\\theta}) \\exp\\left\\{ - \\omega \\sum_{i=1}^n \\ell(\\boldsymbol{\\theta}; y_i) \\right\\},\n is the only coherent update of the prior beliefs about \n\\boldsymbol{\\theta}^* = \\arg\\min_{\\boldsymbol{\\theta}\\in\\Theta} \\int_{\\mathcal{Y}} \\ell(\\boldsymbol{\\theta}; y)\\, F_0(\\mathrm{d}y),\n where \\ell(\\boldsymbol{\\theta}, y) is a loss function, \\omega is the loss-scale, and F_0 is the unknown true sampling distribution.\n\n\nLearning the loss scale \\omega from the data is a delicate task. Assuming a prior for \\omega can lead to degenerate estimates if not accompanied by additional adjustments to the loss function.\nHowever, there are several solutions for its calibration: Holmes and Walker (2017); Lyddon et al. (2019); Syring and Martin (2019); Matsubara et al. (2023).\n\n\nOur contribution: Bayesian inference for generalized linear models via quasi-posteriors."
  },
  {
    "objectID": "post/BISP2025/BISP2025.html#generalized-bayes-for-glms",
    "href": "post/BISP2025/BISP2025.html#generalized-bayes-for-glms",
    "title": "BISP14",
    "section": "Generalized Bayes for GLMs",
    "text": "Generalized Bayes for GLMs\n\nGeneralized linear models (GLMs) are routinely used to model a wide variety of data.\nThe Bayesian approach for GLMs is also incredibly popular, e.g. because of the possibility of naturally incorporating random effects, complex penalizations, prior information, and more.\nHowever, they often incur misspecification, and this could compromise inferential conclusions.\n\n\nA common case is overdispersion, i.e., when proportion or count observations show larger variability than the one assumed by the model.\nTraditional solutions have important drawbacks:\n\nModel-based: may lead to computational bottlenecks and can result again in misspecification.\nNonparametric: increased computational cost and loss of efficiency and interpretability.\n\n\n\nWe rely on a semi-parametric approach, making only assumptions on the mean and variance of the response while preserving computational tractability."
  },
  {
    "objectID": "post/BISP2025/BISP2025.html#second-order-assumptions",
    "href": "post/BISP2025/BISP2025.html#second-order-assumptions",
    "title": "BISP14",
    "section": "Second order assumptions",
    "text": "Second order assumptions\n\nLet Y_i \\in \\mathcal{Y} denote a response variable, \\boldsymbol{x}_i \\in \\mathbb{R}^p be a vector of covariates for i = 1, \\ldots, n, and \\boldsymbol{\\beta} \\in \\mathbb{R}^p be the parameter of interest.\nStandard GLMs assume that observations y_i are independent realizations of Y_i \\mid \\boldsymbol{x}_i, whose distribution belongs to the exponential dispersion family.\n\n\n\n\n\n\n\nWe assume the second-order conditions: \n\\mathbb{E}\\{Y_i\\} = \\mu_i = g^{-1}(\\boldsymbol{x}_i^\\top \\boldsymbol{\\beta}), \\quad\n\\mathrm{var}\\{Y_i\\} = \\psi\\, V(\\mu_i),\n where g(\\cdot) is a link function, V(\\cdot)&gt;0 is a variance function, and \\psi \\in (0,\\infty) is a dispersion parameter.\nWe let (\\boldsymbol{\\beta}_0, \\psi_0) be the true values for the parameters (\\boldsymbol{\\beta}, \\psi) and we assume the data are generated under F_0(\\mathrm{d}y \\mid \\boldsymbol{x}) = F(\\mathrm{d}y \\mid \\boldsymbol{x}, \\boldsymbol{\\beta}_0, \\psi_0).\n\n\n\n\nAlthough the mean and variance functions need to be carefully chosen to fit the data, the resulting inferences are robust to misspecification of higher-order moments."
  },
  {
    "objectID": "post/BISP2025/BISP2025.html#quasi-likelihood",
    "href": "post/BISP2025/BISP2025.html#quasi-likelihood",
    "title": "BISP14",
    "section": "Quasi-likelihood",
    "text": "Quasi-likelihood\n\nUnder the second-order assumptions, it is possible to specify the so-called log-quasi-likelihood function (Wedderburn 1974): \n\\ell_Q(\\boldsymbol{\\beta}; \\mathbf{y}, \\mathbf{X}, \\psi) = \\sum_{i=1}^n \\ell_Q(\\boldsymbol{\\beta}; y_i, \\boldsymbol{x}_i, \\psi) = \\sum_{i=1}^n \\int_a^{\\mu_i(\\boldsymbol{\\beta})} \\frac{y_i - t}{\\psi V(t)} \\, \\mathrm{d}t,\n where a is an arbitrary constant that does not depend on \\boldsymbol{\\beta}.\nThe above integral can be written in closed form for many choices of variance functions, including those associated with exponential family distributions.\n\n\nQuasi-likelihoods retain many properties of genuine likelihoods, such as unbiased estimating equations and the information identity: \n\\mathbb{E}\\left\\{ \\nabla \\ell_Q(\\boldsymbol{\\beta}; \\mathbf{Y}, \\mathbf{X}, \\psi) \\right\\} = 0, \\qquad\n\\mathbb{E}\\left\\{ -\\nabla^2 \\ell_Q(\\boldsymbol{\\beta}; \\mathbf{Y}, \\mathbf{X}, \\psi) \\right\\} =\n\\mathbb{E}\\left\\{ \\nabla \\ell_Q \\nabla \\ell_Q^\\top \\right\\},\n where \\nabla denotes the gradient with respect to \\boldsymbol{\\beta}.\n\n\nUnder mild conditions, the maximum quasi-likelihood is consistent and has the smallest asymptotic variance among estimators derived from linear estimating equations (McCullagh 1983)."
  },
  {
    "objectID": "post/BISP2025/BISP2025.html#quasi-posteriors-i",
    "href": "post/BISP2025/BISP2025.html#quasi-posteriors-i",
    "title": "BISP14",
    "section": "Quasi-posteriors I",
    "text": "Quasi-posteriors I\n\n\n\n\n\n\nLet \\exp\\{\\ell_Q(\\boldsymbol{\\beta}; \\mathbf{y}, \\mathbf{X}, \\psi)\\} be the quasi-likelihood function and p(\\boldsymbol{\\beta}) be the prior distribution for \\boldsymbol{\\beta}. We define the quasi-posterior distribution for \\boldsymbol{\\beta} as: \np_Q(\\boldsymbol{\\beta} \\mid \\mathbf{y}, \\mathbf{X}, \\psi) \\propto p(\\boldsymbol{\\beta}) \\exp \\left\\{ \\ell_Q(\\boldsymbol{\\beta}; \\mathbf{y}, \\mathbf{X}, \\psi) \\right\\} =\np(\\boldsymbol{\\beta}) \\exp \\left\\{ \\frac{1}{\\psi} \\sum_{i=1}^n \\int_a^{\\mu_i(\\boldsymbol{\\beta})} \\frac{y_i - t}{V(t)} \\, \\mathrm{d}t \\right\\}\n\n\n\n\n\nThe quasi-posterior is a rational update of a belief distribution within the generalized Bayesian framework, with loss function: \n\\ell(\\boldsymbol{\\beta}; \\mathbf{y}, \\mathbf{X}) = - \\sum_{i=1}^n \\int_a^{\\mu_i(\\boldsymbol{\\beta})} \\frac{y_i - t}{V(t)} \\, \\mathrm{d}t.\n\nThe dispersion parameter \\psi plays the role of a loss-scale parameter for the quasi-posterior."
  },
  {
    "objectID": "post/BISP2025/BISP2025.html#quasi-posteriors-ii",
    "href": "post/BISP2025/BISP2025.html#quasi-posteriors-ii",
    "title": "BISP14",
    "section": "Quasi-posteriors II",
    "text": "Quasi-posteriors II\n\nThe quasi-posterior represents subjective uncertainty about the unknown parameter value: \n\\boldsymbol{\\beta}^* = \\arg\\min_{\\boldsymbol{\\beta}} \\int_{\\mathcal{Y}} \\ell(\\boldsymbol{\\beta}; \\mathbf{y}, \\mathbf{X}) \\, F_0(d\\mathbf{y} \\mid \\mathbf{X}),\n which is assumed to be unique (Bissiri et al. 2016).\nThe definition of \\boldsymbol{\\beta}^* does not automatically guarantee that \\boldsymbol{\\beta}^* = \\boldsymbol{\\beta}_0.\n\n\n\n\n\n\n\nTheorem (Agnoletto, R., Dunson, 2025)\n\n\n\nAssume the second-order conditions are well-specified, and suppose the target of inference \\boldsymbol{\\beta}^* is unique. Then, for quasi-posteriors, \\boldsymbol{\\beta}^* must coincide with the true value \\boldsymbol{\\beta}_0: \n\\boldsymbol{\\beta}^* = \\arg\\min_{\\boldsymbol{\\beta}} \\int_{\\mathcal{Y}}\n\\sum_{i=1}^n \\int_a^{\\mu_i(\\boldsymbol{\\beta})} \\frac{t - y_i}{V(t)} \\, \\mathrm{d}t \\, F_0(d\\mathbf{y} \\mid \\mathbf{X}) = \\boldsymbol{\\beta}_0."
  },
  {
    "objectID": "post/BISP2025/BISP2025.html#calibration-of-the-dispersion-i",
    "href": "post/BISP2025/BISP2025.html#calibration-of-the-dispersion-i",
    "title": "BISP14",
    "section": "Calibration of the dispersion I",
    "text": "Calibration of the dispersion I\n\nBased on a comparison with the Bayesian bootstrap, Lyddon et al. (2019) propose calibrate the dispersion \\psi setting it equal to: \n\\psi_{\\text{LLB}} =\n\\frac{\\mathrm{tr}\\{j(\\boldsymbol{\\beta}_0)\\}}{\\mathrm{tr}\\{j(\\boldsymbol{\\beta}_0) h(\\boldsymbol{\\beta}_0)^{-1} j(\\boldsymbol{\\beta}_0)\\}},\n where we define \nj(\\boldsymbol{\\beta}) := \\lim_{n \\to \\infty} \\frac{1}{n} \\mathbb{E}\\left[\\nabla^2 \\ell(\\boldsymbol{\\beta}; \\mathbf{Y}, \\mathbf{X})\\right], \\qquad  h(\\boldsymbol{\\beta}) := \\lim_{n \\to \\infty} \\frac{1}{n} \\mathbb{E}\\left[\\nabla \\ell(\\boldsymbol{\\beta}; \\mathbf{Y}, \\mathbf{X}) \\nabla \\ell(\\boldsymbol{\\beta}; \\mathbf{Y}, \\mathbf{X})^\\top\\right].\n\n\n\n\n\n\n\n\nProposition (Agnoletto, R., Dunson, 2025)\n\n\n\nUnder the second order conditions, namely if \\mathbb{E}(Y_i) = g^{-1}(\\mathbf{x}_i^\\top \\boldsymbol{\\beta}_0) and \\mathrm{var}(Y_i) = \\psi_0 V\\{\\mu_i(\\boldsymbol{\\beta}_0)\\}, then for quasi posteriors with loss \\ell(\\boldsymbol{\\beta}; \\mathbf{y}, \\mathbf{X}) = -\\sum_{i=1}^n \\int_a^{\\mu_i(\\boldsymbol{\\beta})} \\frac{y_i - t}{V(t)} \\mathrm{d}t, we have \n\\psi_{\\text{LLB}} = \\psi_0."
  },
  {
    "objectID": "post/BISP2025/BISP2025.html#calibration-of-the-dispersion-ii",
    "href": "post/BISP2025/BISP2025.html#calibration-of-the-dispersion-ii",
    "title": "BISP14",
    "section": "Calibration of the dispersion II",
    "text": "Calibration of the dispersion II\n\n\n\n\n\n\nTheorem (Agnoletto, R., Dunson, 2025)\n\n\n\nAssume the second-order conditions are correctly specified. Let S_1, S_2, \\ldots \\subseteq \\mathbb{R}^p be a sequence of convex credible sets of level \\rho \\in (0,1). Then, under mild conditions and setting \\psi = \\psi_0: \n\\mathbb{P}(\\boldsymbol{\\beta}_0 \\in S_n \\mid \\mathbf{y}, \\mathbf{X}, \\psi_0) \\to \\rho \\quad \\text{as } n \\to \\infty.\n\n\n\n\nAs \\psi_0 is typically unknown, we can use the classical method of moments estimator: \n\\widehat{\\psi} = \\frac{1}{n - p} \\sum_{i=1}^n \\frac{(y_i - \\widehat{\\mu}_i)^2}{V(\\widehat{\\mu}_i)},\n where \\widehat{\\mu}_i = \\mu_i(\\widehat{\\boldsymbol{\\beta}}), which is fast and consistent (McCullagh and Nelder 1989)."
  },
  {
    "objectID": "post/BISP2025/BISP2025.html#simulation-results-i",
    "href": "post/BISP2025/BISP2025.html#simulation-results-i",
    "title": "BISP14",
    "section": "Simulation results I",
    "text": "Simulation results I\n\nData are generated from a distribution with \\mathbb{E}(Y_i) = \\mu_i(\\boldsymbol{\\beta}_0)= \\exp(\\mathbf{x}_i^\\top \\boldsymbol{\\beta}_0) and \\mathrm{var}(Y_i) = \\psi_0 \\mu_i(\\boldsymbol{\\beta}_0) —not a Poisson!— with parameters \\beta_0 = (3.5,\\; 1.5,\\; -1.0,\\; 0.5), and \\psi_0 = 3.5.\nWe computed posterior means and 95\\% credible intervals.\nThe sample size is n = 100; estimates are averages over multiple simulated datasets.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPoisson\nNegative Binomial\nDFD-Bayes\nQuasi-posterior\nQuasi-posterior + alternative \\hat{\\psi}\n\n\n\n\n\\beta_1\nMean\n3.50 (0.035)\n3.49 (0.047)\n57.46 (105.41)\n3.50 (0.035)\n3.50 (0.035)\n\n\n\nCover.\n0.715\n0.920\n0.201\n0.945\n0.970\n\n\n\\beta_2\nMean\n1.50 (0.020)\n1.51 (0.040)\n5.26 (6.31)\n1.50 (0.020)\n1.50 (0.020)\n\n\n\nCover.\n0.675\n0.960\n0.454\n0.945\n0.990\n\n\n\\beta_3\nMean\n-1.00 (0.020)\n-1.01 (0.034)\n-3.98 (6.53)\n-1.00 (0.020)\n-1.00 (0.020)\n\n\n\nCover.\n0.715\n0.995\n0.479\n0.950\n0.965\n\n\n\\beta_4\nMean\n0.50 (0.018)\n0.50 (0.037)\n2.55 (7.37)\n0.50 (0.018)\n0.50 (0.018)\n\n\n\nCover.\n0.655\n0.965\n0.526\n0.950\n0.970"
  },
  {
    "objectID": "post/BISP2025/BISP2025.html#simulation-results-ii",
    "href": "post/BISP2025/BISP2025.html#simulation-results-ii",
    "title": "BISP14",
    "section": "Simulation results II",
    "text": "Simulation results II\n\nData are generated from a distribution with \\mathbb{E}(Y_i) = \\mu_i(\\boldsymbol{\\beta}_0)= \\exp(\\mathbf{x}_i^\\top \\boldsymbol{\\beta}_0) and \\mathrm{var}(Y_i) = \\psi_0 \\mu_i(\\boldsymbol{\\beta}_0) —not a Poisson!— with parameters \\beta_0 = (3.5,\\; 1.5,\\; -1.0,\\; 0.5), and \\psi_0 = 3.5.\nWe computed posterior means and 95\\% credible intervals.\nThe sample size is n = 1000; estimates are averages over multiple simulated datasets.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPoisson\nNegative Binomial\nDFD-Bayes\nQuasi-posterior\nQuasi-posterior + alternative \\hat{\\psi}\n\n\n\n\n\\beta_1\nMean\n3.50 (0.010)\n3.50 (0.015)\n4.64 (8.45)\n3.50 (0.010)\n3.50 (0.010)\n\n\n\nCover.\n0.690\n0.835\n0.070\n0.945\n0.955\n\n\n\\beta_2\nMean\n1.50 (0.005)\n1.50 (0.012)\n1.87 (1.37)\n1.50 (0.005)\n1.50 (0.005)\n\n\n\nCover.\n0.665\n0.910\n0.510\n0.950\n0.960\n\n\n\\beta_3\nMean\n-1.00 (0.005)\n-1.00 (0.010)\n-1.21 (0.81)\n-1.00 (0.005)\n-1.00 (0.005)\n\n\n\nCover.\n0.680\n0.960\n0.690\n0.955\n0.950\n\n\n\\beta_4\nMean\n0.50 (0.005)\n0.50 (0.009)\n0.54 (0.35)\n0.50 (0.005)\n0.50 (0.005)\n\n\n\nCover.\n0.715\n0.950\n0.810\n0.950\n0.940"
  },
  {
    "objectID": "post/BISP2025/BISP2025.html#thank-you",
    "href": "post/BISP2025/BISP2025.html#thank-you",
    "title": "BISP14",
    "section": "Thank you!",
    "text": "Thank you!\n\n\n\n\n\nThe main paper is:\nAgnoletto, D., Rigon, T., and Dunson D.B. (2025+). Bayesian inference for generalized linear models via quasi-posteriors. Biometrika, to appear."
  },
  {
    "objectID": "post/BISP2025/BISP2025.html#references",
    "href": "post/BISP2025/BISP2025.html#references",
    "title": "BISP14",
    "section": "References",
    "text": "References\n\n\nBissiri, P. G., Holmes, C. C., and Walker, S. G. (2016), “A general framework for updating belief distributions,” Journal of the Royal Statistical Society: Series B (Statistical Methodology), Wiley Online Library, 78, 1103–1130.\n\n\nChernozhukov, V., and Hong, H. (2003), “An MCMC approach to classical estimation,” Journal of econometrics, Elsevier, 115, 293–346.\n\n\nDe Finetti, B. (1937), “La prévision: Ses lois logiques, ses sources subjectives,” in Annales de l’institut henri poincaré, pp. 1–68.\n\n\nGrünwald, P. D., and Mehta, N. A. (2020), “Fast rates for general unbounded loss functions: from ERM to generalized Bayes,” The Journal of Machine Learning Research, JMLRORG, 21, 2040–2119.\n\n\nHeide, R. de, Kirichenko, A., Grunwald, P., and Mehta, N. (2020), “Safe-bayesian generalized linear regression,” in Proceedings of the twenty third international conference on artificial intelligence and statistics, PMLR, pp. 2623–2633.\n\n\nHolmes, C. C., and Walker, S. G. (2017), “Assigning a value to a power likelihood in a general Bayesian model,” Biometrika, Oxford University Press, 104, 497–503.\n\n\nJewson, J., and Rossell, D. (2022), “General bayesian loss function selection and the use of improper models,” Journal of the Royal Statistical Society Series B: Statistical Methodology, Oxford University Press, 84, 1640–1665.\n\n\nKnoblauch, J., Jewson, J., and Damoulas, T. (2022), “An optimization-centric view on bayes’ rule: Reviewing and generalizing variational inference,” Journal of Machine Learning Research, 23, 1–109.\n\n\nLyddon, S. P., Holmes, C. C., and Walker, S. G. (2019), “General Bayesian updating and the loss-likelihood bootstrap,” Biometrika, Oxford University Press, 106, 465–478.\n\n\nMatsubara, T., Knoblauch, J., Briol, F.-X., and Oates, C. J. (2022), “Robust generalised bayesian inference for intractable likelihoods,” Journal of the Royal Statistical Society Series B: Statistical Methodology, Oxford University Press, 84, 997–1022.\n\n\nMatsubara, T., Knoblauch, J., Briol, F.-X., and Oates, C. J. (2023), “Generalized bayesian inference for discrete intractable likelihood,” Journal of the American Statistical Association, Taylor & Francis, 1–11.\n\n\nMcCullagh, P. (1983), “Quasi-likelihood functions,” Annals of Statistics, Institute of Mathematical Statistics, 11, 59–67.\n\n\nMcCullagh, P., and Nelder, J. A. (1989), Generalized linear models, Chapman & Hall.\n\n\nRigon, T., Herring, A. H., and Dunson, D. B. (2023), “A generalized Bayes framework for probabilistic clustering,” Biometrika, Oxford University Press, 10, 559–578.\n\n\nSyring, N., and Martin, R. (2019), “Calibrating general posterior credible regions,” Biometrika, Oxford University Press, 106, 479–486.\n\n\nWedderburn, R. W. (1974), “Quasi-likelihood functions, generalized linear models, and the Gauss-Newton method,” Biometrika, Oxford University Press, 61, 439–447."
  },
  {
    "objectID": "post/lotteria_AI/index.html",
    "href": "post/lotteria_AI/index.html",
    "title": "Come mai non è possibile prevedere i numeri del lotto",
    "section": "",
    "text": "Il 5 marzo 2025, tre studenti dell’Università del Salento hanno vinto quasi 50.000 euro al lotto, dichiarando di aver usato un algoritmo di intelligenza artificiale per prevedere i numeri da giocare. La notizia è stata ripresa da molti quotidiani come Repubblica e il Corriere della Sera. È comparsa inoltre su testate minori, tra cui: Torino Cronaca e Orizzonte Scuola.\n\n\n\n\n\nNon sorprende che una notizia del genere abbia attirato l’attenzione, perchè coinvolge temi sensibili come il gioco d’azzardo e l’intelligenza artificiale. Lo sfortunato titolo scelto da Repubblica è “Vinti 50mila euro al Lotto grazie all’intelligenza artificiale”.\nPurtroppo, infatti, la notizia è stata data in modo errato ed estremamente fuorviante. Non è possibile prevedere in alcun modo i numeri del lotto, nè tramite strumenti tradizionali nè tramite l’intelligenza artificiale. La notizia è stata poi rettificata dal Corriere ed infine anche da Repubblica, sebbene con enfasi minore rispetto alla notizia iniziale."
  },
  {
    "objectID": "post/lotteria_AI/index.html#il-caso-di-cronaca",
    "href": "post/lotteria_AI/index.html#il-caso-di-cronaca",
    "title": "Come mai non è possibile prevedere i numeri del lotto",
    "section": "",
    "text": "Il 5 marzo 2025, tre studenti dell’Università del Salento hanno vinto quasi 50.000 euro al lotto, dichiarando di aver usato un algoritmo di intelligenza artificiale per prevedere i numeri da giocare. La notizia è stata ripresa da molti quotidiani come Repubblica e il Corriere della Sera. È comparsa inoltre su testate minori, tra cui: Torino Cronaca e Orizzonte Scuola.\n\n\n\n\n\nNon sorprende che una notizia del genere abbia attirato l’attenzione, perchè coinvolge temi sensibili come il gioco d’azzardo e l’intelligenza artificiale. Lo sfortunato titolo scelto da Repubblica è “Vinti 50mila euro al Lotto grazie all’intelligenza artificiale”.\nPurtroppo, infatti, la notizia è stata data in modo errato ed estremamente fuorviante. Non è possibile prevedere in alcun modo i numeri del lotto, nè tramite strumenti tradizionali nè tramite l’intelligenza artificiale. La notizia è stata poi rettificata dal Corriere ed infine anche da Repubblica, sebbene con enfasi minore rispetto alla notizia iniziale."
  },
  {
    "objectID": "post/lotteria_AI/index.html#è-possibile-prevedere-i-numeri-del-lotto",
    "href": "post/lotteria_AI/index.html#è-possibile-prevedere-i-numeri-del-lotto",
    "title": "Come mai non è possibile prevedere i numeri del lotto",
    "section": "È possibile prevedere i numeri del lotto?",
    "text": "È possibile prevedere i numeri del lotto?\nIl gioco del lotto prevede che i numeri siano estratti casualmente e che ciascun numero abbia la stessa probabilità di essere estratto, cioè 1 / 90. Inoltre, le estrazioni sono tra loro indipendenti, ovvero l’estrazione di un numero non influenza quella successiva. In gergo, si dirà che ciascuna estrazione “non ha memoria” di cosa è successo in passato.\nDunque, nel lungo periodo, la frequenza delle estrazioni di ciascun numero sarà circa 1/90. Di conseguenza, non ha senso basare le proprie scelte sui numeri usciti più di frequente, come fatto dai tre studenti, apparentemente su suggerimento del loro tabaccaio. Eventuali differenze nelle frequenze dei numeri del lotto sono oscillazioni casuali prive di significato.\n\n\n\n\n\n\nI numeri ritardatari\n\n\n\nUna credenza comune è che un numero che non esce da molto tempo abbia maggior probabilità di essere estratto. Questa superstizione è rafforzata dal fatto che, nelle tabaccherie, i “numeri ritardatari” vengono spesso mostrati su monitor e tabelloni, come se questa informazione fosse utile ai giocatori. In realtà, non lo è.1\nCome detto prima, anche se un numero non è stato estratto per molto tempo, la probabilità che esca al prossimo sorteggio rimane invariata, ovvero 1/90. Questo proprio perché le estrazioni sono indipendenti le une dalle altre e “non si ricordano” cosa sia successo in passato.\n\n\nA costo di risultare ripetitivo: sotto queste assunzioni (casualità delle estrazioni, probabilità uguali tra di loro, indipendenza) è impossibile prevedere i numeri del lotto.\nQuesti concetti dovrebbero essere ben noti agli studenti di statistica. Da docente, trovo sconfortante che un fraintendimento così elementare provenga proprio da studenti universitari di matematica e fisica."
  },
  {
    "objectID": "post/lotteria_AI/index.html#le-coincidenze-sono-sorprendenti",
    "href": "post/lotteria_AI/index.html#le-coincidenze-sono-sorprendenti",
    "title": "Come mai non è possibile prevedere i numeri del lotto",
    "section": "Le coincidenze sono sorprendenti?",
    "text": "Le coincidenze sono sorprendenti?\nCerchiamo ora di fare luce su un aspetto controintuitivo di questa vicenda. Infatti, vincere 50.000 euro al lotto è un evento piuttosto raro. Questo potrebbe indurre a pensare che non possa essere stata una coincidenza e che sia sotto qualcosa di più. In realtà, gli eventi rari accadono ogni giorno; altrimenti, nessuno vincerebbe mai il Superenalotto.\nPer districare questo apparente paradosso, è utile considerare il numero di tentativi effettuati. In altre parole, quante persone hanno giocato al lotto in tutta Italia, magari utilizzando l’intelligenza artificiale? Tantissime, e la stragrande maggioranza di loro non ha vinto nulla.\nQuesto stesso fraintendimento, che in questo caso ha conseguenze relativamente innocue, può avere effetti ben più gravi in altri contesti. Un esempio tristemente noto è il caso di Sally Clark, ingiustamente accusata di aver ucciso i propri figli a causa di un’errata interpretazione della probabilità. Purtroppo, dopo essere stata scagionata, si tolse la vita."
  },
  {
    "objectID": "post/lotteria_AI/index.html#lo-scandalo-della-lotteria-dellontario",
    "href": "post/lotteria_AI/index.html#lo-scandalo-della-lotteria-dellontario",
    "title": "Come mai non è possibile prevedere i numeri del lotto",
    "section": "Lo scandalo della lotteria dell’Ontario",
    "text": "Lo scandalo della lotteria dell’Ontario\n\n\n\n\n\n\nSe le assunzioni che abbiamo menzionato sono rispettate, allora non dovrebbero emergere anomalie statistiche. Quando invece queste si manifestano, significa che almeno una delle ipotesi non è rispettata, a volte purtroppo per ragioni fraudolente.\nDopotutto, se fosse davvero possibile prevedere i numeri del lotto, ciò equivarebbe ad accusare lo Stato italiano di incompetenza o addirittura di frode. In effetti, in Canada è capitato davvero, come raccontato in questa storia.\n\n\n\nNel 2006, i produttori del programma investigativo canadese The Fifth Estate della CBC contattarono il prof. Jeff Rosenthal per un caso di frode alla lotteria dell’Ontario. Il caso riguardava Bob Edmonds, un anziano giocatore truffato da un commesso che si appropriò del suo biglietto vincente da 250.000 dollari. Dopo una battaglia legale durata 3,5 anni, Edmonds riuscì a ottenere 200.000 dollari, ma solo accettando un accordo di riservatezza con la Ontario Lottery and Gaming Corporation (OLG). La CBC sospettò che l’OLG volesse coprire altri casi simili e chiese a Rosenthal di analizzare i dati da un punto di vista statistico.\nL’analisi completa è disponibile in questo articolo, che descrive come una semplice analisi statistica abbia contribuito a svelare lo scandalo dei rivenditori della lotteria dell’Ontario. Il caso divenne una notizia di primo piano in Canada, portando a dibattiti legislativi, al licenziamento di due CEO, a diverse accuse penali, condanne al carcere e risarcimenti per oltre venti milioni di dollari.\nQuesta storia rappresenta una potente testimonianza dell’importanza e del potere della statistica."
  },
  {
    "objectID": "post/lotteria_AI/index.html#alcuni-riferimenti",
    "href": "post/lotteria_AI/index.html#alcuni-riferimenti",
    "title": "Come mai non è possibile prevedere i numeri del lotto",
    "section": "Alcuni riferimenti",
    "text": "Alcuni riferimenti\nTutti questi fraintendimenti sono ben raccontati nel libro Struck By Lightning: The Curious World Of Probabilities di Jeff Rosenthal, pubblicato in italiano da Longanesi con il titolo Le regole del caso: istruzioni per l’uso.\nSi veda anche il video YouTube: Statistician Answers Stats Questions From Twitter di WIRED."
  },
  {
    "objectID": "post/lotteria_AI/index.html#footnotes",
    "href": "post/lotteria_AI/index.html#footnotes",
    "title": "Come mai non è possibile prevedere i numeri del lotto",
    "section": "Note",
    "text": "Note\n\n\nÈ possibile che questo fraintendimento derivi dalla errata comprensione della legge dei grandi numeri, la quale stabilisce che la frazione di volte in cui un numero viene estratto si avvicina ad 1/90 all’aumentare del numero di prove. Questa affermazione è vera, ma non implica alcun effetto di “compensazione” tra due estrazioni successive. La probabilità è, in effetti, un argomento a volte controintuitivo.↩︎"
  },
  {
    "objectID": "post/Statalk2025/Statalk2025.html",
    "href": "post/Statalk2025/Statalk2025.html",
    "title": "StaTalk 2025",
    "section": "",
    "text": "Alessandra Guglielmi\n(PoliMi)\n\n\n\n\n\n\n\n\nAlessandro Fassò\n(UniBg)\n\n\n\n\n\n\n\n\nMaria Grazia Valsecchi\n(Unimib)"
  },
  {
    "objectID": "post/Statalk2025/Statalk2025.html#panelists",
    "href": "post/Statalk2025/Statalk2025.html#panelists",
    "title": "StaTalk 2025",
    "section": "",
    "text": "Alessandra Guglielmi\n(PoliMi)\n\n\n\n\n\n\n\n\nAlessandro Fassò\n(UniBg)\n\n\n\n\n\n\n\n\nMaria Grazia Valsecchi\n(Unimib)"
  },
  {
    "objectID": "post/Statalk2025/Statalk2025.html#phd-life-over-time",
    "href": "post/Statalk2025/Statalk2025.html#phd-life-over-time",
    "title": "StaTalk 2025",
    "section": "PhD life over time",
    "text": "PhD life over time\n\nHow did you end up here? Was it a grand plan, a happy accident, or the result of saying “yes” one too many times? We would love to hear any anecdotes about what first sparked your interest in your current research area.\n\n\nHow has academia changed at the PhD and postdoc levels compared to when you went through it? Are things better, worse, or just… different with more Zoom?"
  },
  {
    "objectID": "post/Statalk2025/Statalk2025.html#seeking-wisdom",
    "href": "post/Statalk2025/Statalk2025.html#seeking-wisdom",
    "title": "StaTalk 2025",
    "section": "Seeking wisdom",
    "text": "Seeking wisdom\n\nWhat is your advice for a young investigator? Are there common traps you have seen in your work with students?\n\n\nWhat do you think about the often-repeated quip: “publish or perish”? Is it a painful truth or an exaggeration?\n\n\nWe all have times when we feel daunted by the mathematical challenges associated with our work. How do you respond to feelings of inadequacy or intimidation in the face of difficult problems?\n\n\nWhen is it okay to abandon a project? Not every idea is a brilliant one, and some projects slowly (or quickly) reveal themselves to be dead ends. How do you decide when to pull the plug, both as a junior researcher and, hopefully one day, as a senior one?"
  },
  {
    "objectID": "post/Statalk2025/Statalk2025.html#the-mentor-the-mentee-and-their-delicate-dance",
    "href": "post/Statalk2025/Statalk2025.html#the-mentor-the-mentee-and-their-delicate-dance",
    "title": "StaTalk 2025",
    "section": "The mentor, the mentee, and their delicate dance",
    "text": "The mentor, the mentee, and their delicate dance\n\nWhat are the qualities of a good mentor? In your view, what makes someone a great supervisor or mentor?\nWhat advice do you have for balancing criticism and support as a mentor? How do you cultivate a supporting but rigorous academic environment?\n\n\nWhat are some methods for establishing independence from your advisor(s) as your progress in your PhD?\n\n\nWhat role has community played in your career and in your professional successes? Any anecdotes about relationships formed early in your career that you maintain today?"
  },
  {
    "objectID": "post/Statalk2025/Statalk2025.html#next-generation-challenges",
    "href": "post/Statalk2025/Statalk2025.html#next-generation-challenges",
    "title": "StaTalk 2025",
    "section": "Next generation challenges",
    "text": "Next generation challenges\n\nWhat should statisticians do to avoid being replaced by “data scientists”?\n\n\nIn light of recent developments in AI and large language models (ChatGPT and friends), how should the teaching of Statistics, especially at the bachelor level, evolve? Should we be rethinking what we teach, how we teach it, or both? What about theses and written projects?"
  },
  {
    "objectID": "post/champions/champions2025_esiti.html#comè-andata",
    "href": "post/champions/champions2025_esiti.html#comè-andata",
    "title": "Previsioni Champions League: com’è andata?",
    "section": "Com’è andata?",
    "text": "Com’è andata?\nIeri ho diffuso in un post le previsioni per l’ultima giornata di Champions League, basandomi sulle quotazioni dei bookmakers. Oggi vogliamo controllare com’è andata. Ci sono state previsioni centrate (ad es. Inter - Monaco, Manchester City - Brugge) e altre che invece non si sono avverate (Milan - Dinamo Zagabria)."
  },
  {
    "objectID": "post/champions/champions2025_esiti.html#le-previsioni-principali",
    "href": "post/champions/champions2025_esiti.html#le-previsioni-principali",
    "title": "Previsioni Champions League: com’è andata?",
    "section": "Le previsioni principali",
    "text": "Le previsioni principali\nAnzitutto, facciamo il punto sulle affermazioni fatte in TV o nel mio post.\n\nPrevisione corretta. L’Inter è passata direttamente agli ottavi di finale, classificandosi tra le prime quattro posizioni, come previsto.\nPrevisione errata. Il Milan non è passato agli ottavi, nonostante avesse l’82% di probabilità di farlo.\n\nPrevisione corretta. L’Atalanta è stata effettivamente in bilico (è arrivata 9ª) ed è infine andata ai playoff, come previsto.\n\nPrevisione corretta. La Juventus è andata ai playoff, come ampiamente previsto. Questa però era davvero facile da indovinare.\n\nPrevisione corretta. Il Manchester City è andato ai playoff, come previsto.\n\nEntriamo ora maggiormente nel dettaglio delle singole previsioni."
  },
  {
    "objectID": "post/champions/champions2025_esiti.html#lerrore-commesso-dai-bookmakers",
    "href": "post/champions/champions2025_esiti.html#lerrore-commesso-dai-bookmakers",
    "title": "Previsioni Champions League: com’è andata?",
    "section": "L’errore commesso dai bookmakers",
    "text": "L’errore commesso dai bookmakers\nQuando facciamo una “previsione secca”, cioè indichiamo se l’esito sarà 1, X o 2, ci basiamo sulle probabilità di ciascun risultato. Pertanto, per la i-esima partita la probabilità di indovinare sarà:\n\n\\text{Probabilità di indovinare l'}i\\text{-esima previsione} = \\hat{p}_i = \\max \\{p_{i1}, p_{iX}, p_{i2}\\},\n dove p_{i1}, p_{iX}, p_{i2} sono le probabilità di vittoria, pareggio e sconfitta. Viceversa, la probabilità di sbagliare sarà:\n\n\\text{Probabilità di sbagliare l'}i\\text{-esima previsione} = 1 - \\hat{p}_i.\n\n\nAlcuni esempi\nAd esempio, l’Aston Villa era favorito contro il Celtic FC (ed in effetti ha vinto 4-2) perché il 62% rappresentava una probabilità più alta rispetto al 22% (pareggio) e al 16% (sconfitta).\n\n\n\n\n\n\n\n\n\n\n\n\nSquadra 1\nSquadra 2\n1\nX\n2\nProbabilità di indovinare (\\hat{p}_i)\nProbabilità di sbagliare (1 - \\hat{p}_i)\n\n\n\n\nAston Villa\nCeltic FC\n0.62\n0.22\n0.16\n0.62\n0.38\n\n\nBarcellona\nAtalanta\n0.55\n0.23\n0.22\n0.55\n0.45\n\n\nDinamo Zagabria\nMilan\n0.17\n0.21\n0.62\n0.62\n0.38\n\n\n\n\n\nLe nostre aspettative sugli errori commessi\nSu 18 partite, quindi, ci aspettiamo che il numero medio di errori commessi sia pari alla somma delle probabilità di sbagliare, vale a dire circa:\n\n\\text{Numero medio di errori commessi} = \\sum_{i=1}^{18}(1 - \\hat{p}_i) = 0.38 + 0.45 + 0.48 + \\cdots = 7.17.\n\n\n\n\n\n\n\n\nCome fare per ridurre il numero medio di errori? Una parte di questo errore dipende dalla bravura dei bookmakers. Purtroppo però, oltre una certa soglia, non si può migliorare.\nIl minimo errore di previsione possibile è chiamato “Bayes error rate” ed è ragionevole credere che, nel nostro caso, sia di poco inferiore a 7.17/18 = 40\\%.\nIl Bayes error rate nel calcio è spesso un numero abbastanza alto, perché si tratta di uno sport imprevedibile. Sbagliare, quindi, è inevitabile.\n\n\n\n\n\n\nQuanti errori sono stati effettivamente commessi?\nNella prossima tabella, chiamata matrice di confusione, abbiamo confrontato le previsioni dei bookmakers con i risultati effettivi.\n\n\n\n\n1 (Reale)\nX (Reale)\n2 (Reale)\n\n\n\n\n1 (Previsto)\n7\n2\n1\n\n\nX (Previsto)\n0\n0\n1\n\n\n2 (Previsto)\n3\n0\n4\n\n\n\n\n\n\n\n\n\n\nI bookmakers hanno previsto correttamente l’esito di 7 + 4 = 11 partite su 18, ovvero hanno sbagliato 7 volte su 18.\nTutto è andato esattamente secondo le aspettative: sapevamo che avremmo commesso circa 7 errori, ed è andata proprio così. Uno di questi errori, purtroppo, è stato proprio il Milan."
  },
  {
    "objectID": "post/champions/champions2025_esiti.html#la-previsione-della-classifica",
    "href": "post/champions/champions2025_esiti.html#la-previsione-della-classifica",
    "title": "Previsioni Champions League: com’è andata?",
    "section": "La previsione della classifica",
    "text": "La previsione della classifica\nPrevedere una singola partita è difficile, perché ci sono molti fattori imprevedibili. Tuttavia, prevedere la classifica finale è un compito un po’ più semplice, anche perchè le squadre non partivano dallo stesso punteggio.\nAnche in questo caso, ci sono buone notizie: ci possiamo quindi ritenere ragionevolmente soddisfatti delle nostre previsioni.\n\n\n\n\n\n\n\nIn media, la classifica prevista è più simile a quella finale di quanto non lo fosse quella della 7ª giornata. (I dettagli qui sono omessi).\nQuesto fatto si può quantificare tramite il cosiddetto “errore quadratico medio”, che misura quanto sia diversa la nostra previsione dai risultati reali.\n\n\n\n\nNel seguito ho rappresentato graficamente la classifica alla settima giornata, la previsione ottenuta con il metodo Monte Carlo e, infine, la graduatoria finale."
  },
  {
    "objectID": "post.html",
    "href": "post.html",
    "title": "Blogpost",
    "section": "",
    "text": "ABACO26\n\n\nNonparametric predictive inference for discrete data via Metropolis-adjusted Dirichlet sequences\n\n\nA joint work with Davide Agnoletto and David Dunson presented at the Workshop on Advances in BAyesian COmputation and modeling organized by Emanuele Aliverti at the University of Padova.\n\n\n\n\n\nFeb 6, 2026\n\n\n\n\n\n\n\n\n\n\n\n\nProject presentation\n\n\nPolitecnico di Milano - Mathematical Engineering - Bayesian Statistics\n\n\nPresentation of a project for the course Bayesian Statistics at the Politecnico di Milano\n\n\n\n\n\nSep 26, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nBISP14\n\n\nBayesian inference for generalized linear models via quasi-posteriors\n\n\nA joint work with Davide Agnoletto and David Dunson presented at BISP 14, the 14th workshop on Bayesian Inference for Stochastic processes, organized by CNR-IMATI\n\n\n\n\n\nMay 27, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nLa lotteria nazionale non è difettosa\n\n\n\n\n\n\n\n\n\n\n\nMar 8, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nCome mai non è possibile prevedere i numeri del lotto\n\n\n\n\n\n\n\n\n\n\n\nMar 6, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nPrevisioni Champions League: com’è andata?\n\n\nUna valutazione degli errori\n\n\n\n\n\n\n\n\nJan 30, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nPrevisioni Champions League\n\n\n8a giornata\n\n\n\n\n\n\n\n\nJan 29, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nKarmic dice for Baldur’s Gate III\n\n\nA simple algorithm that accounts for karma\n\n\n\n\n\n\n\n\nAug 20, 2023\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "links.html",
    "href": "links.html",
    "title": "Tommaso Rigon",
    "section": "",
    "text": "Federico Camerlenghi\nRiccardo Cogo, Ph.D. student\nLorenzo Ghilotti, Ph.D. student\nLuca Presicce, Ph.D. student\nCarlo Reverberi"
  },
  {
    "objectID": "links.html#university-of-milano-bicocca",
    "href": "links.html#university-of-milano-bicocca",
    "title": "Tommaso Rigon",
    "section": "",
    "text": "Federico Camerlenghi\nRiccardo Cogo, Ph.D. student\nLorenzo Ghilotti, Ph.D. student\nLuca Presicce, Ph.D. student\nCarlo Reverberi"
  },
  {
    "objectID": "links.html#bocconi-university",
    "href": "links.html#bocconi-university",
    "title": "Tommaso Rigon",
    "section": "Bocconi University",
    "text": "Bocconi University\nDaniele Durante\nAntonio Lijoi, Ph.D. advisor\nSonia Petrone\nIgor Prünster, Ph.D. advisor"
  },
  {
    "objectID": "links.html#duke-university",
    "href": "links.html#duke-university",
    "title": "Tommaso Rigon",
    "section": "Duke University",
    "text": "Duke University\nDavid Dunson, Post-doc advisor\nAmy Herring, Post-doc advisor\nChing-Lung Hsu, Ph.D. student\nAlan Gelfand"
  },
  {
    "objectID": "links.html#university-of-padova",
    "href": "links.html#university-of-padova",
    "title": "Tommaso Rigon",
    "section": "University of Padova",
    "text": "University of Padova\nDavide Agnoletto, Ph.D. student\nEmanuele Aliverti\nAntonio Canale\nFrancesco Denti\nBruno Scarpa, M.Sc. advisor\nAndrea Sottosanti"
  },
  {
    "objectID": "links.html#other-universities",
    "href": "links.html#other-universities",
    "title": "Tommaso Rigon",
    "section": "Other Universities",
    "text": "Other Universities\nAlessia Caponera, Luiss University\nMarta Catalano, Luiss University\nPaolo Cherubini, University of Pavia\nStefano Favaro, University of Turin\nSirio Legramanti, University of Bergamo\nOtso Ovaskainen, University of Helsinki\nSally Paganin, Ohio State University\nFrancesca Panero, Sapienza University\nMassimilano Russo, Ohio State University\nAldo Solari, Ca’ Foscari University\nNicola Torelli, University of Trieste\nAlessandro Zito, Harvard University, Ph.D. student"
  },
  {
    "objectID": "post/BISP2025/BISP2025_slides.html#warm-thanks",
    "href": "post/BISP2025/BISP2025_slides.html#warm-thanks",
    "title": "BISP14",
    "section": "Warm thanks",
    "text": "Warm thanks\n\n\nDavide Agnoletto (Duke University)\n\n\n\n\n\n\n\n\nDavid Dunson (Duke University)"
  },
  {
    "objectID": "post/BISP2025/BISP2025_slides.html#foundations",
    "href": "post/BISP2025/BISP2025_slides.html#foundations",
    "title": "BISP14",
    "section": "Foundations",
    "text": "Foundations\n\nDe Finetti’s representation Theorem (De Finetti 1937) has a central role in Bayesian statistics because it provides the fundamental justification to the two approaches to Bayesian statistics:\n\nthe hypothetical approach;\nthe predictive approach.\n\n\n\n\n\n\nDe Finetti’s representation theorem\n\n\nLet (Y_n)_{n\\ge 1}, Y_n\\in\\mathcal{Y}, be a sequence of exchangeable random variables with probability law P. Then there exists a unique probability measure \\Pi such that, for any n\\ge 1,\n\nP(y_1,\\ldots,y_n) = \\int_{\\mathcal{F}} \\prod_{i=1}^n F(y_i)\\,\\Pi(\\mathrm{d}F).\n\n\n\n\n\n\nWhile representing opposite interpretations of the Theorem, the two approaches are intrinsically connected."
  },
  {
    "objectID": "post/BISP2025/BISP2025_slides.html#hypothetical-approach",
    "href": "post/BISP2025/BISP2025_slides.html#hypothetical-approach",
    "title": "BISP14",
    "section": "Hypothetical approach",
    "text": "Hypothetical approach\n\nThe hypothetical approach represents the the most common way to operate within the Bayesian community.\nIn a parametric setting, \\Pi has support on a class \\Theta\\subseteq\\mathbb{R}^p, p&lt;\\infty, such that \\boldsymbol{\\theta}\\in\\Theta indexes the class of distributions \\mathcal{F}_{\\boldsymbol{\\theta}}=\\{F_{\\boldsymbol{\\theta}} : \\boldsymbol{\\theta} \\in \\Theta\\subseteq\\mathbb{R}^p\\}.\nBayes’ rule takes the well-known formulation: \n\\pi(\\boldsymbol{\\theta}\\mid y_1,\\ldots,y_n) \\propto \\pi(\\boldsymbol{\\theta}) \\prod_{i=1}^n f_{\\boldsymbol{\\theta}}(y_i),\n where \\pi and f_{\\boldsymbol{\\theta}} denote the probability density functions associated with \\Pi and F_{\\boldsymbol{\\theta}}, respectively.\nHowever, when the link between observations and parameter of interest cannot be expressed through a distribution function, the traditional hypothetical approach fails.\n\n\n\nSolution: generalized posterior distributions, sometimes called Gibbs-posteriors.\nThis is a lively recent topic, see for instance: Chernozhukov and Hong (2003); Bissiri et al. (2016) Heide et al. (2020); Grünwald and Mehta (2020); Knoblauch et al. (2022); Matsubara et al. (2022); Matsubara et al. (2023); Jewson and Rossell (2022); Rigon et al. (2023)."
  },
  {
    "objectID": "post/BISP2025/BISP2025_slides.html#generalizations-of-the-hypothetical-approach",
    "href": "post/BISP2025/BISP2025_slides.html#generalizations-of-the-hypothetical-approach",
    "title": "BISP14",
    "section": "Generalizations of the hypothetical approach",
    "text": "Generalizations of the hypothetical approach\n\nBissiri et al. (2016) showed that the generalized posterior \n\\pi_\\omega(\\boldsymbol{\\theta} \\mid \\mathbf{y}_{1:n}) \\propto \\pi(\\boldsymbol{\\theta}) \\exp\\left\\{ - \\omega \\sum_{i=1}^n \\ell(\\boldsymbol{\\theta}; y_i) \\right\\},\n is the only coherent update of the prior beliefs about \n\\boldsymbol{\\theta}^* = \\arg\\min_{\\boldsymbol{\\theta}\\in\\Theta} \\int_{\\mathcal{Y}} \\ell(\\boldsymbol{\\theta}; y)\\, F_0(\\mathrm{d}y),\n where \\ell(\\boldsymbol{\\theta}, y) is a loss function, \\omega is the loss-scale, and F_0 is the unknown true sampling distribution.\n\n\n\nLearning the loss scale \\omega from the data is a delicate task. Assuming a prior for \\omega can lead to degenerate estimates if not accompanied by additional adjustments to the loss function.\nHowever, there are several solutions for its calibration: Holmes and Walker (2017); Lyddon et al. (2019); Syring and Martin (2019); Matsubara et al. (2023).\n\n\n\n\nOur contribution: Bayesian inference for generalized linear models via quasi-posteriors."
  },
  {
    "objectID": "post/BISP2025/BISP2025_slides.html#generalized-bayes-for-glms",
    "href": "post/BISP2025/BISP2025_slides.html#generalized-bayes-for-glms",
    "title": "BISP14",
    "section": "Generalized Bayes for GLMs",
    "text": "Generalized Bayes for GLMs\n\nGeneralized linear models (GLMs) are routinely used to model a wide variety of data.\nThe Bayesian approach for GLMs is also incredibly popular, e.g. because of the possibility of naturally incorporating random effects, complex penalizations, prior information, and more.\nHowever, they often incur misspecification, and this could compromise inferential conclusions.\n\n\n\nA common case is overdispersion, i.e., when proportion or count observations show larger variability than the one assumed by the model.\nTraditional solutions have important drawbacks:\n\nModel-based: may lead to computational bottlenecks and can result again in misspecification.\nNonparametric: increased computational cost and loss of efficiency and interpretability.\n\n\n\n\n\nWe rely on a semi-parametric approach, making only assumptions on the mean and variance of the response while preserving computational tractability."
  },
  {
    "objectID": "post/BISP2025/BISP2025_slides.html#second-order-assumptions",
    "href": "post/BISP2025/BISP2025_slides.html#second-order-assumptions",
    "title": "BISP14",
    "section": "Second order assumptions",
    "text": "Second order assumptions\n\nLet Y_i \\in \\mathcal{Y} denote a response variable, \\boldsymbol{x}_i \\in \\mathbb{R}^p be a vector of covariates for i = 1, \\ldots, n, and \\boldsymbol{\\beta} \\in \\mathbb{R}^p be the parameter of interest.\nStandard GLMs assume that observations y_i are independent realizations of Y_i \\mid \\boldsymbol{x}_i, whose distribution belongs to the exponential dispersion family.\n\n\n\n\n\nWe assume the second-order conditions: \n\\mathbb{E}\\{Y_i\\} = \\mu_i = g^{-1}(\\boldsymbol{x}_i^\\top \\boldsymbol{\\beta}), \\quad\n\\mathrm{var}\\{Y_i\\} = \\psi\\, V(\\mu_i),\n where g(\\cdot) is a link function, V(\\cdot)&gt;0 is a variance function, and \\psi \\in (0,\\infty) is a dispersion parameter.\nWe let (\\boldsymbol{\\beta}_0, \\psi_0) be the true values for the parameters (\\boldsymbol{\\beta}, \\psi) and we assume the data are generated under F_0(\\mathrm{d}y \\mid \\boldsymbol{x}) = F(\\mathrm{d}y \\mid \\boldsymbol{x}, \\boldsymbol{\\beta}_0, \\psi_0).\n\n\n\n\n\n\nAlthough the mean and variance functions need to be carefully chosen to fit the data, the resulting inferences are robust to misspecification of higher-order moments."
  },
  {
    "objectID": "post/BISP2025/BISP2025_slides.html#quasi-likelihood",
    "href": "post/BISP2025/BISP2025_slides.html#quasi-likelihood",
    "title": "BISP14",
    "section": "Quasi-likelihood",
    "text": "Quasi-likelihood\n\nUnder the second-order assumptions, it is possible to specify the so-called log-quasi-likelihood function (Wedderburn 1974): \n\\ell_Q(\\boldsymbol{\\beta}; \\mathbf{y}, \\mathbf{X}, \\psi) = \\sum_{i=1}^n \\ell_Q(\\boldsymbol{\\beta}; y_i, \\boldsymbol{x}_i, \\psi) = \\sum_{i=1}^n \\int_a^{\\mu_i(\\boldsymbol{\\beta})} \\frac{y_i - t}{\\psi V(t)} \\, \\mathrm{d}t,\n where a is an arbitrary constant that does not depend on \\boldsymbol{\\beta}.\nThe above integral can be written in closed form for many choices of variance functions, including those associated with exponential family distributions.\n\n\n\nQuasi-likelihoods retain many properties of genuine likelihoods, such as unbiased estimating equations and the information identity: \n\\mathbb{E}\\left\\{ \\nabla \\ell_Q(\\boldsymbol{\\beta}; \\mathbf{Y}, \\mathbf{X}, \\psi) \\right\\} = 0, \\qquad\n\\mathbb{E}\\left\\{ -\\nabla^2 \\ell_Q(\\boldsymbol{\\beta}; \\mathbf{Y}, \\mathbf{X}, \\psi) \\right\\} =\n\\mathbb{E}\\left\\{ \\nabla \\ell_Q \\nabla \\ell_Q^\\top \\right\\},\n where \\nabla denotes the gradient with respect to \\boldsymbol{\\beta}.\n\n\n\n\nUnder mild conditions, the maximum quasi-likelihood is consistent and has the smallest asymptotic variance among estimators derived from linear estimating equations (McCullagh 1983)."
  },
  {
    "objectID": "post/BISP2025/BISP2025_slides.html#quasi-posteriors-i",
    "href": "post/BISP2025/BISP2025_slides.html#quasi-posteriors-i",
    "title": "BISP14",
    "section": "Quasi-posteriors I",
    "text": "Quasi-posteriors I\n\n\n\nLet \\exp\\{\\ell_Q(\\boldsymbol{\\beta}; \\mathbf{y}, \\mathbf{X}, \\psi)\\} be the quasi-likelihood function and p(\\boldsymbol{\\beta}) be the prior distribution for \\boldsymbol{\\beta}. We define the quasi-posterior distribution for \\boldsymbol{\\beta} as: \np_Q(\\boldsymbol{\\beta} \\mid \\mathbf{y}, \\mathbf{X}, \\psi) \\propto p(\\boldsymbol{\\beta}) \\exp \\left\\{ \\ell_Q(\\boldsymbol{\\beta}; \\mathbf{y}, \\mathbf{X}, \\psi) \\right\\} =\np(\\boldsymbol{\\beta}) \\exp \\left\\{ \\frac{1}{\\psi} \\sum_{i=1}^n \\int_a^{\\mu_i(\\boldsymbol{\\beta})} \\frac{y_i - t}{V(t)} \\, \\mathrm{d}t \\right\\}\n\n\n\n\n\nThe quasi-posterior is a rational update of a belief distribution within the generalized Bayesian framework, with loss function: \n\\ell(\\boldsymbol{\\beta}; \\mathbf{y}, \\mathbf{X}) = - \\sum_{i=1}^n \\int_a^{\\mu_i(\\boldsymbol{\\beta})} \\frac{y_i - t}{V(t)} \\, \\mathrm{d}t.\n\nThe dispersion parameter \\psi plays the role of a loss-scale parameter for the quasi-posterior."
  },
  {
    "objectID": "post/BISP2025/BISP2025_slides.html#quasi-posteriors-ii",
    "href": "post/BISP2025/BISP2025_slides.html#quasi-posteriors-ii",
    "title": "BISP14",
    "section": "Quasi-posteriors II",
    "text": "Quasi-posteriors II\n\nThe quasi-posterior represents subjective uncertainty about the unknown parameter value: \n\\boldsymbol{\\beta}^* = \\arg\\min_{\\boldsymbol{\\beta}} \\int_{\\mathcal{Y}} \\ell(\\boldsymbol{\\beta}; \\mathbf{y}, \\mathbf{X}) \\, F_0(d\\mathbf{y} \\mid \\mathbf{X}),\n which is assumed to be unique (Bissiri et al. 2016).\nThe definition of \\boldsymbol{\\beta}^* does not automatically guarantee that \\boldsymbol{\\beta}^* = \\boldsymbol{\\beta}_0.\n\n\n\n\n\nTheorem (Agnoletto, R., Dunson, 2025)\n\n\nAssume the second-order conditions are well-specified, and suppose the target of inference \\boldsymbol{\\beta}^* is unique. Then, for quasi-posteriors, \\boldsymbol{\\beta}^* must coincide with the true value \\boldsymbol{\\beta}_0: \n\\boldsymbol{\\beta}^* = \\arg\\min_{\\boldsymbol{\\beta}} \\int_{\\mathcal{Y}}\n\\sum_{i=1}^n \\int_a^{\\mu_i(\\boldsymbol{\\beta})} \\frac{t - y_i}{V(t)} \\, \\mathrm{d}t \\, F_0(d\\mathbf{y} \\mid \\mathbf{X}) = \\boldsymbol{\\beta}_0."
  },
  {
    "objectID": "post/BISP2025/BISP2025_slides.html#calibration-of-the-dispersion-i",
    "href": "post/BISP2025/BISP2025_slides.html#calibration-of-the-dispersion-i",
    "title": "BISP14",
    "section": "Calibration of the dispersion I",
    "text": "Calibration of the dispersion I\n\nBased on a comparison with the Bayesian bootstrap, Lyddon et al. (2019) propose calibrate the dispersion \\psi setting it equal to: \n\\psi_{\\text{LLB}} =\n\\frac{\\mathrm{tr}\\{j(\\boldsymbol{\\beta}_0)\\}}{\\mathrm{tr}\\{j(\\boldsymbol{\\beta}_0) h(\\boldsymbol{\\beta}_0)^{-1} j(\\boldsymbol{\\beta}_0)\\}},\n where we define \nj(\\boldsymbol{\\beta}) := \\lim_{n \\to \\infty} \\frac{1}{n} \\mathbb{E}\\left[\\nabla^2 \\ell(\\boldsymbol{\\beta}; \\mathbf{Y}, \\mathbf{X})\\right], \\qquad  h(\\boldsymbol{\\beta}) := \\lim_{n \\to \\infty} \\frac{1}{n} \\mathbb{E}\\left[\\nabla \\ell(\\boldsymbol{\\beta}; \\mathbf{Y}, \\mathbf{X}) \\nabla \\ell(\\boldsymbol{\\beta}; \\mathbf{Y}, \\mathbf{X})^\\top\\right].\n\n\n\n\n\nProposition (Agnoletto, R., Dunson, 2025)\n\n\nUnder the second order conditions, namely if \\mathbb{E}(Y_i) = g^{-1}(\\mathbf{x}_i^\\top \\boldsymbol{\\beta}_0) and \\mathrm{var}(Y_i) = \\psi_0 V\\{\\mu_i(\\boldsymbol{\\beta}_0)\\}, then for quasi posteriors with loss \\ell(\\boldsymbol{\\beta}; \\mathbf{y}, \\mathbf{X}) = -\\sum_{i=1}^n \\int_a^{\\mu_i(\\boldsymbol{\\beta})} \\frac{y_i - t}{V(t)} \\mathrm{d}t, we have \n\\psi_{\\text{LLB}} = \\psi_0."
  },
  {
    "objectID": "post/BISP2025/BISP2025_slides.html#calibration-of-the-dispersion-ii",
    "href": "post/BISP2025/BISP2025_slides.html#calibration-of-the-dispersion-ii",
    "title": "BISP14",
    "section": "Calibration of the dispersion II",
    "text": "Calibration of the dispersion II\n\n\n\nTheorem (Agnoletto, R., Dunson, 2025)\n\n\nAssume the second-order conditions are correctly specified. Let S_1, S_2, \\ldots \\subseteq \\mathbb{R}^p be a sequence of convex credible sets of level \\rho \\in (0,1). Then, under mild conditions and setting \\psi = \\psi_0: \n\\mathbb{P}(\\boldsymbol{\\beta}_0 \\in S_n \\mid \\mathbf{y}, \\mathbf{X}, \\psi_0) \\to \\rho \\quad \\text{as } n \\to \\infty.\n\n\n\n\n\nAs \\psi_0 is typically unknown, we can use the classical method of moments estimator: \n\\widehat{\\psi} = \\frac{1}{n - p} \\sum_{i=1}^n \\frac{(y_i - \\widehat{\\mu}_i)^2}{V(\\widehat{\\mu}_i)},\n where \\widehat{\\mu}_i = \\mu_i(\\widehat{\\boldsymbol{\\beta}}), which is fast and consistent (McCullagh and Nelder 1989)."
  },
  {
    "objectID": "post/BISP2025/BISP2025_slides.html#simulation-results-i",
    "href": "post/BISP2025/BISP2025_slides.html#simulation-results-i",
    "title": "BISP14",
    "section": "Simulation results I",
    "text": "Simulation results I\n\nData are generated from a distribution with \\mathbb{E}(Y_i) = \\mu_i(\\boldsymbol{\\beta}_0)= \\exp(\\mathbf{x}_i^\\top \\boldsymbol{\\beta}_0) and \\mathrm{var}(Y_i) = \\psi_0 \\mu_i(\\boldsymbol{\\beta}_0) —not a Poisson!— with parameters \\beta_0 = (3.5,\\; 1.5,\\; -1.0,\\; 0.5), and \\psi_0 = 3.5.\nWe computed posterior means and 95\\% credible intervals.\nThe sample size is n = 100; estimates are averages over multiple simulated datasets.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPoisson\nNegative Binomial\nDFD-Bayes\nQuasi-posterior\nQuasi-posterior + alternative \\hat{\\psi}\n\n\n\n\n\\beta_1\nMean\n3.50 (0.035)\n3.49 (0.047)\n57.46 (105.41)\n3.50 (0.035)\n3.50 (0.035)\n\n\n\nCover.\n0.715\n0.920\n0.201\n0.945\n0.970\n\n\n\\beta_2\nMean\n1.50 (0.020)\n1.51 (0.040)\n5.26 (6.31)\n1.50 (0.020)\n1.50 (0.020)\n\n\n\nCover.\n0.675\n0.960\n0.454\n0.945\n0.990\n\n\n\\beta_3\nMean\n-1.00 (0.020)\n-1.01 (0.034)\n-3.98 (6.53)\n-1.00 (0.020)\n-1.00 (0.020)\n\n\n\nCover.\n0.715\n0.995\n0.479\n0.950\n0.965\n\n\n\\beta_4\nMean\n0.50 (0.018)\n0.50 (0.037)\n2.55 (7.37)\n0.50 (0.018)\n0.50 (0.018)\n\n\n\nCover.\n0.655\n0.965\n0.526\n0.950\n0.970"
  },
  {
    "objectID": "post/BISP2025/BISP2025_slides.html#simulation-results-ii",
    "href": "post/BISP2025/BISP2025_slides.html#simulation-results-ii",
    "title": "BISP14",
    "section": "Simulation results II",
    "text": "Simulation results II\n\nData are generated from a distribution with \\mathbb{E}(Y_i) = \\mu_i(\\boldsymbol{\\beta}_0)= \\exp(\\mathbf{x}_i^\\top \\boldsymbol{\\beta}_0) and \\mathrm{var}(Y_i) = \\psi_0 \\mu_i(\\boldsymbol{\\beta}_0) —not a Poisson!— with parameters \\beta_0 = (3.5,\\; 1.5,\\; -1.0,\\; 0.5), and \\psi_0 = 3.5.\nWe computed posterior means and 95\\% credible intervals.\nThe sample size is n = 1000; estimates are averages over multiple simulated datasets.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPoisson\nNegative Binomial\nDFD-Bayes\nQuasi-posterior\nQuasi-posterior + alternative \\hat{\\psi}\n\n\n\n\n\\beta_1\nMean\n3.50 (0.010)\n3.50 (0.015)\n4.64 (8.45)\n3.50 (0.010)\n3.50 (0.010)\n\n\n\nCover.\n0.690\n0.835\n0.070\n0.945\n0.955\n\n\n\\beta_2\nMean\n1.50 (0.005)\n1.50 (0.012)\n1.87 (1.37)\n1.50 (0.005)\n1.50 (0.005)\n\n\n\nCover.\n0.665\n0.910\n0.510\n0.950\n0.960\n\n\n\\beta_3\nMean\n-1.00 (0.005)\n-1.00 (0.010)\n-1.21 (0.81)\n-1.00 (0.005)\n-1.00 (0.005)\n\n\n\nCover.\n0.680\n0.960\n0.690\n0.955\n0.950\n\n\n\\beta_4\nMean\n0.50 (0.005)\n0.50 (0.009)\n0.54 (0.35)\n0.50 (0.005)\n0.50 (0.005)\n\n\n\nCover.\n0.715\n0.950\n0.810\n0.950\n0.940"
  },
  {
    "objectID": "post/BISP2025/BISP2025_slides.html#thank-you",
    "href": "post/BISP2025/BISP2025_slides.html#thank-you",
    "title": "BISP14",
    "section": "Thank you!",
    "text": "Thank you!\n\nThe main paper is:\nAgnoletto, D., Rigon, T., and Dunson D.B. (2025+). Bayesian inference for generalized linear models via quasi-posteriors. Biometrika, to appear."
  },
  {
    "objectID": "post/BISP2025/BISP2025_slides.html#references",
    "href": "post/BISP2025/BISP2025_slides.html#references",
    "title": "BISP14",
    "section": "References",
    "text": "References\n\n\nBissiri, P. G., Holmes, C. C., and Walker, S. G. (2016), “A general framework for updating belief distributions,” Journal of the Royal Statistical Society: Series B (Statistical Methodology), Wiley Online Library, 78, 1103–1130.\n\n\nChernozhukov, V., and Hong, H. (2003), “An MCMC approach to classical estimation,” Journal of econometrics, Elsevier, 115, 293–346.\n\n\nDe Finetti, B. (1937), “La prévision: Ses lois logiques, ses sources subjectives,” in Annales de l’institut henri poincaré, pp. 1–68.\n\n\nGrünwald, P. D., and Mehta, N. A. (2020), “Fast rates for general unbounded loss functions: from ERM to generalized Bayes,” The Journal of Machine Learning Research, JMLRORG, 21, 2040–2119.\n\n\nHeide, R. de, Kirichenko, A., Grunwald, P., and Mehta, N. (2020), “Safe-bayesian generalized linear regression,” in Proceedings of the twenty third international conference on artificial intelligence and statistics, PMLR, pp. 2623–2633.\n\n\nHolmes, C. C., and Walker, S. G. (2017), “Assigning a value to a power likelihood in a general Bayesian model,” Biometrika, Oxford University Press, 104, 497–503.\n\n\nJewson, J., and Rossell, D. (2022), “General bayesian loss function selection and the use of improper models,” Journal of the Royal Statistical Society Series B: Statistical Methodology, Oxford University Press, 84, 1640–1665.\n\n\nKnoblauch, J., Jewson, J., and Damoulas, T. (2022), “An optimization-centric view on bayes’ rule: Reviewing and generalizing variational inference,” Journal of Machine Learning Research, 23, 1–109.\n\n\nLyddon, S. P., Holmes, C. C., and Walker, S. G. (2019), “General Bayesian updating and the loss-likelihood bootstrap,” Biometrika, Oxford University Press, 106, 465–478.\n\n\nMatsubara, T., Knoblauch, J., Briol, F.-X., and Oates, C. J. (2022), “Robust generalised bayesian inference for intractable likelihoods,” Journal of the Royal Statistical Society Series B: Statistical Methodology, Oxford University Press, 84, 997–1022.\n\n\nMatsubara, T., Knoblauch, J., Briol, F.-X., and Oates, C. J. (2023), “Generalized bayesian inference for discrete intractable likelihood,” Journal of the American Statistical Association, Taylor & Francis, 1–11.\n\n\nMcCullagh, P. (1983), “Quasi-likelihood functions,” Annals of Statistics, Institute of Mathematical Statistics, 11, 59–67.\n\n\nMcCullagh, P., and Nelder, J. A. (1989), Generalized linear models, Chapman & Hall.\n\n\nRigon, T., Herring, A. H., and Dunson, D. B. (2023), “A generalized Bayes framework for probabilistic clustering,” Biometrika, Oxford University Press, 10, 559–578.\n\n\nSyring, N., and Martin, R. (2019), “Calibrating general posterior credible regions,” Biometrika, Oxford University Press, 106, 479–486.\n\n\nWedderburn, R. W. (1974), “Quasi-likelihood functions, generalized linear models, and the Gauss-Newton method,” Biometrika, Oxford University Press, 61, 439–447."
  },
  {
    "objectID": "post/Statalk2025/Statalk2025_slides.html#panelists",
    "href": "post/Statalk2025/Statalk2025_slides.html#panelists",
    "title": "StaTalk 2025",
    "section": "Panelists",
    "text": "Panelists\n\n\n\n\n\n\n\nAlessandra Guglielmi\n(PoliMi)\n\n\n\n\n\n\n\n\nAlessandro Fassò\n(UniBg)\n\n\n\n\n\n\n\n\nMaria Grazia Valsecchi\n(Unimib)"
  },
  {
    "objectID": "post/Statalk2025/Statalk2025_slides.html#phd-life-over-time",
    "href": "post/Statalk2025/Statalk2025_slides.html#phd-life-over-time",
    "title": "StaTalk 2025",
    "section": "PhD life over time",
    "text": "PhD life over time\n\nHow did you end up here? Was it a grand plan, a happy accident, or the result of saying “yes” one too many times? We would love to hear any anecdotes about what first sparked your interest in your current research area.\n\n\n\nHow has academia changed at the PhD and postdoc levels compared to when you went through it? Are things better, worse, or just… different with more Zoom?"
  },
  {
    "objectID": "post/Statalk2025/Statalk2025_slides.html#seeking-wisdom",
    "href": "post/Statalk2025/Statalk2025_slides.html#seeking-wisdom",
    "title": "StaTalk 2025",
    "section": "Seeking wisdom",
    "text": "Seeking wisdom\n\nWhat is your advice for a young investigator? Are there common traps you have seen in your work with students?\n\n\n\nWhat do you think about the often-repeated quip: “publish or perish”? Is it a painful truth or an exaggeration?\n\n\n\n\nWe all have times when we feel daunted by the mathematical challenges associated with our work. How do you respond to feelings of inadequacy or intimidation in the face of difficult problems?\n\n\n\n\nWhen is it okay to abandon a project? Not every idea is a brilliant one, and some projects slowly (or quickly) reveal themselves to be dead ends. How do you decide when to pull the plug, both as a junior researcher and, hopefully one day, as a senior one?"
  },
  {
    "objectID": "post/Statalk2025/Statalk2025_slides.html#the-mentor-the-mentee-and-their-delicate-dance",
    "href": "post/Statalk2025/Statalk2025_slides.html#the-mentor-the-mentee-and-their-delicate-dance",
    "title": "StaTalk 2025",
    "section": "The mentor, the mentee, and their delicate dance",
    "text": "The mentor, the mentee, and their delicate dance\n\nWhat are the qualities of a good mentor? In your view, what makes someone a great supervisor or mentor?\nWhat advice do you have for balancing criticism and support as a mentor? How do you cultivate a supporting but rigorous academic environment?\n\n\n\nWhat are some methods for establishing independence from your advisor(s) as your progress in your PhD?\n\n\n\n\nWhat role has community played in your career and in your professional successes? Any anecdotes about relationships formed early in your career that you maintain today?"
  },
  {
    "objectID": "post/Statalk2025/Statalk2025_slides.html#next-generation-challenges",
    "href": "post/Statalk2025/Statalk2025_slides.html#next-generation-challenges",
    "title": "StaTalk 2025",
    "section": "Next generation challenges",
    "text": "Next generation challenges\n\nWhat should statisticians do to avoid being replaced by “data scientists”?\n\n\n\nIn light of recent developments in AI and large language models (ChatGPT and friends), how should the teaching of Statistics, especially at the bachelor level, evolve? Should we be rethinking what we teach, how we teach it, or both? What about theses and written projects?"
  },
  {
    "objectID": "post/SISBayes/SISBayes2025.html",
    "href": "post/SISBayes/SISBayes2025.html",
    "title": "A Bayesian theory for estimation of biodiversity",
    "section": "",
    "text": "Ching-Lung Hsu\n(Duke University)\n\n\n\n\n\n\n\n\nAlessandro Zito\n(Harvard School of Public Health)\n\n\n\n\n\n\n\n\nDavid Dunson\n(Duke University)"
  },
  {
    "objectID": "post/SISBayes/SISBayes2025.html#warm-thanks",
    "href": "post/SISBayes/SISBayes2025.html#warm-thanks",
    "title": "A Bayesian theory for estimation of biodiversity",
    "section": "",
    "text": "Ching-Lung Hsu\n(Duke University)\n\n\n\n\n\n\n\n\nAlessandro Zito\n(Harvard School of Public Health)\n\n\n\n\n\n\n\n\nDavid Dunson\n(Duke University)"
  },
  {
    "objectID": "post/SISBayes/SISBayes2025.html#a-journey-into-statistical-biodiversity",
    "href": "post/SISBayes/SISBayes2025.html#a-journey-into-statistical-biodiversity",
    "title": "A Bayesian theory for estimation of biodiversity",
    "section": "A journey into statistical biodiversity",
    "text": "A journey into statistical biodiversity"
  },
  {
    "objectID": "post/SISBayes/SISBayes2025.html#approximately-80-years-ago-fisher_1943",
    "href": "post/SISBayes/SISBayes2025.html#approximately-80-years-ago-fisher_1943",
    "title": "SIS Bayes 2025",
    "section": "Approximately 80 years ago… Fisher et al. (1943)",
    "text": "Approximately 80 years ago… Fisher et al. (1943)"
  },
  {
    "objectID": "post/SISBayes/SISBayes2025.html#the-first-predictor-for-richness-good_1953-good_toulmin_1956",
    "href": "post/SISBayes/SISBayes2025.html#the-first-predictor-for-richness-good_1953-good_toulmin_1956",
    "title": "SIS Bayes 2025",
    "section": "The first predictor for richness (Good 1953; Good and Toulmin 1956)",
    "text": "The first predictor for richness (Good 1953; Good and Toulmin 1956)"
  },
  {
    "objectID": "post/SISBayes/SISBayes2025.html#the-ewens-sampling-formula-ewens72",
    "href": "post/SISBayes/SISBayes2025.html#the-ewens-sampling-formula-ewens72",
    "title": "A Bayesian theory for estimation of biodiversity",
    "section": "The Ewens sampling formula Ewens (1972)",
    "text": "The Ewens sampling formula Ewens (1972)"
  },
  {
    "objectID": "post/SISBayes/SISBayes2025.html#section",
    "href": "post/SISBayes/SISBayes2025.html#section",
    "title": "A Bayesian theory for estimation of biodiversity",
    "section": "",
    "text": "%\n\\end{document}"
  },
  {
    "objectID": "post/SISBayes/SISBayes2025.html#references",
    "href": "post/SISBayes/SISBayes2025.html#references",
    "title": "A Bayesian theory for estimation of biodiversity",
    "section": "References",
    "text": "References\n\n\n\nCamerlenghi, F., Lijoi, A., Orbanz, P., and Prünster, I. (2019), “Distribution theory for hierarchical processes,” Ann. Statist., 47, 67–92.\n\n\nConley, T. G. (1999), “GMM estimation with cross sectional dependence,” Journal of Econometrics, 92, 1–45.\n\n\nDe Blasi, P., Favaro, S., Lijoi, A., Mena, R. H., Prunster, I., and Ruggiero, M. (2015), “Are Gibbs-type priors the most natural generalization of the Dirichlet process?” IEEE Trans. Pattern Anal. Mach. Intell., IEEE, 37, 212–229.\n\n\nEwens, W. (1972), “The sampling theory of selectively neutral alleles,” Theoretical Population Biology, 3, 87–112.\n\n\nFerguson, T. S. (1973), “A Bayesian analysis of some nonparametric problems,” Ann. Statist., 1, 209–230.\n\n\nFisher, R. A., Corbet, A. S., and Williams, C. B. (1943), “The relation between the number of species and the number of individuals in a random sample of an animal population,” J. Anim. Ecol., 12, 42–58.\n\n\nFranzolini, B., Lijoi, A., Prünster, I., and Rebaudo, G. (2025), “Multivariate species sampling models,” arXiv:2503.24004.\n\n\nGhilotti, L., Camerlenghi, F., and Rigon, T. (2025), “Bayesian analysis of product feature allocation models,” Journal of the Royal Statistical Society: Series B (Statistical Methodology).\n\n\nGnedin, A. (2010), “Species sampling model with finitely many types,” Electron. Comm. Prob., 15, 79–88.\n\n\nGnedin, A., and Pitman, J. (2005), “Exchangeable Gibbs partitions and Stirling triangles,” Zapiski Nauchnykh Seminarov, POMI, 325, 83–102.\n\n\nGood, I. J. (1953), “The population frequencies of species and the estimation of population parameters,” Biometrika, 40, 237–264.\n\n\nGood, I. J., and Toulmin, G. H. (1956), “The number of new species, and the increase in population coverage, when a sample is increased,” Biometrika, 43, 45–63.\n\n\nHubbell, S. P. (2001), The unified neutral theory of biodiversity and biogeography, Princeton University Press.\n\n\nHubbell, S. P., He, F., Condit, R., Borda-de-Água, L., Kellner, J., and Ter Steege, H. (2008), “How many tree species are there in the Amazon and how many of them will go extinct?” Proceedings of the National Academy of Sciences of the United States of America, 105, 11498–11504.\n\n\nLijoi, A., Mena, R. H., and Prünster, I. (2007a), “Bayesian nonparametric estimation of the probability of discovering new species,” Biometrika, Oxford University Press, 94, 769–786.\n\n\nLijoi, A., Mena, R. H., and Prünster, I. (2007b), “Controlling the reinforcement in Bayesian non-parametric mixture models,” J. R. Statist. Soc. B, Wiley Online Library, 69, 715–740.\n\n\nLijoi, A., Prünster, I., and Rigon, T. (2020), “The Pitman–Yor multinomial process for mixture modeling,” Biometrika, 107, 891–906.\n\n\nMcCullagh, P. (2016), “Two Early Contributions to the Ewens Saga,” Statist. Sci., 31, 23–26.\n\n\nPitman, J. (1996), “Some developments of the Blackwell-MacQueen urn scheme,” in Statistics, probability and game theory, IMS lecture notes monogr. ser., Inst. Math. Statist., Hayward, CA, pp. 245–267.\n\n\nPitman, J. (2003), “Poisson-Kingman partitions,” Lecture Notes-Monograph Series, 40, 1–34.\n\n\nPitman, J., and Yor, M. (1997), “The two-parameter Poisson-Dirichlet distribution derived from a stable subordinator,” Ann. Probab., 25, 855–900.\n\n\nRigon, T., Hsu, C., and B., D. D. (2025a), “A Bayesian theory for estimation of biodiversity,” arXiv:2502.01333.\n\n\nRigon, T., Petrone, S., and Scarpa, B. (2025b), “Enriched Pitman-Yor processes,” Scand. J. Statist.\n\n\nZito, A., Rigon, T., and Dunson, D. B. (2023), “Inferring taxonomic placement from DNA barcoding allowing discovery of new taxa,” Meth. Ecol. Evol., 14, 529–542.\n\n\nZito, A., Rigon, T., and Dunson, D. B. (2024), “Bayesian nonparametric modeling of latent partitions via Stirling-gamma priors,” Bayesian Analysis."
  },
  {
    "objectID": "post/SISBayes/SISBayes2025_slides.html#warm-thanks",
    "href": "post/SISBayes/SISBayes2025_slides.html#warm-thanks",
    "title": "A Bayesian theory for estimation of biodiversity",
    "section": "Warm thanks",
    "text": "Warm thanks\n\n\n\n\n\n\n\nChing-Lung Hsu\n(Duke University)\n\n\n\n\n\n\n\n\nAlessandro Zito\n(Harvard School of Public Health)\n\n\n\n\n\n\n\n\nDavid Dunson\n(Duke University)"
  },
  {
    "objectID": "post/SISBayes/SISBayes2025_slides.html#a-journey-into-statistical-biodiversity",
    "href": "post/SISBayes/SISBayes2025_slides.html#a-journey-into-statistical-biodiversity",
    "title": "A Bayesian theory for estimation of biodiversity",
    "section": "A journey into statistical biodiversity",
    "text": "A journey into statistical biodiversity"
  },
  {
    "objectID": "post/SISBayes/SISBayes2025_slides.html#approximately-80-years-ago-fisher_1943",
    "href": "post/SISBayes/SISBayes2025_slides.html#approximately-80-years-ago-fisher_1943",
    "title": "A Bayesian theory for estimation of biodiversity",
    "section": "Approximately 80 years ago… Fisher et al. (1943)",
    "text": "Approximately 80 years ago… Fisher et al. (1943)"
  },
  {
    "objectID": "post/SISBayes/SISBayes2025_slides.html#the-first-predictor-for-richness-good_1953-good_toulmin_1956",
    "href": "post/SISBayes/SISBayes2025_slides.html#the-first-predictor-for-richness-good_1953-good_toulmin_1956",
    "title": "SIS Bayes 2025",
    "section": "The first predictor for richness (Good 1953; Good and Toulmin 1956)",
    "text": "The first predictor for richness (Good 1953; Good and Toulmin 1956)"
  },
  {
    "objectID": "post/SISBayes/SISBayes2025_slides.html#the-ewens-sampling-formula-ewens72",
    "href": "post/SISBayes/SISBayes2025_slides.html#the-ewens-sampling-formula-ewens72",
    "title": "A Bayesian theory for estimation of biodiversity",
    "section": "The Ewens sampling formula Ewens (1972)",
    "text": "The Ewens sampling formula Ewens (1972)"
  },
  {
    "objectID": "post/SISBayes/SISBayes2025_slides.html#section",
    "href": "post/SISBayes/SISBayes2025_slides.html#section",
    "title": "A Bayesian theory for estimation of biodiversity",
    "section": "",
    "text": "\\end{document}"
  },
  {
    "objectID": "post/SISBayes/SISBayes2025_slides.html#references",
    "href": "post/SISBayes/SISBayes2025_slides.html#references",
    "title": "A Bayesian theory for estimation of biodiversity",
    "section": "References",
    "text": "References\n\n\n\nCamerlenghi, F., Lijoi, A., Orbanz, P., and Prünster, I. (2019), “Distribution theory for hierarchical processes,” Ann. Statist., 47, 67–92.\n\n\nDe Blasi, P., Favaro, S., Lijoi, A., Mena, R. H., Prunster, I., and Ruggiero, M. (2015), “Are Gibbs-type priors the most natural generalization of the Dirichlet process?” IEEE Trans. Pattern Anal. Mach. Intell., IEEE, 37, 212–229.\n\n\nEwens, W. (1972), “The sampling theory of selectively neutral alleles,” Theoretical Population Biology, 3, 87–112.\n\n\nFerguson, T. S. (1973), “A Bayesian analysis of some nonparametric problems,” Ann. Statist., 1, 209–230.\n\n\nFisher, R. A., Corbet, A. S., and Williams, C. B. (1943), “The relation between the number of species and the number of individuals in a random sample of an animal population,” J. Anim. Ecol., 12, 42–58.\n\n\nFranzolini, B., Lijoi, A., Prünster, I., and Rebaudo, G. (2025), “Multivariate species sampling models,” arXiv:2503.24004.\n\n\nGhilotti, L., Camerlenghi, F., and Rigon, T. (2025), “Bayesian analysis of product feature allocation models,” Journal of the Royal Statistical Society: Series B (Statistical Methodology).\n\n\nGnedin, A. (2010), “Species sampling model with finitely many types,” Electron. Comm. Prob., 15, 79–88.\n\n\nGnedin, A., and Pitman, J. (2005), “Exchangeable Gibbs partitions and Stirling triangles,” Zapiski Nauchnykh Seminarov, POMI, 325, 83–102.\n\n\nHubbell, S. P. (2001), The unified neutral theory of biodiversity and biogeography, Princeton University Press.\n\n\nLijoi, A., Mena, R. H., and Prünster, I. (2007a), “Bayesian nonparametric estimation of the probability of discovering new species,” Biometrika, Oxford University Press, 94, 769–786.\n\n\nLijoi, A., Mena, R. H., and Prünster, I. (2007b), “Controlling the reinforcement in Bayesian non-parametric mixture models,” J. R. Statist. Soc. B, Wiley Online Library, 69, 715–740.\n\n\nLijoi, A., Prünster, I., and Rigon, T. (2020), “The Pitman–Yor multinomial process for mixture modeling,” Biometrika, 107, 891–906.\n\n\nMcCullagh, P. (2016), “Two Early Contributions to the Ewens Saga,” Statist. Sci., 31, 23–26.\n\n\nPitman, J. (1996), “Some developments of the Blackwell-MacQueen urn scheme,” in Statistics, probability and game theory, IMS lecture notes monogr. ser., Inst. Math. Statist., Hayward, CA, pp. 245–267.\n\n\nPitman, J. (2003), “Poisson-Kingman partitions,” Lecture Notes-Monograph Series, 40, 1–34.\n\n\nPitman, J., and Yor, M. (1997), “The two-parameter Poisson-Dirichlet distribution derived from a stable subordinator,” Ann. Probab., 25, 855–900.\n\n\nRigon, T., Hsu, C., and B., D. D. (2025a), “A Bayesian theory for estimation of biodiversity,” arXiv:2502.01333.\n\n\nRigon, T., Petrone, S., and Scarpa, B. (2025b), “Enriched Pitman-Yor processes,” Scand. J. Statist.\n\n\nZito, A., Rigon, T., and Dunson, D. B. (2023), “Inferring taxonomic placement from DNA barcoding allowing discovery of new taxa,” Meth. Ecol. Evol., 14, 529–542."
  },
  {
    "objectID": "post/SISBayes/SISBayes2025_slides.html#the-dirichlet-process-ferguson1973",
    "href": "post/SISBayes/SISBayes2025_slides.html#the-dirichlet-process-ferguson1973",
    "title": "A Bayesian theory for estimation of biodiversity",
    "section": "The Dirichlet process (Ferguson 1973)",
    "text": "The Dirichlet process (Ferguson 1973)"
  },
  {
    "objectID": "post/SISBayes/SISBayes2025_slides.html#and-species-sampling-models-pitman1996",
    "href": "post/SISBayes/SISBayes2025_slides.html#and-species-sampling-models-pitman1996",
    "title": "A Bayesian theory for estimation of biodiversity",
    "section": "… and species sampling models (Pitman 1996)",
    "text": "… and species sampling models (Pitman 1996)"
  },
  {
    "objectID": "post/SISBayes/SISBayes2025_slides.html#ewens-formula-governs-the-neutral-theory-hubbell_2001",
    "href": "post/SISBayes/SISBayes2025_slides.html#ewens-formula-governs-the-neutral-theory-hubbell_2001",
    "title": "A Bayesian theory for estimation of biodiversity",
    "section": "Ewens formula governs the neutral theory! (Hubbell 2001)",
    "text": "Ewens formula governs the neutral theory! (Hubbell 2001)"
  },
  {
    "objectID": "post/SISBayes/SISBayes2025_slides.html#all-ties-together-mccullagh_2016",
    "href": "post/SISBayes/SISBayes2025_slides.html#all-ties-together-mccullagh_2016",
    "title": "A Bayesian theory for estimation of biodiversity",
    "section": "All ties together (McCullagh 2016)",
    "text": "All ties together (McCullagh 2016)"
  },
  {
    "objectID": "post/SISBayes/SISBayes2025_slides.html#section-1",
    "href": "post/SISBayes/SISBayes2025_slides.html#section-1",
    "title": "SIS Bayes 2025",
    "section": "",
    "text": "%\n\\end{document}"
  },
  {
    "objectID": "post/SISBayes/SISBayes2025_slides.html#the-first-predictor-for-richness",
    "href": "post/SISBayes/SISBayes2025_slides.html#the-first-predictor-for-richness",
    "title": "A Bayesian theory for estimation of biodiversity",
    "section": "The first predictor for richness",
    "text": "The first predictor for richness\n\n(Good 1953; Good and Toulmin 1956)"
  },
  {
    "objectID": "post/SISBayes/SISBayes2025_slides.html#early-ideas-good_1953-good_toulmin_1956",
    "href": "post/SISBayes/SISBayes2025_slides.html#early-ideas-good_1953-good_toulmin_1956",
    "title": "A Bayesian theory for estimation of biodiversity",
    "section": "Early ideas (Good 1953; Good and Toulmin 1956)",
    "text": "Early ideas (Good 1953; Good and Toulmin 1956)"
  },
  {
    "objectID": "post/SISBayes/SISBayes2025_slides.html#species-sampling-models",
    "href": "post/SISBayes/SISBayes2025_slides.html#species-sampling-models",
    "title": "A Bayesian theory for estimation of biodiversity",
    "section": "Species sampling models",
    "text": "Species sampling models\n\nLet X_1,\\dots,X_n be some collection of species or taxa. Suppose X_n are conditionally iid samples from a species sampling model: \n(X_n \\mid \\tilde{p}) \\overset{\\text{iid}}{\\sim} \\tilde{p}, \\qquad\n\\tilde{p}(\\cdot) = \\sum_{h=1}^{\\infty}\\pi_h \\delta_{Z_h}(\\cdot),\n\\qquad n \\ge 1,\n where (\\pi_h)_{h \\ge 1} is a set of probabilities (species proportions) and Z_h represent distinct species.\nThe discreteness of \\tilde{p} identifies Y^{(n)}=y distinct taxa X_1^*, \\ldots, X_y^* with frequencies n_1, \\ldots, n_y, called abundances in ecology.\nGibbs-type priors have emerged as the most natural extension of the DP (De Blasi et al. 2015).\nThe predictive distribution of a Gibbs-type prior is given by: \n\\mathbb{P}(X_{n+1} \\in \\cdot \\mid X_1,\\dots,X_n)\n= \\frac{V_{n+1, y+1}}{V_{n,y}}P(\\cdot)\n+ \\frac{V_{n+1, y}}{V_{n,y}}\\sum_{j=1}^y(n_j - \\sigma)\\delta_{X^*_j}(\\cdot).\n where (a)_{n} denotes a rising factorial, and \\sigma &lt; 1 is the discount parameter. The V_{n,y}’s are non-negative weights satisfying a forward recursive equation."
  },
  {
    "objectID": "post/SISBayes/SISBayes2025_slides.html#gibbs-type-priors",
    "href": "post/SISBayes/SISBayes2025_slides.html#gibbs-type-priors",
    "title": "A Bayesian theory for estimation of biodiversity",
    "section": "Gibbs-type priors",
    "text": "Gibbs-type priors\n\nHubbell’s theory generated considerable controversy. It could be incompatible with real data and it disregards important ecological mechanisms.\nGibbs-type priors have emerged as the most natural extension of the DP (De Blasi et al., 2015).\nA species sampling model \\tilde{p} is a Gibbs process if the law of the random partition it induces, called a Gibbs partition, is such that \n\\Pi_n\\left(n_{1},\\dots,n_{k}\\right)\n= \\mathds{P}\\!\\left(\\Psi_n = \\{C_1,\\dots,C_k\\}\\right)\n= V_{n,k}\\prod_{j=1}^{k}(1-\\sigma)_{n_j-1},\n where (a)_{n} denotes a rising factorial, and \\sigma &lt; 1 is the discount parameter. The V_{n,k}’s are non-negative weights satisfying a forward recursive equation.\nThe predictive distribution of a Gibbs-type prior is given by:\n\\mathbb{P}(X_{n+1} \\in \\cdot \\mid X_1,\\dots,X_n)\n= \\frac{V_{n+1, k+1}}{V_{n,k}}P(\\cdot)\n+ \\frac{V_{n+1, k}}{V_{n,k}}\\sum_{j=1}^k(n_j - \\sigma)\\delta_{X^*_j}(\\cdot)."
  },
  {
    "objectID": "post/SISBayes/SISBayes2025_slides.html#three-notable-examples",
    "href": "post/SISBayes/SISBayes2025_slides.html#three-notable-examples",
    "title": "A Bayesian theory for estimation of biodiversity",
    "section": "Three notable examples",
    "text": "Three notable examples\n\nDirichlet multinomial (\\sigma &lt; 0). For \\sigma &lt; 0 and H \\in \\mathbb{N}, a valid set of Gibbs coefficients is given by: \nV_{n, y}(\\sigma, H) := \\frac{|\\sigma|^{y-1}\\prod_{j=1}^{y-1}(H - j)}{(H|\\sigma| +1)_{n-1}}\n\\mathbb{I}(y \\le H).\n\nDirichlet process (\\sigma = 0). For \\sigma = 0 and \\alpha &gt; 0, a valid set of Gibbs coefficients is given by: \nV_{n, y}(\\alpha) := \\frac{\\alpha^y}{(\\alpha)_n}.\n\n\\sigma-stable Poisson–Kingman process (0 &lt; \\sigma &lt; 1). For \\sigma \\in (0,1) and gamma \\gamma &gt; 0, a valid set of Gibbs coefficients is defined as: \nV_{n, y}(\\sigma, \\gamma) := \\frac{\\sigma^y\\gamma^y}{\\Gamma(n-y\\sigma)f_{\\sigma}\\left(\\gamma^{-1/\\sigma}\\right)}\n\\int_{0}^{1}s^{n-1-y\\sigma}f_{\\sigma}\\left(\\left(1-s\\right)\\gamma^{-1/\\sigma}\\right)\\, \\mathrm{d}s,\n where f_\\sigma(t) = (\\pi)^{-1} \\sum_{h=1}^\\infty (-1)^{h+1}\\sin(h \\pi \\sigma )\\Gamma(h\\sigma + 1) / t^{h\\sigma + 1}."
  },
  {
    "objectID": "post/SISBayes/SISBayes2025_slides.html#a-characterization-theorem",
    "href": "post/SISBayes/SISBayes2025_slides.html#a-characterization-theorem",
    "title": "A Bayesian theory for estimation of biodiversity",
    "section": "A characterization theorem",
    "text": "A characterization theorem\n\n\n\nTheorem (Gnedin and Pitman 2005)\n\n\nThe Gibbs coefficients V_{n, y} satisfy the recursive equation in the following three cases:\n\nIf \\sigma &lt; 0, whenever V_{n, y} = \\sum_{h=1}^\\infty V_{n, y}(\\sigma, h) p(h), for some discrete random variable H \\in \\mathbb{N} with pdf p(h), where the V_{n, y}(\\sigma, h)’s are those of the Dirichlet multinomial.\nIf \\sigma = 0, whenever V_{n, y} = \\int_{\\mathbb{R}^+} V_{n, y}(\\alpha) p(\\mathrm{d}\\alpha), for some positive random variable \\alpha with probability measure p(\\mathrm{d}\\alpha), where the V_{n, y}(\\alpha)’s are those of the Dirichlet process.\nIf \\sigma \\in (0,1), whenever V_{n, y} = \\int_{\\mathbb{R}^+} V_{n, y}(\\sigma, \\gamma) p(\\mathrm{d}\\gamma), for some positive random variable \\gamma with probability measure p(\\mathrm{d}\\gamma), where the V_{n, y}(\\sigma, \\gamma)’s are those of the \\sigma-stable PK.\n\n\n\n\n\nThe Dirichlet multinomial, Dirichlet process, and \\sigma-stable PK form the foundation of any Gibbs-type prior.\nIn fact, any Gibbs-type process can be represented hierarchically, involving a suitable prior distribution for the key parameters H, \\alpha, and \\gamma."
  },
  {
    "objectID": "post/SISBayes/SISBayes2025_slides.html#the-quantification-of-biodiversity",
    "href": "post/SISBayes/SISBayes2025_slides.html#the-quantification-of-biodiversity",
    "title": "A Bayesian theory for estimation of biodiversity",
    "section": "The quantification of biodiversity",
    "text": "The quantification of biodiversity\n\nThe simplest measure of biodiversity is arguably the taxon richness Y^{(n)} = y.\nA priori, the distribution of Y^{(n)} induced by a Gibbs-type prior has a simple form: \n\\mathbb{P}(Y^{(n)} = y)=V_{n,y}\\frac{\\mathscr{C}(n,y;\\sigma)}{\\sigma^y},\n where \\mathscr{C}(n, y;\\sigma) denotes a generalized factorial coefficient.\nThe a priori expectations \\mathbb{E}(K_1),\\dots,\\mathbb{E}(K_n) define a model-based rarefaction curve.\nThe posterior distribution of the number of previously unobserved taxa Y_m^{(n)} is \n\\mathbb{P}(Y_m^{(n)}= j \\mid X_1,\\dots,X_n)=\\frac{V_{n + m, y + j}}{V_{n,k}}\\frac{\\mathscr{C}(m, j;\\sigma, -n + y\\sigma)}{\\sigma^j}, \\qquad j=0,\\dots,m,\n where \\mathscr{C}(m, j;\\sigma, -n + y\\sigma) is the noncentral generalized factorial coefficient.\nThe posterior expectations \\mathbb{E}(Y^{(n+1)} \\mid Y^{(n)} = y), \\dots, \\mathbb{E}(Y^{(n + m)} \\mid Y^{(n)} = y) represents a model-based extrapolation of the accumulation curve."
  },
  {
    "objectID": "post/SISBayes/SISBayes2025_slides.html#rarefaction-and-extrapolation",
    "href": "post/SISBayes/SISBayes2025_slides.html#rarefaction-and-extrapolation",
    "title": "A Bayesian theory for estimation of biodiversity",
    "section": "Rarefaction and extrapolation",
    "text": "Rarefaction and extrapolation"
  },
  {
    "objectID": "post/SISBayes/SISBayes2025_slides.html#the-sigma-diversity-pitman-2003",
    "href": "post/SISBayes/SISBayes2025_slides.html#the-sigma-diversity-pitman-2003",
    "title": "A Bayesian theory for estimation of biodiversity",
    "section": "The \\sigma-diversity (Pitman, 2003)",
    "text": "The \\sigma-diversity (Pitman, 2003)\n\nLet Y^{(n)} be the number of distinct values arising from a Gibbs-type prior:\n\nLet \\sigma &lt; 0 and V_{n, y}(\\sigma, H) be the weights of a Dirichlet multinomial, then Y^{(n)} \\rightarrow H a.s.\nLet \\sigma = 0 and V_{n, y}(\\alpha) be the weights of a Dirichlet process, then Y^{(n)} / \\log(n) \\rightarrow \\alpha a.s.\nLet \\sigma \\in (0,1) and V_{n, y}(\\sigma, \\gamma) be the weights of a \\sigma-stable PK, then Y^{(n)} / n^\\sigma \\rightarrow \\gamma a.s.\n\nMoreover, consider a generic set of weights V_{n,k} and let \nc_\\sigma(n) =\n\\begin{cases}\n1, & \\sigma &lt;0,\\\\[2mm]\n\\log(n), & \\sigma = 0,\\\\[1mm]\nn^{\\sigma}, & \\sigma \\in (0,1)\n\\end{cases}\n Then, as n \\rightarrow \\infty: \n\\frac{Y^{(n)}}{c_\\sigma(n)} \\overset{\\textup{a.s.}}{\\longrightarrow} S_{\\sigma}.\n\nThe r.v. S_{\\sigma} is the \\sigma-diversity and its distribution coincides with the prior for H, \\alpha, and \\gamma."
  },
  {
    "objectID": "post/SISBayes/SISBayes2025_slides.html#the-sigma-diversity-pitman2003",
    "href": "post/SISBayes/SISBayes2025_slides.html#the-sigma-diversity-pitman2003",
    "title": "A Bayesian theory for estimation of biodiversity",
    "section": "The \\sigma-diversity (Pitman 2003)",
    "text": "The \\sigma-diversity (Pitman 2003)\n\nLet Y^{(n)} be the number of distinct values arising from a Gibbs-type prior:\n\nLet \\sigma &lt; 0 and V_{n, y}(\\sigma, H) be the weights of a Dirichlet multinomial, then Y^{(n)} \\rightarrow H a.s.\nLet \\sigma = 0 and V_{n, y}(\\alpha) be the weights of a Dirichlet process, then Y^{(n)} / \\log(n) \\rightarrow \\alpha a.s.\nLet \\sigma \\in (0,1) and V_{n, y}(\\sigma, \\gamma) be the weights of a \\sigma-stable PK, then Y^{(n)} / n^\\sigma \\rightarrow \\gamma a.s.\n\nMoreover, consider a generic set of weights V_{n, y} and let \nc_\\sigma(n) =\n\\begin{cases}\n1, & \\sigma &lt;0,\\\\[2mm]\n\\log(n), & \\sigma = 0,\\\\[1mm]\nn^{\\sigma}, & \\sigma \\in (0,1)\n\\end{cases}\n Then, as n \\rightarrow \\infty: \n\\frac{Y^{(n)}}{c_\\sigma(n)} \\overset{\\textup{a.s.}}{\\longrightarrow} S_{\\sigma}.\n The r.v. S_{\\sigma} is the \\sigma-diversity and its distribution coincides with the prior for H, \\alpha, and \\gamma."
  },
  {
    "objectID": "post/SISBayes/SISBayes2025_slides.html#the-quantification-of-biodiversity-lijoi2007",
    "href": "post/SISBayes/SISBayes2025_slides.html#the-quantification-of-biodiversity-lijoi2007",
    "title": "A Bayesian theory for estimation of biodiversity",
    "section": "The quantification of biodiversity (Lijoi et al. 2007a)",
    "text": "The quantification of biodiversity (Lijoi et al. 2007a)\n\nThe simplest measure of biodiversity is arguably the taxon richness Y^{(n)} = y.\nA priori, the distribution of Y^{(n)} induced by a Gibbs-type prior has a simple form: \n\\mathbb{P}(Y^{(n)} = y)=V_{n,y}\\frac{\\mathscr{C}(n,y;\\sigma)}{\\sigma^y},\n where \\mathscr{C}(n, y;\\sigma) denotes a generalized factorial coefficient.\nThe a priori expectations \\mathbb{E}(K_1),\\dots,\\mathbb{E}(K_n) define a model-based rarefaction curve.\nThe posterior distribution of the number of previously unobserved taxa Y_m^{(n)} is \n\\mathbb{P}(Y_m^{(n)}= j \\mid X_1,\\dots,X_n)=\\frac{V_{n + m, y + j}}{V_{n, y}}\\frac{\\mathscr{C}(m, j;\\sigma, -n + y\\sigma)}{\\sigma^j}, \\qquad j=0,\\dots,m,\n where \\mathscr{C}(m, j;\\sigma, -n + y\\sigma) is the noncentral generalized factorial coefficient.\nThe posterior expectations \\mathbb{E}(Y^{(n+1)} \\mid Y^{(n)} = y), \\dots, \\mathbb{E}(Y^{(n + m)} \\mid Y^{(n)} = y) represents a model-based extrapolation of the accumulation curve."
  },
  {
    "objectID": "post/SISBayes/SISBayes2025_slides.html#approximately-80-years-ago-fisher1943",
    "href": "post/SISBayes/SISBayes2025_slides.html#approximately-80-years-ago-fisher1943",
    "title": "A Bayesian theory for estimation of biodiversity",
    "section": "Approximately 80 years ago… Fisher et al. (1943)",
    "text": "Approximately 80 years ago… Fisher et al. (1943)"
  },
  {
    "objectID": "post/SISBayes/SISBayes2025_slides.html#the-sigma-diversity-pitman2003-1",
    "href": "post/SISBayes/SISBayes2025_slides.html#the-sigma-diversity-pitman2003-1",
    "title": "A Bayesian theory for estimation of biodiversity",
    "section": "The \\sigma-diversity (Pitman 2003)",
    "text": "The \\sigma-diversity (Pitman 2003)\n\nBroadly speaking, the \\sigma-diversity can be seen as a rescaled richness measure.\nThe \\sigma-diversity is deterministic and assumed to be known in the Dirichlet multinomial, Dirichlet process, and \\sigma-stable PK.\nHowever, the \\sigma-diversities H, \\alpha, or \\gamma are typically unknown, and they can be estimated employing a prior distribution, leading to a Gibbs-type prior.\nThe posterior law of the \\sigma-diversity is a key quantity for measuring biodiversity. The posterior law of S_\\sigma has an elegant connection with accumulation curves.\n\n\n\n\nTheorem (Rigon et al. 2025b)\n\n\nLet X_1,\\dots,X_{n+m} be a sample from a Gibbs-type prior with Y^{(n+m)} distinct values. Then:\n\n\\left(\\frac{Y^{(n+m)}}{c_\\sigma(m)} \\mid X_1,\\dots,X_n \\right) \\overset{d}{\\longrightarrow} \\tilde{S}_{\\sigma}, \\qquad\n\\tilde{S}_\\sigma \\overset{d}{=} (S_\\sigma \\mid X_1,\\dots,X_n),\n as m \\rightarrow \\infty, where S_\\sigma is the \\sigma-diversity."
  },
  {
    "objectID": "post/SISBayes/SISBayes2025_slides.html#tree-species-in-the-amazon-basin",
    "href": "post/SISBayes/SISBayes2025_slides.html#tree-species-in-the-amazon-basin",
    "title": "A Bayesian theory for estimation of biodiversity",
    "section": "Tree species in the Amazon Basin",
    "text": "Tree species in the Amazon Basin\n\n\n\n\n\n\n\nHow to get a Bayesian estimator for the biodiversity? We can place a prior on \\alpha and the compute its posterior distribution\nRight panel: posterior distribution of the fundamental biodiversity number \\alpha, using the prior Zito et al. (2024). The dotted lines represent 95% credible intervals. The dashed line is the posterior mean.\n\n%\n\\end{document}"
  },
  {
    "objectID": "post/SISBayes/SISBayes2025_slides.html#tree-species-in-the-amazon-basin-hubbel2008",
    "href": "post/SISBayes/SISBayes2025_slides.html#tree-species-in-the-amazon-basin-hubbel2008",
    "title": "A Bayesian theory for estimation of biodiversity",
    "section": "Tree species in the Amazon Basin (Hubbel2008?)",
    "text": "Tree species in the Amazon Basin (Hubbel2008?)\n\n\n\n\n\n\n\nHow to get a Bayesian estimator for the biodiversity? We can place a prior on \\alpha and the compute its posterior distribution\nRight panel: posterior distribution of the fundamental biodiversity number \\alpha, using the prior Zito et al. (2024). The dotted lines represent 95% credible intervals. The dashed line is the posterior mean.\n\n%\n\\end{document}"
  },
  {
    "objectID": "post/SISBayes/SISBayes2025_slides.html#tree-species-in-the-amazon-basin-hubbell2008",
    "href": "post/SISBayes/SISBayes2025_slides.html#tree-species-in-the-amazon-basin-hubbell2008",
    "title": "A Bayesian theory for estimation of biodiversity",
    "section": "Tree species in the Amazon Basin (Hubbell et al. 2008)",
    "text": "Tree species in the Amazon Basin (Hubbell et al. 2008)\n\n\n\n\n\n\n\nHow to get a Bayesian estimator for the biodiversity? We can place a prior on \\alpha and the compute its posterior distribution using Bayes theorem.\nPosterior distribution of the \\sigma-diversity \\alpha, under a (conjugate) Stirling-gamma prior (Zito et al. 2024). The dotted lines represent 95% credible intervals. The dashed line is the posterior mean."
  },
  {
    "objectID": "post/SISBayes/SISBayes2025_slides.html#historical-summary-and-extensions-and-question",
    "href": "post/SISBayes/SISBayes2025_slides.html#historical-summary-and-extensions-and-question",
    "title": "A Bayesian theory for estimation of biodiversity",
    "section": "Historical summary and extensions and question",
    "text": "Historical summary and extensions and question\n\nThe fundamental biodiversity number \\alpha is the growth-adjusted richness: \n\\frac{Y^{(n)}}{\\log n} \\stackrel{\\mathrm{a.s.}}{\\longrightarrow} \\alpha, \\qquad n\\to \\infty\n\nThe \\alpha from the neutral theory, DP and Fisher log-series is the same for large n: \n\\underbrace{y = \\hat{\\alpha}^\\mathrm{Fisher}\\log\\Big(1 + \\frac{n}{\\hat{\\alpha}^\\mathrm{Fisher}}\\Big)}_{\\text{Solution from Fisher}} \\ \\stackrel{\\text{Under large } n}{\\approx} \\ \\underbrace{\\sum_{j = 1}^{n} \\frac{\\hat{\\alpha}^{\\text{DP}}}{\\hat{\\alpha}^{\\text{DP}} + j - 1} = y}_{\\text{MLE from Hubbell and DP}}\n\nExtensions:\n\nIn , species migrate from metacommunity to local communities w.p. m \\in [0, 1].\nIn BNP, \\sigma &lt; 1 allows for polynomial growth or finite asymptotic richness .\nHierarchies and multiple samples: hierarchical DP and multi-sample neutral theory ."
  },
  {
    "objectID": "post/SISBayes/SISBayes2025_slides.html#historical-summary-and-extensions",
    "href": "post/SISBayes/SISBayes2025_slides.html#historical-summary-and-extensions",
    "title": "A Bayesian theory for estimation of biodiversity",
    "section": "Historical summary and extensions",
    "text": "Historical summary and extensions\n\nThe fundamental biodiversity number \\alpha is a growth-adjusted richness: \n\\frac{Y^{(n)}}{\\log n} \\stackrel{\\mathrm{a.s.}}{\\longrightarrow} \\alpha, \\qquad n\\to \\infty\n\nThe \\alpha from the neutral theory, DP and Fisher log-series is the same for large n: \n\\underbrace{y = \\hat{\\alpha}^\\mathrm{Fisher}\\log\\Big(1 + \\frac{n}{\\hat{\\alpha}^\\mathrm{Fisher}}\\Big)}_{\\text{Solution from Fisher}} \\ \\stackrel{\\text{Under large } n}{\\approx} \\ \\underbrace{\\sum_{j = 1}^{n} \\frac{\\hat{\\alpha}^{\\text{DP}}}{\\hat{\\alpha}^{\\text{DP}} + j - 1} = y}_{\\text{MLE from Hubbell and DP}}\n\nThere are a lot of possible extensions. This is a very biased list:\n\nThe choice \\sigma \\in (0, 1) allows for polynomial growth (Lijoi et al. 2007b; Pitman and Yor 1997) while \\sigma &lt; 0 leads to a finite richness (Gnedin 2010; Lijoi et al. 2020).\nMultiple samples, a.k.a. partial exchangeability (Camerlenghi et al. 2019; Franzolini et al. 2025)\nEnriched processes, taxonomical data (Rigon et al. 2025a; Rigon et al. 2025b; Zito et al. 2023).\nIncidence data and feature models (Ghilotti et al. 2025).\nWhat about covariates?"
  },
  {
    "objectID": "post/SISBayes/SISBayes2025.html#approximately-80-years-ago-fisher1943",
    "href": "post/SISBayes/SISBayes2025.html#approximately-80-years-ago-fisher1943",
    "title": "A Bayesian theory for estimation of biodiversity",
    "section": "Approximately 80 years ago… Fisher et al. (1943)",
    "text": "Approximately 80 years ago… Fisher et al. (1943)"
  },
  {
    "objectID": "post/SISBayes/SISBayes2025.html#early-ideas-good_1953-good_toulmin_1956",
    "href": "post/SISBayes/SISBayes2025.html#early-ideas-good_1953-good_toulmin_1956",
    "title": "A Bayesian theory for estimation of biodiversity",
    "section": "Early ideas (Good 1953; Good and Toulmin 1956)",
    "text": "Early ideas (Good 1953; Good and Toulmin 1956)"
  },
  {
    "objectID": "post/SISBayes/SISBayes2025.html#the-dirichlet-process-ferguson1973",
    "href": "post/SISBayes/SISBayes2025.html#the-dirichlet-process-ferguson1973",
    "title": "A Bayesian theory for estimation of biodiversity",
    "section": "The Dirichlet process (Ferguson 1973)",
    "text": "The Dirichlet process (Ferguson 1973)"
  },
  {
    "objectID": "post/SISBayes/SISBayes2025.html#and-species-sampling-models-pitman1996",
    "href": "post/SISBayes/SISBayes2025.html#and-species-sampling-models-pitman1996",
    "title": "A Bayesian theory for estimation of biodiversity",
    "section": "… and species sampling models (Pitman 1996)",
    "text": "… and species sampling models (Pitman 1996)"
  },
  {
    "objectID": "post/SISBayes/SISBayes2025.html#ewens-formula-governs-the-neutral-theory-hubbell_2001",
    "href": "post/SISBayes/SISBayes2025.html#ewens-formula-governs-the-neutral-theory-hubbell_2001",
    "title": "A Bayesian theory for estimation of biodiversity",
    "section": "Ewens formula governs the neutral theory! (Hubbell 2001)",
    "text": "Ewens formula governs the neutral theory! (Hubbell 2001)"
  },
  {
    "objectID": "post/SISBayes/SISBayes2025.html#all-ties-together-mccullagh_2016",
    "href": "post/SISBayes/SISBayes2025.html#all-ties-together-mccullagh_2016",
    "title": "A Bayesian theory for estimation of biodiversity",
    "section": "All ties together (McCullagh 2016)",
    "text": "All ties together (McCullagh 2016)"
  },
  {
    "objectID": "post/SISBayes/SISBayes2025.html#species-sampling-models",
    "href": "post/SISBayes/SISBayes2025.html#species-sampling-models",
    "title": "A Bayesian theory for estimation of biodiversity",
    "section": "Species sampling models",
    "text": "Species sampling models\n\nLet X_1,\\dots,X_n be some collection of species or taxa. Suppose X_n are conditionally iid samples from a species sampling model: \n(X_n \\mid \\tilde{p}) \\overset{\\text{iid}}{\\sim} \\tilde{p}, \\qquad\n\\tilde{p}(\\cdot) = \\sum_{h=1}^{\\infty}\\pi_h \\delta_{Z_h}(\\cdot),\n\\qquad n \\ge 1,\n where (\\pi_h)_{h \\ge 1} is a set of probabilities (species proportions) and Z_h represent distinct species.\nThe discreteness of \\tilde{p} identifies Y^{(n)}=y distinct taxa X_1^*, \\ldots, X_y^* with frequencies n_1, \\ldots, n_y, called abundances in ecology.\nGibbs-type priors have emerged as the most natural extension of the DP (De Blasi et al. 2015).\nThe predictive distribution of a Gibbs-type prior is given by: \n\\mathbb{P}(X_{n+1} \\in \\cdot \\mid X_1,\\dots,X_n)\n= \\frac{V_{n+1, y+1}}{V_{n,y}}P(\\cdot)\n+ \\frac{V_{n+1, y}}{V_{n,y}}\\sum_{j=1}^y(n_j - \\sigma)\\delta_{X^*_j}(\\cdot).\n where (a)_{n} denotes a rising factorial, and \\sigma &lt; 1 is the discount parameter. The V_{n,y}’s are non-negative weights satisfying a forward recursive equation."
  },
  {
    "objectID": "post/SISBayes/SISBayes2025.html#three-notable-examples",
    "href": "post/SISBayes/SISBayes2025.html#three-notable-examples",
    "title": "A Bayesian theory for estimation of biodiversity",
    "section": "Three notable examples",
    "text": "Three notable examples\n\nDirichlet multinomial (\\sigma &lt; 0). For \\sigma &lt; 0 and H \\in \\mathbb{N}, a valid set of Gibbs coefficients is given by: \nV_{n, y}(\\sigma, H) := \\frac{|\\sigma|^{y-1}\\prod_{j=1}^{y-1}(H - j)}{(H|\\sigma| +1)_{n-1}}\n\\mathbb{I}(y \\le H).\n\nDirichlet process (\\sigma = 0). For \\sigma = 0 and \\alpha &gt; 0, a valid set of Gibbs coefficients is given by: \nV_{n, y}(\\alpha) := \\frac{\\alpha^y}{(\\alpha)_n}.\n\n\\sigma-stable Poisson–Kingman process (0 &lt; \\sigma &lt; 1). For \\sigma \\in (0,1) and gamma \\gamma &gt; 0, a valid set of Gibbs coefficients is defined as: \nV_{n, y}(\\sigma, \\gamma) := \\frac{\\sigma^y\\gamma^y}{\\Gamma(n-y\\sigma)f_{\\sigma}\\left(\\gamma^{-1/\\sigma}\\right)}\n\\int_{0}^{1}s^{n-1-y\\sigma}f_{\\sigma}\\left(\\left(1-s\\right)\\gamma^{-1/\\sigma}\\right)\\, \\mathrm{d}s,\n where f_\\sigma(t) = (\\pi)^{-1} \\sum_{h=1}^\\infty (-1)^{h+1}\\sin(h \\pi \\sigma )\\Gamma(h\\sigma + 1) / t^{h\\sigma + 1}."
  },
  {
    "objectID": "post/SISBayes/SISBayes2025.html#a-characterization-theorem",
    "href": "post/SISBayes/SISBayes2025.html#a-characterization-theorem",
    "title": "A Bayesian theory for estimation of biodiversity",
    "section": "A characterization theorem",
    "text": "A characterization theorem\n\n\n\n\n\n\nTheorem (Gnedin and Pitman 2005)\n\n\n\nThe Gibbs coefficients V_{n, y} satisfy the recursive equation in the following three cases:\n\nIf \\sigma &lt; 0, whenever V_{n, y} = \\sum_{h=1}^\\infty V_{n, y}(\\sigma, h) p(h), for some discrete random variable H \\in \\mathbb{N} with pdf p(h), where the V_{n, y}(\\sigma, h)’s are those of the Dirichlet multinomial.\nIf \\sigma = 0, whenever V_{n, y} = \\int_{\\mathbb{R}^+} V_{n, y}(\\alpha) p(\\mathrm{d}\\alpha), for some positive random variable \\alpha with probability measure p(\\mathrm{d}\\alpha), where the V_{n, y}(\\alpha)’s are those of the Dirichlet process.\nIf \\sigma \\in (0,1), whenever V_{n, y} = \\int_{\\mathbb{R}^+} V_{n, y}(\\sigma, \\gamma) p(\\mathrm{d}\\gamma), for some positive random variable \\gamma with probability measure p(\\mathrm{d}\\gamma), where the V_{n, y}(\\sigma, \\gamma)’s are those of the \\sigma-stable PK.\n\n\n\n\nThe Dirichlet multinomial, Dirichlet process, and \\sigma-stable PK form the foundation of any Gibbs-type prior.\nIn fact, any Gibbs-type process can be represented hierarchically, involving a suitable prior distribution for the key parameters H, \\alpha, and \\gamma."
  },
  {
    "objectID": "post/SISBayes/SISBayes2025.html#the-quantification-of-biodiversity-lijoi2007",
    "href": "post/SISBayes/SISBayes2025.html#the-quantification-of-biodiversity-lijoi2007",
    "title": "A Bayesian theory for estimation of biodiversity",
    "section": "The quantification of biodiversity (Lijoi et al. 2007a)",
    "text": "The quantification of biodiversity (Lijoi et al. 2007a)\n\nThe simplest measure of biodiversity is arguably the taxon richness Y^{(n)} = y.\nA priori, the distribution of Y^{(n)} induced by a Gibbs-type prior has a simple form: \n\\mathbb{P}(Y^{(n)} = y)=V_{n,y}\\frac{\\mathscr{C}(n,y;\\sigma)}{\\sigma^y},\n where \\mathscr{C}(n, y;\\sigma) denotes a generalized factorial coefficient.\nThe a priori expectations \\mathbb{E}(K_1),\\dots,\\mathbb{E}(K_n) define a model-based rarefaction curve.\nThe posterior distribution of the number of previously unobserved taxa Y_m^{(n)} is \n\\mathbb{P}(Y_m^{(n)}= j \\mid X_1,\\dots,X_n)=\\frac{V_{n + m, y + j}}{V_{n, y}}\\frac{\\mathscr{C}(m, j;\\sigma, -n + y\\sigma)}{\\sigma^j}, \\qquad j=0,\\dots,m,\n where \\mathscr{C}(m, j;\\sigma, -n + y\\sigma) is the noncentral generalized factorial coefficient.\nThe posterior expectations \\mathbb{E}(Y^{(n+1)} \\mid Y^{(n)} = y), \\dots, \\mathbb{E}(Y^{(n + m)} \\mid Y^{(n)} = y) represents a model-based extrapolation of the accumulation curve."
  },
  {
    "objectID": "post/SISBayes/SISBayes2025.html#rarefaction-and-extrapolation",
    "href": "post/SISBayes/SISBayes2025.html#rarefaction-and-extrapolation",
    "title": "A Bayesian theory for estimation of biodiversity",
    "section": "Rarefaction and extrapolation",
    "text": "Rarefaction and extrapolation"
  },
  {
    "objectID": "post/SISBayes/SISBayes2025.html#the-sigma-diversity-pitman2003",
    "href": "post/SISBayes/SISBayes2025.html#the-sigma-diversity-pitman2003",
    "title": "A Bayesian theory for estimation of biodiversity",
    "section": "The \\sigma-diversity (Pitman 2003)",
    "text": "The \\sigma-diversity (Pitman 2003)\n\nLet Y^{(n)} be the number of distinct values arising from a Gibbs-type prior:\n\nLet \\sigma &lt; 0 and V_{n, y}(\\sigma, H) be the weights of a Dirichlet multinomial, then Y^{(n)} \\rightarrow H a.s.\nLet \\sigma = 0 and V_{n, y}(\\alpha) be the weights of a Dirichlet process, then Y^{(n)} / \\log(n) \\rightarrow \\alpha a.s.\nLet \\sigma \\in (0,1) and V_{n, y}(\\sigma, \\gamma) be the weights of a \\sigma-stable PK, then Y^{(n)} / n^\\sigma \\rightarrow \\gamma a.s.\n\nMoreover, consider a generic set of weights V_{n, y} and let \nc_\\sigma(n) =\n\\begin{cases}\n1, & \\sigma &lt;0,\\\\[2mm]\n\\log(n), & \\sigma = 0,\\\\[1mm]\nn^{\\sigma}, & \\sigma \\in (0,1)\n\\end{cases}\n Then, as n \\rightarrow \\infty: \n\\frac{Y^{(n)}}{c_\\sigma(n)} \\overset{\\textup{a.s.}}{\\longrightarrow} S_{\\sigma}.\n The r.v. S_{\\sigma} is the \\sigma-diversity and its distribution coincides with the prior for H, \\alpha, and \\gamma."
  },
  {
    "objectID": "post/SISBayes/SISBayes2025.html#the-sigma-diversity-pitman2003-1",
    "href": "post/SISBayes/SISBayes2025.html#the-sigma-diversity-pitman2003-1",
    "title": "A Bayesian theory for estimation of biodiversity",
    "section": "The \\sigma-diversity (Pitman 2003)",
    "text": "The \\sigma-diversity (Pitman 2003)\n\nBroadly speaking, the \\sigma-diversity can be seen as a rescaled richness measure.\nThe \\sigma-diversity is deterministic and assumed to be known in the Dirichlet multinomial, Dirichlet process, and \\sigma-stable PK.\nHowever, the \\sigma-diversities H, \\alpha, or \\gamma are typically unknown, and they can be estimated employing a prior distribution, leading to a Gibbs-type prior.\nThe posterior law of the \\sigma-diversity is a key quantity for measuring biodiversity. The posterior law of S_\\sigma has an elegant connection with accumulation curves.\n\n\n\n\n\n\n\nTheorem (Rigon et al. 2025b)\n\n\n\nLet X_1,\\dots,X_{n+m} be a sample from a Gibbs-type prior with Y^{(n+m)} distinct values. Then:\n\n\\left(\\frac{Y^{(n+m)}}{c_\\sigma(m)} \\mid X_1,\\dots,X_n \\right) \\overset{d}{\\longrightarrow} \\tilde{S}_{\\sigma}, \\qquad\n\\tilde{S}_\\sigma \\overset{d}{=} (S_\\sigma \\mid X_1,\\dots,X_n),\n as m \\rightarrow \\infty, where S_\\sigma is the \\sigma-diversity."
  },
  {
    "objectID": "post/SISBayes/SISBayes2025.html#tree-species-in-the-amazon-basin-hubbell2008",
    "href": "post/SISBayes/SISBayes2025.html#tree-species-in-the-amazon-basin-hubbell2008",
    "title": "A Bayesian theory for estimation of biodiversity",
    "section": "Tree species in the Amazon Basin (Hubbell et al. 2008)",
    "text": "Tree species in the Amazon Basin (Hubbell et al. 2008)\n\n\n\n\n\n\n\n\nHow to get a Bayesian estimator for the biodiversity? We can place a prior on \\alpha and the compute its posterior distribution using Bayes theorem.\nPosterior distribution of the \\sigma-diversity \\alpha, under a (conjugate) Stirling-gamma prior (Zito et al. 2024). The dotted lines represent 95% credible intervals. The dashed line is the posterior mean."
  },
  {
    "objectID": "post/SISBayes/SISBayes2025.html#historical-summary-and-extensions",
    "href": "post/SISBayes/SISBayes2025.html#historical-summary-and-extensions",
    "title": "A Bayesian theory for estimation of biodiversity",
    "section": "Historical summary and extensions",
    "text": "Historical summary and extensions\n\nThe fundamental biodiversity number \\alpha is a growth-adjusted richness: \n\\frac{Y^{(n)}}{\\log n} \\stackrel{\\mathrm{a.s.}}{\\longrightarrow} \\alpha, \\qquad n\\to \\infty\n\nThe \\alpha from the neutral theory, DP and Fisher log-series is the same for large n: \n\\underbrace{y = \\hat{\\alpha}^\\mathrm{Fisher}\\log\\Big(1 + \\frac{n}{\\hat{\\alpha}^\\mathrm{Fisher}}\\Big)}_{\\text{Solution from Fisher}} \\ \\stackrel{\\text{Under large } n}{\\approx} \\ \\underbrace{\\sum_{j = 1}^{n} \\frac{\\hat{\\alpha}^{\\text{DP}}}{\\hat{\\alpha}^{\\text{DP}} + j - 1} = y}_{\\text{MLE from Hubbell and DP}}\n\nThere are a lot of possible extensions. This is a very biased list:\n\nThe choice \\sigma \\in (0, 1) allows for polynomial growth (Lijoi et al. 2007b; Pitman and Yor 1997) while \\sigma &lt; 0 leads to a finite richness (Gnedin 2010; Lijoi et al. 2020).\nMultiple samples, a.k.a. partial exchangeability (Camerlenghi et al. 2019; Franzolini et al. 2025)\nEnriched processes, taxonomical data (Rigon et al. 2025a; Rigon et al. 2025b; Zito et al. 2023).\nIncidence data and feature models (Ghilotti et al. 2025).\nWhat about covariates?"
  },
  {
    "objectID": "post/SISBayes/SISBayes2025_slides.html#specimens-and-bins-in-barcode-of-life",
    "href": "post/SISBayes/SISBayes2025_slides.html#specimens-and-bins-in-barcode-of-life",
    "title": "A Bayesian theory for estimation of biodiversity",
    "section": "Specimens and BINs in Barcode of Life",
    "text": "Specimens and BINs in Barcode of Life"
  },
  {
    "objectID": "post/SISBayes/SISBayes2025_slides.html#how-can-we-link-covariates-with-biodiversity-glms",
    "href": "post/SISBayes/SISBayes2025_slides.html#how-can-we-link-covariates-with-biodiversity-glms",
    "title": "A Bayesian theory for estimation of biodiversity",
    "section": "How can we link covariates with biodiversity? GLMs",
    "text": "How can we link covariates with biodiversity? GLMs\n\nFor outcome Y_i and covariates \\mathbf{x}_i = (x_{i1}, \\ldots, x_{ip}) \\in \\mathds{R}^p, i = 1, \\ldots, N, a generalized linear model (GLM) extends the linear model in non-Gaussian settings via three ingredients:\nThe exponential dispersion family directing Y_i: \nf_Y(y; \\theta, \\phi) = \\exp\\left\\{\\frac{y \\theta - b(\\theta)}{a(\\phi)} + c(y, \\phi)\\right\\},\n where \\theta \\in \\mathds{R} is the natural parameter, \\phi &gt;0 is the dispersion, and a,b,c are some functions.\nThe linear predictor \\eta_i = \\mathbf{x}_i^\\top \\bbeta, with \\bbeta \\in \\mathds{R}^p.\nA monotonic link function g that links \\eta_i with the mean of Y_i, namely \\mu(\\theta_i) = \\mu_i: \n\\mathds{E}(Y_i\\mid \\mathbf{x}_i) = \\mu_i = g^{-1}(\\eta_i).\n The link is canonical when g is such that \\theta_i = \\eta_i.\n\n\nIs there an exponential family that depends on \\alpha?"
  },
  {
    "objectID": "post/SISBayes/SISBayes2025_slides.html#how-can-we-link-covariates-with-biodiversity",
    "href": "post/SISBayes/SISBayes2025_slides.html#how-can-we-link-covariates-with-biodiversity",
    "title": "A Bayesian theory for estimation of biodiversity",
    "section": "How can we link covariates with biodiversity?",
    "text": "How can we link covariates with biodiversity?\n\nFor outcome Y_i and covariates \\bm{x}_i = (x_{i1}, \\ldots, x_{ip}) \\in \\mathbb{R}^p, i = 1, \\ldots, N, a generalized linear model (GLM) extends the linear model in non-Gaussian settings via three ingredients:\nThe exponential dispersion family directing each Y_i: \nf(y_i; \\theta_i, \\phi) = \\exp\\left\\{\\frac{y_i \\theta_i - b(\\theta_i)}{a_i(\\phi)} + c(y_i, \\phi)\\right\\},\n where \\theta_i \\in \\Theta is the natural parameter, \\phi &gt; 0 is the dispersion, and a_i,b,c are given functions.\nThe linear predictor \\eta_i = \\bm{x}_i^\\top \\bm{\\beta}, with \\bm{\\beta} \\in \\mathbb{R}^p.\nA monotonic link function g that links \\eta_i with the mean of Y_i, namely \\mu(\\theta_i) = \\mu_i: \n\\mathbb{E}(Y_i \\mid \\bm{x}_i) = \\mu_i = g^{-1}(\\eta_i).\n The link is canonical when g is such that \\theta_i = \\eta_i.\nIs there an exponential family that is related to a sample-specific diversity \\alpha_i?"
  },
  {
    "objectID": "post/SISBayes/SISBayes2025_slides.html#the-hubbell-regression",
    "href": "post/SISBayes/SISBayes2025_slides.html#the-hubbell-regression",
    "title": "A Bayesian theory for estimation of biodiversity",
    "section": "The Hubbell regression",
    "text": "The Hubbell regression\n\nThe number of distinct species from a DP is an exponential family! \n\\mathbb{P}(Y^{(n)} = y; \\alpha) = \\frac{\\alpha^y}{(\\alpha)_n}|s(n, y)|, \\qquad y \\in\\{ 1, \\ldots, n\\},\n where (\\alpha)_n = \\Gamma(\\alpha + n)/\\Gamma(\\alpha) and |s(n,y)| are signless Stirling numbers of the first kind.\n\n\n\n\nThe Hubbell regression is a GLM where:\n\nThe distinct species Y_i^{(n_i)} for i=1,\\dots,N are independent draws from the distribution \nf(y_i; \\theta_i) = \\exp \\left\\{y_i \\theta_i - \\log[\\Gamma(e^{\\theta_i} + n_i) - \\Gamma(e^{\\theta_i})] + \\log|s(n_i, y_i)|\\right\\},\n with \\alpha_i = e^{\\theta_i} whose mean and variance functions are \n\\mu_i = \\mu(\\theta_i, n_i) = \\sum_{j=0}^{n_i-1} \\frac{e^{\\theta_i}}{e^{\\theta_i} + j}, \\qquad\nV(\\mu(\\theta_i, n_i)) = \\sum_{j=0}^{n_i-1} \\frac{e^{\\theta_i}}{e^{\\theta_i} + j}\\left(1 - \\frac{e^{\\theta_i}}{e^{\\theta_i} + j}\\right).\n\nThe link function g depends on \\eta_i and n_i and satisfies \\mathbb{E}(Y_i\\mid \\bm{x}_i, n_i) = \\mu_i = g^{-1}(\\bm{x}_i^\\top \\bm{\\beta}, n_i)."
  },
  {
    "objectID": "post/SISBayes/SISBayes2025_slides.html#the-canonical-link",
    "href": "post/SISBayes/SISBayes2025_slides.html#the-canonical-link",
    "title": "A Bayesian theory for estimation of biodiversity",
    "section": "The canonical link",
    "text": "The canonical link\n\nUsing the canonical link, we have: \n\\log \\alpha_i = \\theta_i = \\bm{x}_i^\\top \\bm{\\beta} = \\eta_i, \\qquad\n\\mu_i = \\mu(\\bm{x}_i^\\top \\bm{\\beta}, n_i) = \\sum_{j = 0}^{n_i-1} \\frac{e^{\\bm{x}_i^\\top \\bm{\\beta}}}{e^{\\bm{x}_i^\\top \\bm{\\beta}} + j} = g^{-1}(\\bm{x}_i^\\top \\bm{\\beta}, n_i).\n\nCoefficients \\bm{\\beta} have the classic interpretation of log-linear models in terms of \\alpha-diversity\n\n\nIf x_{ip} ↑ by 1 unit → \\alpha_i ↑ by 100(e^{\\beta_p} - 1)\\%\n\n\nThe saturated model corresponds to the case in which \\hat{\\alpha}_i \\approx \\hat{\\alpha}^{\\text{Fisher}}_i. The null model corresponds to the case \\hat{\\alpha}_i = \\hat{\\alpha} for all sites, i.e. there is no variation in biodiversity across sampling sites.\n\n\n\n\nTheorem (Poisson regression with log-log offset)\n\n\nCall \\gamma = 0.5772\\dots the Euler’s constant. Under large values of n, when each n_i \\to \\infty we have \nY^{(n)}_i \\;\\dot{\\sim}\\; 1 + \\mathrm{Poisson}\\left(\\exp\\left\\{\\bm{x}_i^\\top\\bm{\\beta} + \\log(\\gamma + \\log n_i)\\right\\}\\right).\n Informally, we will say that \\hat{\\bm{\\beta}}^{\\mathrm{Hubbell}} \\approx \\hat{\\bm{\\beta}}^{\\mathrm{Poisson}} ."
  },
  {
    "objectID": "post/SISBayes/SISBayes2025_slides.html#beyond-the-logarithmic-growth-non-canonical-links",
    "href": "post/SISBayes/SISBayes2025_slides.html#beyond-the-logarithmic-growth-non-canonical-links",
    "title": "A Bayesian theory for estimation of biodiversity",
    "section": "Beyond the logarithmic growth: non-canonical links",
    "text": "Beyond the logarithmic growth: non-canonical links\n\nExtensions of the neutral theory of Hubbell (2001) proposes a power-law behavior with a dispersal limitation parameter \\omega \\ge 0 so that \n\\mathbb{E}\\left(Y_i^{(n)}\\right) = \\sum_{j = 0}^{n_i-1} j^{-\\omega} \\frac{\\alpha_i}{\\alpha_i + j}.\n\nOur proposal: a flexible growth depending on \\sigma &lt; 1 via non-canonical link \n\\mu_i = \\mu(\\bm{x}_i^\\top\\bm{\\beta}, n_i, \\sigma) = \\sum_{j = 0}^{n_i-1} \\frac{e^{\\bm{x}_i^\\top\\bm{\\beta}}}{e^{\\bm{x}_i^\\top\\bm{\\beta}} + j^{1-\\sigma}} = g^{-1}(\\bm{x}_i^\\top\\bm{\\beta}, n_i, \\sigma), \\qquad \\sigma &lt; 1.\n The parameter \\sigma plays the same role it has in Gibbs-type priors."
  },
  {
    "objectID": "post/SISBayes/SISBayes2025_slides.html#regression-based-biodiversity-indexes",
    "href": "post/SISBayes/SISBayes2025_slides.html#regression-based-biodiversity-indexes",
    "title": "A Bayesian theory for estimation of biodiversity",
    "section": "Regression-based biodiversity indexes",
    "text": "Regression-based biodiversity indexes\n\n\n\nTheorem (Regression-based diversity)\n\n\nFor the mean function of the a Hubbell regression model with the aforementioned non-canonical link, it holds \n\\frac{\\mu(\\bm{x}_i^\\top \\bm{\\beta}; n, \\sigma)}{c_n(\\sigma)} \\longrightarrow S_\\sigma(\\bm{x}_i^\\top\\bm{\\beta}), \\qquad n \\to \\infty,\n Moreover:\n\nIf \\sigma = 0, then c_n(\\sigma) = \\log n (logarithmic growth) and S_\\sigma(\\bm{x}_i^\\top\\bm{\\beta}) = e^{\\bm{x}_i^\\top\\bm{\\beta}} = \\alpha_i;\nIf \\sigma \\in (0, 1), then c_n(\\sigma) = n^{\\sigma} (polynomial growth) and S_\\sigma(\\bm{x}_i^\\top\\bm{\\beta}) = e^{\\bm{x}_i^\\top\\bm{\\beta}}/\\sigma = \\alpha_i / \\sigma;\n\nif \\sigma &lt; 0, then c_n(\\sigma) = 1 (finite richness) and S_\\sigma(\\bm{x}_i^\\top\\bm{\\beta}) \\approx C_\\sigma^{-1}e^{\\bm{x}_i^\\top\\bm{\\beta}/(1-\\sigma)} = C_\\sigma^{-1} \\alpha_i^{1/(1-\\sigma)}, with C_\\sigma a known constant.\n\n\n\n\n\n\nThis gives a novel notion of regression-based biodiversity."
  },
  {
    "objectID": "post/SISBayes/SISBayes2025_slides.html#mle-algorithm-iteratively-reweighted-least-squares-irls",
    "href": "post/SISBayes/SISBayes2025_slides.html#mle-algorithm-iteratively-reweighted-least-squares-irls",
    "title": "A Bayesian theory for estimation of biodiversity",
    "section": "MLE algorithm: iteratively reweighted least squares (IRLS)",
    "text": "MLE algorithm: iteratively reweighted least squares (IRLS)\n\nUpdate the mean: \n\\mu_i^{(t)} \\gets \\sum_{j = 0}^{n_i-1}\n\\frac{e^{\\eta_i^{(t)}}}{e^{\\eta_i^{(t)}} + j^{1-\\sigma}},\n\\qquad \\eta_i^{(t)} = \\bm{x}_i^\\top \\bm{\\beta}^{(t)}.\n\nRetrieve \\alpha_i and compute variances: \n\\alpha_i^{(t)} \\gets\n\\texttt{uniroot}\\!\\left(\\sum_{j=0}^{n_i-1}\\frac{x}{x+j}=\\mu_i^{(t)}\\right)\\texttt{\\$root}, \\qquad v_i^{(t)} \\gets \\sum_{j=0}^{n_i-1}\nj\\,\\frac{\\alpha_i^{(t)}}{(\\alpha_i^{(t)}+j)^2}.\n\nCompute working weights: \nw_i^{(t)} \\gets \\frac{1}{v_i^{(t)}}\n\\sum_{j=0}^{n_i-1}\nj\\frac{e^{\\eta_i^{(t)}}}\n     {(e^{\\eta_i^{(t)}}+j^{1-\\sigma})^2}.\n\nUpdate regression coefficients:\n\n\\bm{\\beta}^{(t+1)} \\gets\n(\\mathbf{X}^\\top \\mathbf{W}^{(t)} \\mathbf{X})^{-1}\n\\mathbf{X}^\\top \\mathbf{W}^{(t)} \\mathbf{z}^{(t)}, \\qquad\nz_i^{(t)} \\gets \\eta_i^{(t)} +\n\\frac{y_i - \\mu_i^{(t)}}{\\{w_i^{(t)}v_i^{(t)}\\}^{1/2}}."
  },
  {
    "objectID": "post/SISBayes/SISBayes2025_slides.html#iteratively-reweighted-least-squares-irls",
    "href": "post/SISBayes/SISBayes2025_slides.html#iteratively-reweighted-least-squares-irls",
    "title": "A Bayesian theory for estimation of biodiversity",
    "section": "Iteratively reweighted least squares (IRLS)",
    "text": "Iteratively reweighted least squares (IRLS)\n\nUpdate the mean: \n\\mu_i^{(t)} \\gets \\sum_{j = 0}^{n_i-1}\n\\frac{e^{\\eta_i^{(t)}}}{e^{\\eta_i^{(t)}} + j^{1-\\sigma}},\n\\qquad \\eta_i^{(t)} = \\bm{x}_i^\\top \\bm{\\beta}^{(t)}.\n\nNumerically retrieve the diversity \\alpha_i and compute variances: \n\\alpha_i^{(t)} \\gets\n\\texttt{uniroot}\\!\\left(\\sum_{j=0}^{n_i-1}\\frac{x}{x+j}=\\mu_i^{(t)}\\right)\\texttt{\\$root}, \\qquad v_i^{(t)} \\gets \\sum_{j=0}^{n_i-1}\nj\\,\\frac{\\alpha_i^{(t)}}{(\\alpha_i^{(t)}+j)^2}.\n\nCompute working weights: \nw_i^{(t)} \\gets \\frac{1}{v_i^{(t)}}\n\\sum_{j=0}^{n_i-1}\nj\\frac{\\alpha_i^{(t)}}\n     {(\\alpha_i^{(t)}+j^{1-\\sigma})^2}.\n\nUpdate the regression coefficients:\n\n\\bm{\\beta}^{(t+1)} \\gets\n(\\bm{X}^\\top \\bm{W}^{(t)} \\bm{X})^{-1}\n\\bm{X}^\\top \\bm{W}^{(t)} \\bm{z}^{(t)}, \\qquad\nz_i^{(t)} \\gets \\eta_i^{(t)} +\n\\frac{y_i - \\mu_i^{(t)}}{(w_i^{(t)}v_i^{(t)})^{1/2}}."
  },
  {
    "objectID": "post/SISBayes/SISBayes2025_slides.html#composite-marginal-likelihood-and-standard-errors",
    "href": "post/SISBayes/SISBayes2025_slides.html#composite-marginal-likelihood-and-standard-errors",
    "title": "A Bayesian theory for estimation of biodiversity",
    "section": "Composite marginal likelihood and standard errors",
    "text": "Composite marginal likelihood and standard errors\n\nThe assumption behind GLMs is that observations are independent. In our case, we might have seen the same species (or BIN) at two different locations!\nThis information is lost when calculating the number Y_i^{(n_i)}. However, we can interpret \nL(\\boldsymbol{\\beta}) \\propto\n\\prod_{i=1}^N\n\\frac{\\alpha_i^{y_i}}\n   {(\\alpha_i)_{n_i}},\n as a composite marginal likelihood. The estimating equation is still unbiased and estimates consistent.\nWe propose spatially heteroskedastic-consistent standard errors (Conley 1999) that use the Jaccard Index (or similar indices) between two samples:\n\\widehat{\\mathrm{se}}(\\hat{\\boldsymbol{\\beta}}) =\n(\\bm{X}^\\top \\hat{\\bm{W}}\\bm{X})^{-1}\n\\textstyle\\sum_{ik}\n{\\color{#8B0000}{s_{ik}}}\\,\n{\\color{blue}{\\hat{\\bm{u}}_i\\hat{\\bm{u}}_k^\\top}}\\,\n(\\bm{X}^\\top \\hat{\\bm{W}}\\bm{X})^{-1},\n\\qquad\n{\\color{blue}{\\hat{\\bm{u}}_i}} =\n\\frac{\\partial}{\\partial\\boldsymbol{\\beta}}\n\\log \\mathcal{L}(\\hat{\\boldsymbol{\\beta}}),\n where {\\color{#8B0000}{s_{ik}}} \\in [0,1] is the fraction of shared BINs between locations i and k."
  },
  {
    "objectID": "post/SISBayes/SISBayes2025_slides.html#the-package-hubbellglm",
    "href": "post/SISBayes/SISBayes2025_slides.html#the-package-hubbellglm",
    "title": "A Bayesian theory for estimation of biodiversity",
    "section": "The package HubbellGLM",
    "text": "The package HubbellGLM"
  },
  {
    "objectID": "post/SISBayes/SISBayes2025_slides.html#determinants-of-biodiversity-in-the-global-malaise-trap-project",
    "href": "post/SISBayes/SISBayes2025_slides.html#determinants-of-biodiversity-in-the-global-malaise-trap-project",
    "title": "A Bayesian theory for estimation of biodiversity",
    "section": "Determinants of biodiversity in the global malaise trap project",
    "text": "Determinants of biodiversity in the global malaise trap project\n\nWe run the regression model\nWe estimate \\hat{\\sigma} = 0.53, which amounts to a square root growth.\n\np-values are corrected for shared species via the sandwich estimator.\n\n\n\n\n\n\n\n\n\n\n\nVariable\nEstimate\nStd. Error\nPr(&gt;\nz\n\n\n\n\n(Intercept)\n-0.019\n0.489\n0.969\n\n\n\nrealm: Australasia\n-0.486\n0.148\n0.001\n**\n\n\nrealm: Indomalayan\n0.089\n0.142\n0.529\n\n\n\nrealm: Nearctic\n-0.150\n0.469\n0.750\n\n\n\nrealm: Neotropic\n0.262\n0.420\n0.533\n\n\n\nrealm: Palearctic\n-0.005\n0.187\n0.977\n\n\n\nhabitat: Grassland\n-0.274\n0.053\n0.000\n***\n\n\nhabitat: Mixed\n0.044\n0.059\n0.463\n\n\n\nhabitat: Tundra\n-0.334\n0.177\n0.059\n.\n\n\nhabitat: Urban\n-0.374\n0.067\n0.000\n***\n\n\nhabitat: Wetland\n-0.087\n0.069\n0.210\n\n\n\ntemperature at 2m\n0.049\n0.005\n&lt; 2.2e-16\n***\n\n\ntotal precipitation\n-0.009\n0.003\n0.001\n**\n\n\nrelative humidity\n0.008\n0.002\n0.000\n***\n\n\nwind speed\n-0.049\n0.012\n0.000\n***"
  },
  {
    "objectID": "post/SISBayes/SISBayes2025_slides.html#determinants-of-biodiversity-in-the-global-malaise-trap",
    "href": "post/SISBayes/SISBayes2025_slides.html#determinants-of-biodiversity-in-the-global-malaise-trap",
    "title": "A Bayesian theory for estimation of biodiversity",
    "section": "Determinants of biodiversity in the global malaise trap",
    "text": "Determinants of biodiversity in the global malaise trap\n\nWe run the Hubbel regression model:\n\ncbind(n, y) ~ realm + habitat_type + temperature_2m + total_precipitation + relative_humidity + windspeed + month + year + ns(elev, df = 10) + ns(Latitude, df = 10) + ns(Longitude, df = 10)\n\nWe estimate \\hat{\\sigma} = 0.53 via maximum likelihood, which amounts to a square root growth.\nP-values are corrected for shared species via the sandwich estimator.\n\n\n| Variable                        | Estimate | Std. Error | Pr(&gt;|z|)     | Signif. |\n|---------------------------------|---------:|-----------:|-------------:|---------|\n| (Intercept)                     | -0.019   | 0.489      | 0.969        |         |\n| realm: Australasia              | -0.486   | 0.148      | 0.001        | **      |\n| realm: Indomalayan              | 0.089    | 0.142      | 0.529        |         |\n| realm: Nearctic                 | -0.150   | 0.469      | 0.750        |         |\n| realm: Neotropic                | 0.262    | 0.420      | 0.533        |         |\n| realm: Palearctic               | -0.005   | 0.187      | 0.977        |         |\n| habitat: Grassland              | -0.274   | 0.053      | 0.000        | ***     |\n| habitat: Mixed                  | 0.044    | 0.059      | 0.463        |         |\n| habitat: Tundra                 | -0.334   | 0.177      | 0.059        | .       |\n| habitat: Urban                  | -0.374   | 0.067      | 0.000        | ***     |\n| habitat: Wetland                | -0.087   | 0.069      | 0.210        |         |\n| temperature at 2m               | 0.049    | 0.005      | &lt; 2.2e-16    | ***     |\n| total precipitation             | -0.009   | 0.003      | 0.001        | **      |\n| relative humidity               | 0.008    | 0.002      | 0.000        | ***     |\n| wind speed                      | -0.049   | 0.012      | 0.000        | ***     |"
  },
  {
    "objectID": "post/SISBayes/SISBayes2025_slides.html#results-latitude-has-a-major-effect",
    "href": "post/SISBayes/SISBayes2025_slides.html#results-latitude-has-a-major-effect",
    "title": "A Bayesian theory for estimation of biodiversity",
    "section": "Results: latitude has a major effect!",
    "text": "Results: latitude has a major effect!"
  },
  {
    "objectID": "post/SISBayes/SISBayes2025_slides.html#summary-and-future-directions",
    "href": "post/SISBayes/SISBayes2025_slides.html#summary-and-future-directions",
    "title": "A Bayesian theory for estimation of biodiversity",
    "section": "Summary and future directions",
    "text": "Summary and future directions\n\nFisher’s \\alpha is the same quantity described by Hubbell’s unified neutral theory of biodiversity, and it has deep connections with the Dirichlet process.\n\n\n\n\nRigon, T., Hsu, C., and Dunson D.B. (2025+). A Bayesian theory for estimation of biodiversity. arXiv:2502.01333\n\n\n\n\nHubbell can be extended to regression model to make it covariate-dependent.\nDifferent growth rates are captured via \\sigma &lt; 0, \\sigma = 0, and \\sigma \\in (0,1).\nFrom here to infinity: next steps\n\nExtend to mixed models, allowing for random effects;\nMore on nonparametrics: GAMs\nFinish the paper! :)\n\nThank you for your attention!"
  },
  {
    "objectID": "post/SISBayes/SISBayes2025.html#specimens-and-bins-in-barcode-of-life",
    "href": "post/SISBayes/SISBayes2025.html#specimens-and-bins-in-barcode-of-life",
    "title": "A Bayesian theory for estimation of biodiversity",
    "section": "Specimens and BINs in Barcode of Life",
    "text": "Specimens and BINs in Barcode of Life"
  },
  {
    "objectID": "post/SISBayes/SISBayes2025.html#how-can-we-link-covariates-with-biodiversity",
    "href": "post/SISBayes/SISBayes2025.html#how-can-we-link-covariates-with-biodiversity",
    "title": "A Bayesian theory for estimation of biodiversity",
    "section": "How can we link covariates with biodiversity?",
    "text": "How can we link covariates with biodiversity?\n\nFor outcome Y_i and covariates \\bm{x}_i = (x_{i1}, \\ldots, x_{ip}) \\in \\mathbb{R}^p, i = 1, \\ldots, N, a generalized linear model (GLM) extends the linear model in non-Gaussian settings via three ingredients:\nThe exponential dispersion family directing each Y_i: \nf(y_i; \\theta_i, \\phi) = \\exp\\left\\{\\frac{y_i \\theta_i - b(\\theta_i)}{a_i(\\phi)} + c(y_i, \\phi)\\right\\},\n where \\theta_i \\in \\Theta is the natural parameter, \\phi &gt; 0 is the dispersion, and a_i,b,c are given functions.\nThe linear predictor \\eta_i = \\bm{x}_i^\\top \\bm{\\beta}, with \\bm{\\beta} \\in \\mathbb{R}^p.\nA monotonic link function g that links \\eta_i with the mean of Y_i, namely \\mu(\\theta_i) = \\mu_i: \n\\mathbb{E}(Y_i \\mid \\bm{x}_i) = \\mu_i = g^{-1}(\\eta_i).\n The link is canonical when g is such that \\theta_i = \\eta_i.\nIs there an exponential family that is related to a sample-specific diversity \\alpha_i?"
  },
  {
    "objectID": "post/SISBayes/SISBayes2025.html#the-hubbell-regression",
    "href": "post/SISBayes/SISBayes2025.html#the-hubbell-regression",
    "title": "A Bayesian theory for estimation of biodiversity",
    "section": "The Hubbell regression",
    "text": "The Hubbell regression\n\nThe number of distinct species from a DP is an exponential family! \n\\mathbb{P}(Y^{(n)} = y; \\alpha) = \\frac{\\alpha^y}{(\\alpha)_n}|s(n, y)|, \\qquad y \\in\\{ 1, \\ldots, n\\},\n where (\\alpha)_n = \\Gamma(\\alpha + n)/\\Gamma(\\alpha) and |s(n,y)| are signless Stirling numbers of the first kind.\n\n\n\n\n\n\n\nThe Hubbell regression is a GLM where:\n\nThe distinct species Y_i^{(n_i)} for i=1,\\dots,N are independent draws from the distribution \nf(y_i; \\theta_i) = \\exp \\left\\{y_i \\theta_i - \\log[\\Gamma(e^{\\theta_i} + n_i) - \\Gamma(e^{\\theta_i})] + \\log|s(n_i, y_i)|\\right\\},\n with \\alpha_i = e^{\\theta_i} whose mean and variance functions are \n\\mu_i = \\mu(\\theta_i, n_i) = \\sum_{j=0}^{n_i-1} \\frac{e^{\\theta_i}}{e^{\\theta_i} + j}, \\qquad\nV(\\mu(\\theta_i, n_i)) = \\sum_{j=0}^{n_i-1} \\frac{e^{\\theta_i}}{e^{\\theta_i} + j}\\left(1 - \\frac{e^{\\theta_i}}{e^{\\theta_i} + j}\\right).\n\nThe link function g depends on \\eta_i and n_i and satisfies \\mathbb{E}(Y_i\\mid \\bm{x}_i, n_i) = \\mu_i = g^{-1}(\\bm{x}_i^\\top \\bm{\\beta}, n_i)."
  },
  {
    "objectID": "post/SISBayes/SISBayes2025.html#the-canonical-link",
    "href": "post/SISBayes/SISBayes2025.html#the-canonical-link",
    "title": "A Bayesian theory for estimation of biodiversity",
    "section": "The canonical link",
    "text": "The canonical link\n\nUsing the canonical link, we have: \n\\log \\alpha_i = \\theta_i = \\bm{x}_i^\\top \\bm{\\beta} = \\eta_i, \\qquad\n\\mu_i = \\mu(\\bm{x}_i^\\top \\bm{\\beta}, n_i) = \\sum_{j = 0}^{n_i-1} \\frac{e^{\\bm{x}_i^\\top \\bm{\\beta}}}{e^{\\bm{x}_i^\\top \\bm{\\beta}} + j} = g^{-1}(\\bm{x}_i^\\top \\bm{\\beta}, n_i).\n\nCoefficients \\bm{\\beta} have the classic interpretation of log-linear models in terms of \\alpha-diversity\n\n\nIf x_{ip} ↑ by 1 unit → \\alpha_i ↑ by 100(e^{\\beta_p} - 1)\\%\n\n\nThe saturated model corresponds to the case in which \\hat{\\alpha}_i \\approx \\hat{\\alpha}^{\\text{Fisher}}_i. The null model corresponds to the case \\hat{\\alpha}_i = \\hat{\\alpha} for all sites, i.e. there is no variation in biodiversity across sampling sites.\n\n\n\n\n\n\n\nTheorem (Poisson regression with log-log offset)\n\n\n\nCall \\gamma = 0.5772\\dots the Euler’s constant. Under large values of n, when each n_i \\to \\infty we have \nY^{(n)}_i \\;\\dot{\\sim}\\; 1 + \\mathrm{Poisson}\\left(\\exp\\left\\{\\bm{x}_i^\\top\\bm{\\beta} + \\log(\\gamma + \\log n_i)\\right\\}\\right).\n Informally, we will say that \\hat{\\bm{\\beta}}^{\\mathrm{Hubbell}} \\approx \\hat{\\bm{\\beta}}^{\\mathrm{Poisson}} ."
  },
  {
    "objectID": "post/SISBayes/SISBayes2025.html#beyond-the-logarithmic-growth-non-canonical-links",
    "href": "post/SISBayes/SISBayes2025.html#beyond-the-logarithmic-growth-non-canonical-links",
    "title": "A Bayesian theory for estimation of biodiversity",
    "section": "Beyond the logarithmic growth: non-canonical links",
    "text": "Beyond the logarithmic growth: non-canonical links\n\nExtensions of the neutral theory of Hubbell (2001) proposes a power-law behavior with a dispersal limitation parameter \\omega \\ge 0 so that \n\\mathbb{E}\\left(Y_i^{(n)}\\right) = \\sum_{j = 0}^{n_i-1} j^{-\\omega} \\frac{\\alpha_i}{\\alpha_i + j}.\n\nOur proposal: a flexible growth depending on \\sigma &lt; 1 via non-canonical link \n\\mu_i = \\mu(\\bm{x}_i^\\top\\bm{\\beta}, n_i, \\sigma) = \\sum_{j = 0}^{n_i-1} \\frac{e^{\\bm{x}_i^\\top\\bm{\\beta}}}{e^{\\bm{x}_i^\\top\\bm{\\beta}} + j^{1-\\sigma}} = g^{-1}(\\bm{x}_i^\\top\\bm{\\beta}, n_i, \\sigma), \\qquad \\sigma &lt; 1.\n The parameter \\sigma plays the same role it has in Gibbs-type priors."
  },
  {
    "objectID": "post/SISBayes/SISBayes2025.html#regression-based-biodiversity-indexes",
    "href": "post/SISBayes/SISBayes2025.html#regression-based-biodiversity-indexes",
    "title": "A Bayesian theory for estimation of biodiversity",
    "section": "Regression-based biodiversity indexes",
    "text": "Regression-based biodiversity indexes\n\n\n\n\n\n\nTheorem (Regression-based diversity)\n\n\n\nFor the mean function of the a Hubbell regression model with the aforementioned non-canonical link, it holds \n\\frac{\\mu(\\bm{x}_i^\\top \\bm{\\beta}; n, \\sigma)}{c_n(\\sigma)} \\longrightarrow S_\\sigma(\\bm{x}_i^\\top\\bm{\\beta}), \\qquad n \\to \\infty,\n Moreover:\n\nIf \\sigma = 0, then c_n(\\sigma) = \\log n (logarithmic growth) and S_\\sigma(\\bm{x}_i^\\top\\bm{\\beta}) = e^{\\bm{x}_i^\\top\\bm{\\beta}} = \\alpha_i;\nIf \\sigma \\in (0, 1), then c_n(\\sigma) = n^{\\sigma} (polynomial growth) and S_\\sigma(\\bm{x}_i^\\top\\bm{\\beta}) = e^{\\bm{x}_i^\\top\\bm{\\beta}}/\\sigma = \\alpha_i / \\sigma;\n\nif \\sigma &lt; 0, then c_n(\\sigma) = 1 (finite richness) and S_\\sigma(\\bm{x}_i^\\top\\bm{\\beta}) \\approx C_\\sigma^{-1}e^{\\bm{x}_i^\\top\\bm{\\beta}/(1-\\sigma)} = C_\\sigma^{-1} \\alpha_i^{1/(1-\\sigma)}, with C_\\sigma a known constant.\n\n\n\n\n\nThis gives a novel notion of regression-based biodiversity."
  },
  {
    "objectID": "post/SISBayes/SISBayes2025.html#iteratively-reweighted-least-squares-irls",
    "href": "post/SISBayes/SISBayes2025.html#iteratively-reweighted-least-squares-irls",
    "title": "A Bayesian theory for estimation of biodiversity",
    "section": "Iteratively reweighted least squares (IRLS)",
    "text": "Iteratively reweighted least squares (IRLS)\n\nUpdate the mean: \n\\mu_i^{(t)} \\gets \\sum_{j = 0}^{n_i-1}\n\\frac{e^{\\eta_i^{(t)}}}{e^{\\eta_i^{(t)}} + j^{1-\\sigma}},\n\\qquad \\eta_i^{(t)} = \\bm{x}_i^\\top \\bm{\\beta}^{(t)}.\n\nNumerically retrieve the diversity \\alpha_i and compute variances: \n\\alpha_i^{(t)} \\gets\n\\texttt{uniroot}\\!\\left(\\sum_{j=0}^{n_i-1}\\frac{x}{x+j}=\\mu_i^{(t)}\\right)\\texttt{\\$root}, \\qquad v_i^{(t)} \\gets \\sum_{j=0}^{n_i-1}\nj\\,\\frac{\\alpha_i^{(t)}}{(\\alpha_i^{(t)}+j)^2}.\n\nCompute working weights: \nw_i^{(t)} \\gets \\frac{1}{v_i^{(t)}}\n\\sum_{j=0}^{n_i-1}\nj\\frac{\\alpha_i^{(t)}}\n     {(\\alpha_i^{(t)}+j^{1-\\sigma})^2}.\n\nUpdate the regression coefficients:\n\n\\bm{\\beta}^{(t+1)} \\gets\n(\\bm{X}^\\top \\bm{W}^{(t)} \\bm{X})^{-1}\n\\bm{X}^\\top \\bm{W}^{(t)} \\bm{z}^{(t)}, \\qquad\nz_i^{(t)} \\gets \\eta_i^{(t)} +\n\\frac{y_i - \\mu_i^{(t)}}{(w_i^{(t)}v_i^{(t)})^{1/2}}."
  },
  {
    "objectID": "post/SISBayes/SISBayes2025.html#composite-marginal-likelihood-and-standard-errors",
    "href": "post/SISBayes/SISBayes2025.html#composite-marginal-likelihood-and-standard-errors",
    "title": "A Bayesian theory for estimation of biodiversity",
    "section": "Composite marginal likelihood and standard errors",
    "text": "Composite marginal likelihood and standard errors\n\nThe assumption behind GLMs is that observations are independent. In our case, we might have seen the same species (or BIN) at two different locations!\nThis information is lost when calculating the number Y_i^{(n_i)}. However, we can interpret \nL(\\boldsymbol{\\beta}) \\propto\n\\prod_{i=1}^N\n\\frac{\\alpha_i^{y_i}}\n   {(\\alpha_i)_{n_i}},\n as a composite marginal likelihood. The estimating equation is still unbiased and estimates consistent.\nWe propose spatially heteroskedastic-consistent standard errors (Conley 1999) that use the Jaccard Index (or similar indices) between two samples:\n\\widehat{\\mathrm{se}}(\\hat{\\boldsymbol{\\beta}}) =\n(\\bm{X}^\\top \\hat{\\bm{W}}\\bm{X})^{-1}\n\\textstyle\\sum_{ik}\n{\\color{#8B0000}{s_{ik}}}\\,\n{\\color{blue}{\\hat{\\bm{u}}_i\\hat{\\bm{u}}_k^\\top}}\\,\n(\\bm{X}^\\top \\hat{\\bm{W}}\\bm{X})^{-1},\n\\qquad\n{\\color{blue}{\\hat{\\bm{u}}_i}} =\n\\frac{\\partial}{\\partial\\boldsymbol{\\beta}}\n\\log \\mathcal{L}(\\hat{\\boldsymbol{\\beta}}),\n where {\\color{#8B0000}{s_{ik}}} \\in [0,1] is the fraction of shared BINs between locations i and k."
  },
  {
    "objectID": "post/SISBayes/SISBayes2025.html#the-package-hubbellglm",
    "href": "post/SISBayes/SISBayes2025.html#the-package-hubbellglm",
    "title": "A Bayesian theory for estimation of biodiversity",
    "section": "The package HubbellGLM",
    "text": "The package HubbellGLM"
  },
  {
    "objectID": "post/SISBayes/SISBayes2025.html#determinants-of-biodiversity-in-the-global-malaise-trap",
    "href": "post/SISBayes/SISBayes2025.html#determinants-of-biodiversity-in-the-global-malaise-trap",
    "title": "A Bayesian theory for estimation of biodiversity",
    "section": "Determinants of biodiversity in the global malaise trap",
    "text": "Determinants of biodiversity in the global malaise trap\n\nWe run the Hubbel regression model:\n\ncbind(n, y) ~ realm + habitat_type + temperature_2m + total_precipitation + relative_humidity + windspeed + month + year + ns(elev, df = 10) + ns(Latitude, df = 10) + ns(Longitude, df = 10)\n\nWe estimate \\hat{\\sigma} = 0.53 via maximum likelihood, which amounts to a square root growth.\nP-values are corrected for shared species via the sandwich estimator.\n\n\n| Variable                        | Estimate | Std. Error | Pr(&gt;|z|)     | Signif. |\n|---------------------------------|---------:|-----------:|-------------:|---------|\n| (Intercept)                     | -0.019   | 0.489      | 0.969        |         |\n| realm: Australasia              | -0.486   | 0.148      | 0.001        | **      |\n| realm: Indomalayan              | 0.089    | 0.142      | 0.529        |         |\n| realm: Nearctic                 | -0.150   | 0.469      | 0.750        |         |\n| realm: Neotropic                | 0.262    | 0.420      | 0.533        |         |\n| realm: Palearctic               | -0.005   | 0.187      | 0.977        |         |\n| habitat: Grassland              | -0.274   | 0.053      | 0.000        | ***     |\n| habitat: Mixed                  | 0.044    | 0.059      | 0.463        |         |\n| habitat: Tundra                 | -0.334   | 0.177      | 0.059        | .       |\n| habitat: Urban                  | -0.374   | 0.067      | 0.000        | ***     |\n| habitat: Wetland                | -0.087   | 0.069      | 0.210        |         |\n| temperature at 2m               | 0.049    | 0.005      | &lt; 2.2e-16    | ***     |\n| total precipitation             | -0.009   | 0.003      | 0.001        | **      |\n| relative humidity               | 0.008    | 0.002      | 0.000        | ***     |\n| wind speed                      | -0.049   | 0.012      | 0.000        | ***     |"
  },
  {
    "objectID": "post/SISBayes/SISBayes2025.html#results-latitude-has-a-major-effect",
    "href": "post/SISBayes/SISBayes2025.html#results-latitude-has-a-major-effect",
    "title": "A Bayesian theory for estimation of biodiversity",
    "section": "Results: latitude has a major effect!",
    "text": "Results: latitude has a major effect!"
  },
  {
    "objectID": "post/SISBayes/SISBayes2025.html#summary-and-future-directions",
    "href": "post/SISBayes/SISBayes2025.html#summary-and-future-directions",
    "title": "A Bayesian theory for estimation of biodiversity",
    "section": "Summary and future directions",
    "text": "Summary and future directions\n\nFisher’s \\alpha is the same quantity described by Hubbell’s unified neutral theory of biodiversity, and it has deep connections with the Dirichlet process.\n\n\n\n\n\n\n\nRigon, T., Hsu, C., and Dunson D.B. (2025+). A Bayesian theory for estimation of biodiversity. arXiv:2502.01333\n\n\n\n\nThe Hubbell regression extends the neutral theory in a covariate-dependent GLM framework.\nDifferent growth rates are captured via \\sigma &lt; 0, \\sigma = 0, and \\sigma \\in (0,1).\nFrom here to infinity: next steps\n\nExtend to mixed models, allowing for random effects;\nMore on nonparametrics: GAMs\nFinish the paper! :)\n\nThank you for your attention!"
  },
  {
    "objectID": "post/SISBayes/GRASPA.html",
    "href": "post/SISBayes/GRASPA.html",
    "title": "A Bayesian theory for estimation of biodiversity",
    "section": "",
    "text": "Ching-Lung Hsu\n(Duke University)\n\n\n\n\n\n\n\n\nAlessandro Zito\n(Harvard School of Public Health)\n\n\n\n\n\n\n\n\nDavid Dunson\n(Duke University)"
  },
  {
    "objectID": "post/SISBayes/GRASPA.html#warm-thanks",
    "href": "post/SISBayes/GRASPA.html#warm-thanks",
    "title": "A Bayesian theory for estimation of biodiversity",
    "section": "",
    "text": "Ching-Lung Hsu\n(Duke University)\n\n\n\n\n\n\n\n\nAlessandro Zito\n(Harvard School of Public Health)\n\n\n\n\n\n\n\n\nDavid Dunson\n(Duke University)"
  },
  {
    "objectID": "post/SISBayes/GRASPA.html#a-journey-into-statistical-biodiversity",
    "href": "post/SISBayes/GRASPA.html#a-journey-into-statistical-biodiversity",
    "title": "A Bayesian theory for estimation of biodiversity",
    "section": "A journey into statistical biodiversity",
    "text": "A journey into statistical biodiversity"
  },
  {
    "objectID": "post/SISBayes/GRASPA.html#approximately-80-years-ago-fisher1943",
    "href": "post/SISBayes/GRASPA.html#approximately-80-years-ago-fisher1943",
    "title": "A Bayesian theory for estimation of biodiversity",
    "section": "Approximately 80 years ago… Fisher et al. (1943)",
    "text": "Approximately 80 years ago… Fisher et al. (1943)"
  },
  {
    "objectID": "post/SISBayes/GRASPA.html#early-ideas-good_1953-good_toulmin_1956",
    "href": "post/SISBayes/GRASPA.html#early-ideas-good_1953-good_toulmin_1956",
    "title": "A Bayesian theory for estimation of biodiversity",
    "section": "Early ideas (Good 1953; Good and Toulmin 1956)",
    "text": "Early ideas (Good 1953; Good and Toulmin 1956)"
  },
  {
    "objectID": "post/SISBayes/GRASPA.html#the-ewens-sampling-formula-ewens72",
    "href": "post/SISBayes/GRASPA.html#the-ewens-sampling-formula-ewens72",
    "title": "A Bayesian theory for estimation of biodiversity",
    "section": "The Ewens sampling formula Ewens (1972)",
    "text": "The Ewens sampling formula Ewens (1972)"
  },
  {
    "objectID": "post/SISBayes/GRASPA.html#the-dirichlet-process-ferguson1973",
    "href": "post/SISBayes/GRASPA.html#the-dirichlet-process-ferguson1973",
    "title": "A Bayesian theory for estimation of biodiversity",
    "section": "The Dirichlet process (Ferguson 1973)",
    "text": "The Dirichlet process (Ferguson 1973)"
  },
  {
    "objectID": "post/SISBayes/GRASPA.html#and-species-sampling-models-pitman1996",
    "href": "post/SISBayes/GRASPA.html#and-species-sampling-models-pitman1996",
    "title": "A Bayesian theory for estimation of biodiversity",
    "section": "… and species sampling models (Pitman 1996)",
    "text": "… and species sampling models (Pitman 1996)"
  },
  {
    "objectID": "post/SISBayes/GRASPA.html#ewens-formula-governs-the-neutral-theory-hubbell_2001",
    "href": "post/SISBayes/GRASPA.html#ewens-formula-governs-the-neutral-theory-hubbell_2001",
    "title": "A Bayesian theory for estimation of biodiversity",
    "section": "Ewens formula governs the neutral theory! (Hubbell 2001)",
    "text": "Ewens formula governs the neutral theory! (Hubbell 2001)"
  },
  {
    "objectID": "post/SISBayes/GRASPA.html#all-ties-together-mccullagh_2016",
    "href": "post/SISBayes/GRASPA.html#all-ties-together-mccullagh_2016",
    "title": "A Bayesian theory for estimation of biodiversity",
    "section": "All ties together (McCullagh 2016)",
    "text": "All ties together (McCullagh 2016)"
  },
  {
    "objectID": "post/SISBayes/GRASPA.html#species-sampling-models",
    "href": "post/SISBayes/GRASPA.html#species-sampling-models",
    "title": "A Bayesian theory for estimation of biodiversity",
    "section": "Species sampling models",
    "text": "Species sampling models\n\nLet X_1,\\dots,X_n be some collection of species or taxa. Suppose X_n are conditionally iid samples from a species sampling model: \n(X_n \\mid \\tilde{p}) \\overset{\\text{iid}}{\\sim} \\tilde{p}, \\qquad\n\\tilde{p}(\\cdot) = \\sum_{h=1}^{\\infty}\\pi_h \\delta_{Z_h}(\\cdot),\n\\qquad n \\ge 1,\n where (\\pi_h)_{h \\ge 1} is a set of probabilities (species proportions) and Z_h represent distinct species.\nThe discreteness of \\tilde{p} identifies Y^{(n)}=y distinct taxa X_1^*, \\ldots, X_y^* with frequencies n_1, \\ldots, n_y, called abundances in ecology.\nGibbs-type priors have emerged as the most natural extension of the DP (De Blasi et al. 2015).\nThe predictive distribution of a Gibbs-type prior is given by: \n\\mathbb{P}(X_{n+1} \\in \\cdot \\mid X_1,\\dots,X_n)\n= \\frac{V_{n+1, y+1}}{V_{n,y}}P(\\cdot)\n+ \\frac{V_{n+1, y}}{V_{n,y}}\\sum_{j=1}^y(n_j - \\sigma)\\delta_{X^*_j}(\\cdot).\n where (a)_{n} denotes a rising factorial, and \\sigma &lt; 1 is the discount parameter. The V_{n,y}’s are non-negative weights satisfying a forward recursive equation."
  },
  {
    "objectID": "post/SISBayes/GRASPA.html#three-notable-examples",
    "href": "post/SISBayes/GRASPA.html#three-notable-examples",
    "title": "A Bayesian theory for estimation of biodiversity",
    "section": "Three notable examples",
    "text": "Three notable examples\n\nDirichlet multinomial (\\sigma &lt; 0). For \\sigma &lt; 0 and H \\in \\mathbb{N}, a valid set of Gibbs coefficients is given by: \nV_{n, y}(\\sigma, H) := \\frac{|\\sigma|^{y-1}\\prod_{j=1}^{y-1}(H - j)}{(H|\\sigma| +1)_{n-1}}\n\\mathbb{I}(y \\le H).\n\nDirichlet process (\\sigma = 0). For \\sigma = 0 and \\alpha &gt; 0, a valid set of Gibbs coefficients is given by: \nV_{n, y}(\\alpha) := \\frac{\\alpha^y}{(\\alpha)_n}.\n\n\\sigma-stable Poisson–Kingman process (0 &lt; \\sigma &lt; 1). For \\sigma \\in (0,1) and gamma \\gamma &gt; 0, a valid set of Gibbs coefficients is defined as: \nV_{n, y}(\\sigma, \\gamma) := \\frac{\\sigma^y\\gamma^y}{\\Gamma(n-y\\sigma)f_{\\sigma}\\left(\\gamma^{-1/\\sigma}\\right)}\n\\int_{0}^{1}s^{n-1-y\\sigma}f_{\\sigma}\\left(\\left(1-s\\right)\\gamma^{-1/\\sigma}\\right)\\, \\mathrm{d}s,\n where f_\\sigma(t) = (\\pi)^{-1} \\sum_{h=1}^\\infty (-1)^{h+1}\\sin(h \\pi \\sigma )\\Gamma(h\\sigma + 1) / t^{h\\sigma + 1}."
  },
  {
    "objectID": "post/SISBayes/GRASPA.html#a-characterization-theorem",
    "href": "post/SISBayes/GRASPA.html#a-characterization-theorem",
    "title": "A Bayesian theory for estimation of biodiversity",
    "section": "A characterization theorem",
    "text": "A characterization theorem\n\n\n\n\n\n\nTheorem (Gnedin and Pitman 2005)\n\n\n\nThe Gibbs coefficients V_{n, y} satisfy the recursive equation in the following three cases:\n\nIf \\sigma &lt; 0, whenever V_{n, y} = \\sum_{h=1}^\\infty V_{n, y}(\\sigma, h) p(h), for some discrete random variable H \\in \\mathbb{N} with pdf p(h), where the V_{n, y}(\\sigma, h)’s are those of the Dirichlet multinomial.\nIf \\sigma = 0, whenever V_{n, y} = \\int_{\\mathbb{R}^+} V_{n, y}(\\alpha) p(\\mathrm{d}\\alpha), for some positive random variable \\alpha with probability measure p(\\mathrm{d}\\alpha), where the V_{n, y}(\\alpha)’s are those of the Dirichlet process.\nIf \\sigma \\in (0,1), whenever V_{n, y} = \\int_{\\mathbb{R}^+} V_{n, y}(\\sigma, \\gamma) p(\\mathrm{d}\\gamma), for some positive random variable \\gamma with probability measure p(\\mathrm{d}\\gamma), where the V_{n, y}(\\sigma, \\gamma)’s are those of the \\sigma-stable PK.\n\n\n\n\nThe Dirichlet multinomial, Dirichlet process, and \\sigma-stable PK form the foundation of any Gibbs-type prior.\nIn fact, any Gibbs-type process can be represented hierarchically, involving a suitable prior distribution for the key parameters H, \\alpha, and \\gamma."
  },
  {
    "objectID": "post/SISBayes/GRASPA.html#the-quantification-of-biodiversity-lijoi2007",
    "href": "post/SISBayes/GRASPA.html#the-quantification-of-biodiversity-lijoi2007",
    "title": "A Bayesian theory for estimation of biodiversity",
    "section": "The quantification of biodiversity (Lijoi et al. 2007a)",
    "text": "The quantification of biodiversity (Lijoi et al. 2007a)\n\nThe simplest measure of biodiversity is arguably the taxon richness Y^{(n)} = y.\nA priori, the distribution of Y^{(n)} induced by a Gibbs-type prior has a simple form: \n\\mathbb{P}(Y^{(n)} = y)=V_{n,y}\\frac{\\mathscr{C}(n,y;\\sigma)}{\\sigma^y},\n where \\mathscr{C}(n, y;\\sigma) denotes a generalized factorial coefficient.\nThe a priori expectations \\mathbb{E}(K_1),\\dots,\\mathbb{E}(K_n) define a model-based rarefaction curve.\nThe posterior distribution of the number of previously unobserved taxa Y_m^{(n)} is \n\\mathbb{P}(Y_m^{(n)}= j \\mid X_1,\\dots,X_n)=\\frac{V_{n + m, y + j}}{V_{n, y}}\\frac{\\mathscr{C}(m, j;\\sigma, -n + y\\sigma)}{\\sigma^j}, \\qquad j=0,\\dots,m,\n where \\mathscr{C}(m, j;\\sigma, -n + y\\sigma) is the noncentral generalized factorial coefficient.\nThe posterior expectations \\mathbb{E}(Y^{(n+1)} \\mid Y^{(n)} = y), \\dots, \\mathbb{E}(Y^{(n + m)} \\mid Y^{(n)} = y) represents a model-based extrapolation of the accumulation curve."
  },
  {
    "objectID": "post/SISBayes/GRASPA.html#rarefaction-and-extrapolation",
    "href": "post/SISBayes/GRASPA.html#rarefaction-and-extrapolation",
    "title": "A Bayesian theory for estimation of biodiversity",
    "section": "Rarefaction and extrapolation",
    "text": "Rarefaction and extrapolation"
  },
  {
    "objectID": "post/SISBayes/GRASPA.html#the-sigma-diversity-pitman2003",
    "href": "post/SISBayes/GRASPA.html#the-sigma-diversity-pitman2003",
    "title": "A Bayesian theory for estimation of biodiversity",
    "section": "The \\sigma-diversity (Pitman 2003)",
    "text": "The \\sigma-diversity (Pitman 2003)\n\nLet Y^{(n)} be the number of distinct values arising from a Gibbs-type prior:\n\nLet \\sigma &lt; 0 and V_{n, y}(\\sigma, H) be the weights of a Dirichlet multinomial, then Y^{(n)} \\rightarrow H a.s.\nLet \\sigma = 0 and V_{n, y}(\\alpha) be the weights of a Dirichlet process, then Y^{(n)} / \\log(n) \\rightarrow \\alpha a.s.\nLet \\sigma \\in (0,1) and V_{n, y}(\\sigma, \\gamma) be the weights of a \\sigma-stable PK, then Y^{(n)} / n^\\sigma \\rightarrow \\gamma a.s.\n\nMoreover, consider a generic set of weights V_{n, y} and let \nc_\\sigma(n) =\n\\begin{cases}\n1, & \\sigma &lt;0,\\\\[2mm]\n\\log(n), & \\sigma = 0,\\\\[1mm]\nn^{\\sigma}, & \\sigma \\in (0,1)\n\\end{cases}\n Then, as n \\rightarrow \\infty: \n\\frac{Y^{(n)}}{c_\\sigma(n)} \\overset{\\textup{a.s.}}{\\longrightarrow} S_{\\sigma}.\n The r.v. S_{\\sigma} is the \\sigma-diversity and its distribution coincides with the prior for H, \\alpha, and \\gamma."
  },
  {
    "objectID": "post/SISBayes/GRASPA.html#the-sigma-diversity-pitman2003-1",
    "href": "post/SISBayes/GRASPA.html#the-sigma-diversity-pitman2003-1",
    "title": "A Bayesian theory for estimation of biodiversity",
    "section": "The \\sigma-diversity (Pitman 2003)",
    "text": "The \\sigma-diversity (Pitman 2003)\n\nBroadly speaking, the \\sigma-diversity can be seen as a rescaled richness measure.\nThe \\sigma-diversity is deterministic and assumed to be known in the Dirichlet multinomial, Dirichlet process, and \\sigma-stable PK.\nHowever, the \\sigma-diversities H, \\alpha, or \\gamma are typically unknown, and they can be estimated employing a prior distribution, leading to a Gibbs-type prior.\nThe posterior law of the \\sigma-diversity is a key quantity for measuring biodiversity. The posterior law of S_\\sigma has an elegant connection with accumulation curves.\n\n\n\n\n\n\n\nTheorem (Rigon et al. 2025b)\n\n\n\nLet X_1,\\dots,X_{n+m} be a sample from a Gibbs-type prior with Y^{(n+m)} distinct values. Then:\n\n\\left(\\frac{Y^{(n+m)}}{c_\\sigma(m)} \\mid X_1,\\dots,X_n \\right) \\overset{d}{\\longrightarrow} \\tilde{S}_{\\sigma}, \\qquad\n\\tilde{S}_\\sigma \\overset{d}{=} (S_\\sigma \\mid X_1,\\dots,X_n),\n as m \\rightarrow \\infty, where S_\\sigma is the \\sigma-diversity."
  },
  {
    "objectID": "post/SISBayes/GRASPA.html#tree-species-in-the-amazon-basin-hubbell2008",
    "href": "post/SISBayes/GRASPA.html#tree-species-in-the-amazon-basin-hubbell2008",
    "title": "A Bayesian theory for estimation of biodiversity",
    "section": "Tree species in the Amazon Basin (Hubbell et al. 2008)",
    "text": "Tree species in the Amazon Basin (Hubbell et al. 2008)\n\n\n\n\n\n\n\n\nHow to get a Bayesian estimator for the biodiversity? We can place a prior on \\alpha and the compute its posterior distribution using Bayes theorem.\nPosterior distribution of the \\sigma-diversity \\alpha, under a (conjugate) Stirling-gamma prior (Zito et al. 2024). The dotted lines represent 95% credible intervals. The dashed line is the posterior mean."
  },
  {
    "objectID": "post/SISBayes/GRASPA.html#historical-summary-and-extensions",
    "href": "post/SISBayes/GRASPA.html#historical-summary-and-extensions",
    "title": "A Bayesian theory for estimation of biodiversity",
    "section": "Historical summary and extensions",
    "text": "Historical summary and extensions\n\nThe fundamental biodiversity number \\alpha is a growth-adjusted richness: \n\\frac{Y^{(n)}}{\\log n} \\stackrel{\\mathrm{a.s.}}{\\longrightarrow} \\alpha, \\qquad n\\to \\infty\n\nThe \\alpha from the neutral theory, DP and Fisher log-series is the same for large n: \n\\underbrace{y = \\hat{\\alpha}^\\mathrm{Fisher}\\log\\Big(1 + \\frac{n}{\\hat{\\alpha}^\\mathrm{Fisher}}\\Big)}_{\\text{Solution from Fisher}} \\ \\stackrel{\\text{Under large } n}{\\approx} \\ \\underbrace{\\sum_{j = 1}^{n} \\frac{\\hat{\\alpha}^{\\text{DP}}}{\\hat{\\alpha}^{\\text{DP}} + j - 1} = y}_{\\text{MLE from Hubbell and DP}}\n\nThere are a lot of possible extensions. This is a very biased list:\n\nThe choice \\sigma \\in (0, 1) allows for polynomial growth (Lijoi et al. 2007b; Pitman and Yor 1997) while \\sigma &lt; 0 leads to a finite richness (Gnedin 2010; Lijoi et al. 2020).\nMultiple samples, a.k.a. partial exchangeability (Camerlenghi et al. 2019; Franzolini et al. 2025)\nEnriched processes, taxonomical data (Rigon et al. 2025a; Rigon et al. 2025b; Zito et al. 2023).\nIncidence data and feature models (Ghilotti et al. 2025).\nWhat about covariates?"
  },
  {
    "objectID": "post/SISBayes/GRASPA.html#specimens-and-bins-in-barcode-of-life",
    "href": "post/SISBayes/GRASPA.html#specimens-and-bins-in-barcode-of-life",
    "title": "A Bayesian theory for estimation of biodiversity",
    "section": "Specimens and BINs in Barcode of Life",
    "text": "Specimens and BINs in Barcode of Life"
  },
  {
    "objectID": "post/SISBayes/GRASPA.html#how-can-we-link-covariates-with-biodiversity",
    "href": "post/SISBayes/GRASPA.html#how-can-we-link-covariates-with-biodiversity",
    "title": "A Bayesian theory for estimation of biodiversity",
    "section": "How can we link covariates with biodiversity?",
    "text": "How can we link covariates with biodiversity?\n\nFor outcome Y_i and covariates \\bm{x}_i = (x_{i1}, \\ldots, x_{ip}) \\in \\mathbb{R}^p, i = 1, \\ldots, N, a generalized linear model (GLM) extends the linear model in non-Gaussian settings via three ingredients:\nThe exponential dispersion family directing each Y_i: \nf(y_i; \\theta_i, \\phi) = \\exp\\left\\{\\frac{y_i \\theta_i - b(\\theta_i)}{a_i(\\phi)} + c(y_i, \\phi)\\right\\},\n where \\theta_i \\in \\Theta is the natural parameter, \\phi &gt; 0 is the dispersion, and a_i,b,c are given functions.\nThe linear predictor \\eta_i = \\bm{x}_i^\\top \\bm{\\beta}, with \\bm{\\beta} \\in \\mathbb{R}^p.\nA monotonic link function g that links \\eta_i with the mean of Y_i, namely \\mu(\\theta_i) = \\mu_i: \n\\mathbb{E}(Y_i \\mid \\bm{x}_i) = \\mu_i = g^{-1}(\\eta_i).\n The link is canonical when g is such that \\theta_i = \\eta_i.\nIs there an exponential family that is related to a sample-specific diversity \\alpha_i?"
  },
  {
    "objectID": "post/SISBayes/GRASPA.html#the-hubbell-regression",
    "href": "post/SISBayes/GRASPA.html#the-hubbell-regression",
    "title": "A Bayesian theory for estimation of biodiversity",
    "section": "The Hubbell regression",
    "text": "The Hubbell regression\n\nThe number of distinct species from a DP is an exponential family! \n\\mathbb{P}(Y^{(n)} = y; \\alpha) = \\frac{\\alpha^y}{(\\alpha)_n}|s(n, y)|, \\qquad y \\in\\{ 1, \\ldots, n\\},\n where (\\alpha)_n = \\Gamma(\\alpha + n)/\\Gamma(\\alpha) and |s(n,y)| are signless Stirling numbers of the first kind.\n\n\n\n\n\n\n\nThe Hubbell regression is a GLM where:\n\nThe distinct species Y_i^{(n_i)} for i=1,\\dots,N are independent draws from the distribution \nf(y_i; \\theta_i) = \\exp \\left\\{y_i \\theta_i - \\log[\\Gamma(e^{\\theta_i} + n_i) - \\Gamma(e^{\\theta_i})] + \\log|s(n_i, y_i)|\\right\\},\n with \\alpha_i = e^{\\theta_i} whose mean and variance functions are \n\\mu_i = \\mu(\\theta_i, n_i) = \\sum_{j=0}^{n_i-1} \\frac{e^{\\theta_i}}{e^{\\theta_i} + j}, \\qquad\nV(\\mu(\\theta_i, n_i)) = \\sum_{j=0}^{n_i-1} \\frac{e^{\\theta_i}}{e^{\\theta_i} + j}\\left(1 - \\frac{e^{\\theta_i}}{e^{\\theta_i} + j}\\right).\n\nThe link function g depends on \\eta_i and n_i and satisfies \\mathbb{E}(Y_i\\mid \\bm{x}_i, n_i) = \\mu_i = g^{-1}(\\bm{x}_i^\\top \\bm{\\beta}, n_i)."
  },
  {
    "objectID": "post/SISBayes/GRASPA.html#the-canonical-link",
    "href": "post/SISBayes/GRASPA.html#the-canonical-link",
    "title": "A Bayesian theory for estimation of biodiversity",
    "section": "The canonical link",
    "text": "The canonical link\n\nUsing the canonical link, we have: \n\\log \\alpha_i = \\theta_i = \\bm{x}_i^\\top \\bm{\\beta} = \\eta_i, \\qquad\n\\mu_i = \\mu(\\bm{x}_i^\\top \\bm{\\beta}, n_i) = \\sum_{j = 0}^{n_i-1} \\frac{e^{\\bm{x}_i^\\top \\bm{\\beta}}}{e^{\\bm{x}_i^\\top \\bm{\\beta}} + j} = g^{-1}(\\bm{x}_i^\\top \\bm{\\beta}, n_i).\n\nCoefficients \\bm{\\beta} have the classic interpretation of log-linear models in terms of \\alpha-diversity\n\n\nIf x_{ip} ↑ by 1 unit → \\alpha_i ↑ by 100(e^{\\beta_p} - 1)\\%\n\n\nThe saturated model corresponds to the case in which \\hat{\\alpha}_i \\approx \\hat{\\alpha}^{\\text{Fisher}}_i. The null model corresponds to the case \\hat{\\alpha}_i = \\hat{\\alpha} for all sites, i.e. there is no variation in biodiversity across sampling sites.\n\n\n\n\n\n\n\nTheorem (Poisson regression with log-log offset)\n\n\n\nCall \\gamma = 0.5772\\dots the Euler’s constant. Under large values of n, when each n_i \\to \\infty we have \nY^{(n)}_i \\;\\dot{\\sim}\\; 1 + \\mathrm{Poisson}\\left(\\exp\\left\\{\\bm{x}_i^\\top\\bm{\\beta} + \\log(\\gamma + \\log n_i)\\right\\}\\right).\n Informally, we will say that \\hat{\\bm{\\beta}}^{\\mathrm{Hubbell}} \\approx \\hat{\\bm{\\beta}}^{\\mathrm{Poisson}} ."
  },
  {
    "objectID": "post/SISBayes/GRASPA.html#beyond-the-logarithmic-growth-non-canonical-links",
    "href": "post/SISBayes/GRASPA.html#beyond-the-logarithmic-growth-non-canonical-links",
    "title": "A Bayesian theory for estimation of biodiversity",
    "section": "Beyond the logarithmic growth: non-canonical links",
    "text": "Beyond the logarithmic growth: non-canonical links\n\nExtensions of the neutral theory of Hubbell (2001) proposes a power-law behavior with a dispersal limitation parameter \\omega \\ge 0 so that \n\\mathbb{E}\\left(Y_i^{(n)}\\right) = \\sum_{j = 0}^{n_i-1} j^{-\\omega} \\frac{\\alpha_i}{\\alpha_i + j}.\n\nOur proposal: a flexible growth depending on \\sigma &lt; 1 via non-canonical link \n\\mu_i = \\mu(\\bm{x}_i^\\top\\bm{\\beta}, n_i, \\sigma) = \\sum_{j = 0}^{n_i-1} \\frac{e^{\\bm{x}_i^\\top\\bm{\\beta}}}{e^{\\bm{x}_i^\\top\\bm{\\beta}} + j^{1-\\sigma}} = g^{-1}(\\bm{x}_i^\\top\\bm{\\beta}, n_i, \\sigma), \\qquad \\sigma &lt; 1.\n The parameter \\sigma plays the same role it has in Gibbs-type priors."
  },
  {
    "objectID": "post/SISBayes/GRASPA.html#regression-based-biodiversity-indexes",
    "href": "post/SISBayes/GRASPA.html#regression-based-biodiversity-indexes",
    "title": "A Bayesian theory for estimation of biodiversity",
    "section": "Regression-based biodiversity indexes",
    "text": "Regression-based biodiversity indexes\n\n\n\n\n\n\nTheorem (Regression-based diversity)\n\n\n\nFor the mean function of the a Hubbell regression model with the aforementioned non-canonical link, it holds \n\\frac{\\mu(\\bm{x}_i^\\top \\bm{\\beta}; n, \\sigma)}{c_n(\\sigma)} \\longrightarrow S_\\sigma(\\bm{x}_i^\\top\\bm{\\beta}), \\qquad n \\to \\infty,\n Moreover:\n\nIf \\sigma = 0, then c_n(\\sigma) = \\log n (logarithmic growth) and S_\\sigma(\\bm{x}_i^\\top\\bm{\\beta}) = e^{\\bm{x}_i^\\top\\bm{\\beta}} = \\alpha_i;\nIf \\sigma \\in (0, 1), then c_n(\\sigma) = n^{\\sigma} (polynomial growth) and S_\\sigma(\\bm{x}_i^\\top\\bm{\\beta}) = e^{\\bm{x}_i^\\top\\bm{\\beta}}/\\sigma = \\alpha_i / \\sigma;\n\nif \\sigma &lt; 0, then c_n(\\sigma) = 1 (finite richness) and S_\\sigma(\\bm{x}_i^\\top\\bm{\\beta}) \\approx C_\\sigma^{-1}e^{\\bm{x}_i^\\top\\bm{\\beta}/(1-\\sigma)} = C_\\sigma^{-1} \\alpha_i^{1/(1-\\sigma)}, with C_\\sigma a known constant.\n\n\n\n\n\nThis gives a novel notion of regression-based biodiversity."
  },
  {
    "objectID": "post/SISBayes/GRASPA.html#iteratively-reweighted-least-squares-irls",
    "href": "post/SISBayes/GRASPA.html#iteratively-reweighted-least-squares-irls",
    "title": "A Bayesian theory for estimation of biodiversity",
    "section": "Iteratively reweighted least squares (IRLS)",
    "text": "Iteratively reweighted least squares (IRLS)\n\nUpdate the mean: \n\\mu_i^{(t)} \\gets \\sum_{j = 0}^{n_i-1}\n\\frac{e^{\\eta_i^{(t)}}}{e^{\\eta_i^{(t)}} + j^{1-\\sigma}},\n\\qquad \\eta_i^{(t)} = \\bm{x}_i^\\top \\bm{\\beta}^{(t)}.\n\nNumerically retrieve the diversity \\alpha_i and compute variances: \n\\alpha_i^{(t)} \\gets\n\\texttt{uniroot}\\!\\left(\\sum_{j=0}^{n_i-1}\\frac{x}{x+j}=\\mu_i^{(t)}\\right)\\texttt{\\$root}, \\qquad v_i^{(t)} \\gets \\sum_{j=0}^{n_i-1}\nj\\,\\frac{\\alpha_i^{(t)}}{(\\alpha_i^{(t)}+j)^2}.\n\nCompute working weights: \nw_i^{(t)} \\gets \\frac{1}{v_i^{(t)}}\n\\sum_{j=0}^{n_i-1}\nj\\frac{\\alpha_i^{(t)}}\n     {(\\alpha_i^{(t)}+j^{1-\\sigma})^2}.\n\nUpdate the regression coefficients:\n\n\\bm{\\beta}^{(t+1)} \\gets\n(\\bm{X}^\\top \\bm{W}^{(t)} \\bm{X})^{-1}\n\\bm{X}^\\top \\bm{W}^{(t)} \\bm{z}^{(t)}, \\qquad\nz_i^{(t)} \\gets \\eta_i^{(t)} +\n\\frac{y_i - \\mu_i^{(t)}}{(w_i^{(t)}v_i^{(t)})^{1/2}}."
  },
  {
    "objectID": "post/SISBayes/GRASPA.html#composite-marginal-likelihood-and-standard-errors",
    "href": "post/SISBayes/GRASPA.html#composite-marginal-likelihood-and-standard-errors",
    "title": "A Bayesian theory for estimation of biodiversity",
    "section": "Composite marginal likelihood and standard errors",
    "text": "Composite marginal likelihood and standard errors\n\nThe assumption behind GLMs is that observations are independent. In our case, we might have seen the same species (or BIN) at two different locations!\nThis information is lost when calculating the number Y_i^{(n_i)}. However, we can interpret \nL(\\boldsymbol{\\beta}) \\propto\n\\prod_{i=1}^N\n\\frac{\\alpha_i^{y_i}}\n   {(\\alpha_i)_{n_i}},\n as a composite marginal likelihood. The estimating equation is still unbiased and estimates consistent.\nWe propose spatially heteroskedastic-consistent standard errors (Conley 1999) that use the Jaccard Index (or similar indices) between two samples:\n\\widehat{\\mathrm{se}}(\\hat{\\boldsymbol{\\beta}}) =\n(\\bm{X}^\\top \\hat{\\bm{W}}\\bm{X})^{-1}\n\\textstyle\\sum_{ik}\n{\\color{#8B0000}{s_{ik}}}\\,\n{\\color{blue}{\\hat{\\bm{u}}_i\\hat{\\bm{u}}_k^\\top}}\\,\n(\\bm{X}^\\top \\hat{\\bm{W}}\\bm{X})^{-1},\n\\qquad\n{\\color{blue}{\\hat{\\bm{u}}_i}} =\n\\frac{\\partial}{\\partial\\boldsymbol{\\beta}}\n\\log \\mathcal{L}(\\hat{\\boldsymbol{\\beta}}),\n where {\\color{#8B0000}{s_{ik}}} \\in [0,1] is the fraction of shared BINs between locations i and k."
  },
  {
    "objectID": "post/SISBayes/GRASPA.html#the-package-hubbellglm",
    "href": "post/SISBayes/GRASPA.html#the-package-hubbellglm",
    "title": "A Bayesian theory for estimation of biodiversity",
    "section": "The package HubbellGLM",
    "text": "The package HubbellGLM"
  },
  {
    "objectID": "post/SISBayes/GRASPA.html#determinants-of-biodiversity-in-the-global-malaise-trap",
    "href": "post/SISBayes/GRASPA.html#determinants-of-biodiversity-in-the-global-malaise-trap",
    "title": "A Bayesian theory for estimation of biodiversity",
    "section": "Determinants of biodiversity in the global malaise trap",
    "text": "Determinants of biodiversity in the global malaise trap\n\nWe run the Hubbel regression model:\n\ncbind(n, y) ~ realm + habitat_type + temperature_2m + total_precipitation + relative_humidity + windspeed + month + year + ns(elev, df = 10) + ns(Latitude, df = 10) + ns(Longitude, df = 10)\n\nWe estimate \\hat{\\sigma} = 0.53 via maximum likelihood, which amounts to a square root growth.\nP-values are corrected for shared species via the sandwich estimator.\n\n\n| Variable                        | Estimate | Std. Error | Pr(&gt;|z|)     | Signif. |\n|---------------------------------|---------:|-----------:|-------------:|---------|\n| (Intercept)                     | -0.019   | 0.489      | 0.969        |         |\n| realm: Australasia              | -0.486   | 0.148      | 0.001        | **      |\n| realm: Indomalayan              | 0.089    | 0.142      | 0.529        |         |\n| realm: Nearctic                 | -0.150   | 0.469      | 0.750        |         |\n| realm: Neotropic                | 0.262    | 0.420      | 0.533        |         |\n| realm: Palearctic               | -0.005   | 0.187      | 0.977        |         |\n| habitat: Grassland              | -0.274   | 0.053      | 0.000        | ***     |\n| habitat: Mixed                  | 0.044    | 0.059      | 0.463        |         |\n| habitat: Tundra                 | -0.334   | 0.177      | 0.059        | .       |\n| habitat: Urban                  | -0.374   | 0.067      | 0.000        | ***     |\n| habitat: Wetland                | -0.087   | 0.069      | 0.210        |         |\n| temperature at 2m               | 0.049    | 0.005      | &lt; 2.2e-16    | ***     |\n| total precipitation             | -0.009   | 0.003      | 0.001        | **      |\n| relative humidity               | 0.008    | 0.002      | 0.000        | ***     |\n| wind speed                      | -0.049   | 0.012      | 0.000        | ***     |"
  },
  {
    "objectID": "post/SISBayes/GRASPA.html#results-latitude-has-a-major-effect",
    "href": "post/SISBayes/GRASPA.html#results-latitude-has-a-major-effect",
    "title": "A Bayesian theory for estimation of biodiversity",
    "section": "Results: latitude has a major effect!",
    "text": "Results: latitude has a major effect!"
  },
  {
    "objectID": "post/SISBayes/GRASPA.html#summary-and-future-directions",
    "href": "post/SISBayes/GRASPA.html#summary-and-future-directions",
    "title": "A Bayesian theory for estimation of biodiversity",
    "section": "Summary and future directions",
    "text": "Summary and future directions\n\nFisher’s \\alpha is the same quantity described by Hubbell’s unified neutral theory of biodiversity, and it has deep connections with the Dirichlet process.\n\n\n\n\n\n\n\nRigon, T., Hsu, C., and Dunson D.B. (2025+). A Bayesian theory for estimation of biodiversity. arXiv:2502.01333\n\n\n\n\nHubbell can be extended to regression model to make it covariate-dependent.\nDifferent growth rates are captured via \\sigma &lt; 0, \\sigma = 0, and \\sigma \\in (0,1).\nFrom here to infinity: next steps\n\nExtend to mixed models, allowing for random effects;\nMore on nonparametrics: GAMs\nFinish the paper! :)\n\nThank you for your attention!"
  },
  {
    "objectID": "post/SISBayes/GRASPA.html#references",
    "href": "post/SISBayes/GRASPA.html#references",
    "title": "A Bayesian theory for estimation of biodiversity",
    "section": "References",
    "text": "References\n\n\n\nCamerlenghi, F., Lijoi, A., Orbanz, P., and Prünster, I. (2019), “Distribution theory for hierarchical processes,” Ann. Statist., 47, 67–92.\n\n\nDe Blasi, P., Favaro, S., Lijoi, A., Mena, R. H., Prunster, I., and Ruggiero, M. (2015), “Are Gibbs-type priors the most natural generalization of the Dirichlet process?” IEEE Trans. Pattern Anal. Mach. Intell., IEEE, 37, 212–229.\n\n\nEwens, W. (1972), “The sampling theory of selectively neutral alleles,” Theoretical Population Biology, 3, 87–112.\n\n\nFerguson, T. S. (1973), “A Bayesian analysis of some nonparametric problems,” Ann. Statist., 1, 209–230.\n\n\nFisher, R. A., Corbet, A. S., and Williams, C. B. (1943), “The relation between the number of species and the number of individuals in a random sample of an animal population,” J. Anim. Ecol., 12, 42–58.\n\n\nFranzolini, B., Lijoi, A., Prünster, I., and Rebaudo, G. (2025), “Multivariate species sampling models,” arXiv:2503.24004.\n\n\nGhilotti, L., Camerlenghi, F., and Rigon, T. (2025), “Bayesian analysis of product feature allocation models,” Journal of the Royal Statistical Society: Series B (Statistical Methodology).\n\n\nGnedin, A. (2010), “Species sampling model with finitely many types,” Electron. Comm. Prob., 15, 79–88.\n\n\nGnedin, A., and Pitman, J. (2005), “Exchangeable Gibbs partitions and Stirling triangles,” Zapiski Nauchnykh Seminarov, POMI, 325, 83–102.\n\n\nHubbell, S. P. (2001), The unified neutral theory of biodiversity and biogeography, Princeton University Press.\n\n\nLijoi, A., Mena, R. H., and Prünster, I. (2007a), “Bayesian nonparametric estimation of the probability of discovering new species,” Biometrika, Oxford University Press, 94, 769–786.\n\n\nLijoi, A., Mena, R. H., and Prünster, I. (2007b), “Controlling the reinforcement in Bayesian non-parametric mixture models,” J. R. Statist. Soc. B, Wiley Online Library, 69, 715–740.\n\n\nLijoi, A., Prünster, I., and Rigon, T. (2020), “The Pitman–Yor multinomial process for mixture modeling,” Biometrika, 107, 891–906.\n\n\nMcCullagh, P. (2016), “Two Early Contributions to the Ewens Saga,” Statist. Sci., 31, 23–26.\n\n\nPitman, J. (1996), “Some developments of the Blackwell-MacQueen urn scheme,” in Statistics, probability and game theory, IMS lecture notes monogr. ser., Inst. Math. Statist., Hayward, CA, pp. 245–267.\n\n\nPitman, J. (2003), “Poisson-Kingman partitions,” Lecture Notes-Monograph Series, 40, 1–34.\n\n\nPitman, J., and Yor, M. (1997), “The two-parameter Poisson-Dirichlet distribution derived from a stable subordinator,” Ann. Probab., 25, 855–900.\n\n\nRigon, T., Hsu, C., and B., D. D. (2025a), “A Bayesian theory for estimation of biodiversity,” arXiv:2502.01333.\n\n\nRigon, T., Petrone, S., and Scarpa, B. (2025b), “Enriched Pitman-Yor processes,” Scand. J. Statist.\n\n\nZito, A., Rigon, T., and Dunson, D. B. (2023), “Inferring taxonomic placement from DNA barcoding allowing discovery of new taxa,” Meth. Ecol. Evol., 14, 529–542."
  },
  {
    "objectID": "post/SISBayes/GRASPA_slides.html#warm-thanks",
    "href": "post/SISBayes/GRASPA_slides.html#warm-thanks",
    "title": "A Bayesian theory for estimation of biodiversity",
    "section": "Warm thanks",
    "text": "Warm thanks\n\n\n\n\n\n\n\nChing-Lung Hsu\n(Duke University)\n\n\n\n\n\n\n\n\nAlessandro Zito\n(Harvard School of Public Health)\n\n\n\n\n\n\n\n\nDavid Dunson\n(Duke University)"
  },
  {
    "objectID": "post/SISBayes/GRASPA_slides.html#a-journey-into-statistical-biodiversity",
    "href": "post/SISBayes/GRASPA_slides.html#a-journey-into-statistical-biodiversity",
    "title": "A Bayesian theory for estimation of biodiversity",
    "section": "A journey into statistical biodiversity",
    "text": "A journey into statistical biodiversity"
  },
  {
    "objectID": "post/SISBayes/GRASPA_slides.html#approximately-80-years-ago-fisher1943",
    "href": "post/SISBayes/GRASPA_slides.html#approximately-80-years-ago-fisher1943",
    "title": "A Bayesian theory for estimation of biodiversity",
    "section": "Approximately 80 years ago… Fisher et al. (1943)",
    "text": "Approximately 80 years ago… Fisher et al. (1943)"
  },
  {
    "objectID": "post/SISBayes/GRASPA_slides.html#the-ewens-sampling-formula-ewens72",
    "href": "post/SISBayes/GRASPA_slides.html#the-ewens-sampling-formula-ewens72",
    "title": "A Bayesian theory for estimation of biodiversity",
    "section": "The Ewens sampling formula Ewens (1972)",
    "text": "The Ewens sampling formula Ewens (1972)"
  },
  {
    "objectID": "post/SISBayes/GRASPA_slides.html#the-dirichlet-process-ferguson1973",
    "href": "post/SISBayes/GRASPA_slides.html#the-dirichlet-process-ferguson1973",
    "title": "A Bayesian theory for estimation of biodiversity",
    "section": "The Dirichlet process (Ferguson 1973)",
    "text": "The Dirichlet process (Ferguson 1973)"
  },
  {
    "objectID": "post/SISBayes/GRASPA_slides.html#and-species-sampling-models-pitman1996",
    "href": "post/SISBayes/GRASPA_slides.html#and-species-sampling-models-pitman1996",
    "title": "A Bayesian theory for estimation of biodiversity",
    "section": "… and species sampling models (Pitman 1996)",
    "text": "… and species sampling models (Pitman 1996)"
  },
  {
    "objectID": "post/SISBayes/GRASPA_slides.html#ewens-formula-governs-the-neutral-theory-hubbell_2001",
    "href": "post/SISBayes/GRASPA_slides.html#ewens-formula-governs-the-neutral-theory-hubbell_2001",
    "title": "A Bayesian theory for estimation of biodiversity",
    "section": "Ewens formula governs the neutral theory! (Hubbell 2001)",
    "text": "Ewens formula governs the neutral theory! (Hubbell 2001)"
  },
  {
    "objectID": "post/SISBayes/GRASPA_slides.html#all-ties-together-mccullagh_2016",
    "href": "post/SISBayes/GRASPA_slides.html#all-ties-together-mccullagh_2016",
    "title": "A Bayesian theory for estimation of biodiversity",
    "section": "All ties together (McCullagh 2016)",
    "text": "All ties together (McCullagh 2016)"
  },
  {
    "objectID": "post/SISBayes/GRASPA_slides.html#species-sampling-models",
    "href": "post/SISBayes/GRASPA_slides.html#species-sampling-models",
    "title": "A Bayesian theory for estimation of biodiversity",
    "section": "Species sampling models",
    "text": "Species sampling models\n\nLet X_1,\\dots,X_n be some collection of species or taxa. Suppose X_n are conditionally iid samples from a species sampling model: \n(X_n \\mid \\tilde{p}) \\overset{\\text{iid}}{\\sim} \\tilde{p}, \\qquad\n\\tilde{p}(\\cdot) = \\sum_{h=1}^{\\infty}\\pi_h \\delta_{Z_h}(\\cdot),\n\\qquad n \\ge 1,\n where (\\pi_h)_{h \\ge 1} is a set of probabilities (species proportions) and Z_h represent distinct species.\nThe discreteness of \\tilde{p} identifies Y^{(n)}=y distinct taxa X_1^*, \\ldots, X_y^* with frequencies n_1, \\ldots, n_y, called abundances in ecology.\nGibbs-type priors have emerged as the most natural extension of the DP (De Blasi et al. 2015).\nThe predictive distribution of a Gibbs-type prior is given by: \n\\mathbb{P}(X_{n+1} \\in \\cdot \\mid X_1,\\dots,X_n)\n= \\frac{V_{n+1, y+1}}{V_{n,y}}P(\\cdot)\n+ \\frac{V_{n+1, y}}{V_{n,y}}\\sum_{j=1}^y(n_j - \\sigma)\\delta_{X^*_j}(\\cdot).\n where (a)_{n} denotes a rising factorial, and \\sigma &lt; 1 is the discount parameter. The V_{n,y}’s are non-negative weights satisfying a forward recursive equation."
  },
  {
    "objectID": "post/SISBayes/GRASPA_slides.html#three-notable-examples",
    "href": "post/SISBayes/GRASPA_slides.html#three-notable-examples",
    "title": "A Bayesian theory for estimation of biodiversity",
    "section": "Three notable examples",
    "text": "Three notable examples\n\nDirichlet multinomial (\\sigma &lt; 0). For \\sigma &lt; 0 and H \\in \\mathbb{N}, a valid set of Gibbs coefficients is given by: \nV_{n, y}(\\sigma, H) := \\frac{|\\sigma|^{y-1}\\prod_{j=1}^{y-1}(H - j)}{(H|\\sigma| +1)_{n-1}}\n\\mathbb{I}(y \\le H).\n\nDirichlet process (\\sigma = 0). For \\sigma = 0 and \\alpha &gt; 0, a valid set of Gibbs coefficients is given by: \nV_{n, y}(\\alpha) := \\frac{\\alpha^y}{(\\alpha)_n}.\n\n\\sigma-stable Poisson–Kingman process (0 &lt; \\sigma &lt; 1). For \\sigma \\in (0,1) and gamma \\gamma &gt; 0, a valid set of Gibbs coefficients is defined as: \nV_{n, y}(\\sigma, \\gamma) := \\frac{\\sigma^y\\gamma^y}{\\Gamma(n-y\\sigma)f_{\\sigma}\\left(\\gamma^{-1/\\sigma}\\right)}\n\\int_{0}^{1}s^{n-1-y\\sigma}f_{\\sigma}\\left(\\left(1-s\\right)\\gamma^{-1/\\sigma}\\right)\\, \\mathrm{d}s,\n where f_\\sigma(t) = (\\pi)^{-1} \\sum_{h=1}^\\infty (-1)^{h+1}\\sin(h \\pi \\sigma )\\Gamma(h\\sigma + 1) / t^{h\\sigma + 1}."
  },
  {
    "objectID": "post/SISBayes/GRASPA_slides.html#a-characterization-theorem",
    "href": "post/SISBayes/GRASPA_slides.html#a-characterization-theorem",
    "title": "A Bayesian theory for estimation of biodiversity",
    "section": "A characterization theorem",
    "text": "A characterization theorem\n\n\n\nTheorem (Gnedin and Pitman 2005)\n\n\nThe Gibbs coefficients V_{n, y} satisfy the recursive equation in the following three cases:\n\nIf \\sigma &lt; 0, whenever V_{n, y} = \\sum_{h=1}^\\infty V_{n, y}(\\sigma, h) p(h), for some discrete random variable H \\in \\mathbb{N} with pdf p(h), where the V_{n, y}(\\sigma, h)’s are those of the Dirichlet multinomial.\nIf \\sigma = 0, whenever V_{n, y} = \\int_{\\mathbb{R}^+} V_{n, y}(\\alpha) p(\\mathrm{d}\\alpha), for some positive random variable \\alpha with probability measure p(\\mathrm{d}\\alpha), where the V_{n, y}(\\alpha)’s are those of the Dirichlet process.\nIf \\sigma \\in (0,1), whenever V_{n, y} = \\int_{\\mathbb{R}^+} V_{n, y}(\\sigma, \\gamma) p(\\mathrm{d}\\gamma), for some positive random variable \\gamma with probability measure p(\\mathrm{d}\\gamma), where the V_{n, y}(\\sigma, \\gamma)’s are those of the \\sigma-stable PK.\n\n\n\n\n\nThe Dirichlet multinomial, Dirichlet process, and \\sigma-stable PK form the foundation of any Gibbs-type prior.\nIn fact, any Gibbs-type process can be represented hierarchically, involving a suitable prior distribution for the key parameters H, \\alpha, and \\gamma."
  },
  {
    "objectID": "post/SISBayes/GRASPA_slides.html#the-quantification-of-biodiversity-lijoi2007",
    "href": "post/SISBayes/GRASPA_slides.html#the-quantification-of-biodiversity-lijoi2007",
    "title": "A Bayesian theory for estimation of biodiversity",
    "section": "The quantification of biodiversity (Lijoi et al. 2007a)",
    "text": "The quantification of biodiversity (Lijoi et al. 2007a)\n\nThe simplest measure of biodiversity is arguably the taxon richness Y^{(n)} = y.\nA priori, the distribution of Y^{(n)} induced by a Gibbs-type prior has a simple form: \n\\mathbb{P}(Y^{(n)} = y)=V_{n,y}\\frac{\\mathscr{C}(n,y;\\sigma)}{\\sigma^y},\n where \\mathscr{C}(n, y;\\sigma) denotes a generalized factorial coefficient.\nThe a priori expectations \\mathbb{E}(K_1),\\dots,\\mathbb{E}(K_n) define a model-based rarefaction curve.\nThe posterior distribution of the number of previously unobserved taxa Y_m^{(n)} is \n\\mathbb{P}(Y_m^{(n)}= j \\mid X_1,\\dots,X_n)=\\frac{V_{n + m, y + j}}{V_{n, y}}\\frac{\\mathscr{C}(m, j;\\sigma, -n + y\\sigma)}{\\sigma^j}, \\qquad j=0,\\dots,m,\n where \\mathscr{C}(m, j;\\sigma, -n + y\\sigma) is the noncentral generalized factorial coefficient.\nThe posterior expectations \\mathbb{E}(Y^{(n+1)} \\mid Y^{(n)} = y), \\dots, \\mathbb{E}(Y^{(n + m)} \\mid Y^{(n)} = y) represents a model-based extrapolation of the accumulation curve."
  },
  {
    "objectID": "post/SISBayes/GRASPA_slides.html#rarefaction-and-extrapolation",
    "href": "post/SISBayes/GRASPA_slides.html#rarefaction-and-extrapolation",
    "title": "A Bayesian theory for estimation of biodiversity",
    "section": "Rarefaction and extrapolation",
    "text": "Rarefaction and extrapolation"
  },
  {
    "objectID": "post/SISBayes/GRASPA_slides.html#the-sigma-diversity-pitman2003",
    "href": "post/SISBayes/GRASPA_slides.html#the-sigma-diversity-pitman2003",
    "title": "A Bayesian theory for estimation of biodiversity",
    "section": "The \\sigma-diversity (Pitman 2003)",
    "text": "The \\sigma-diversity (Pitman 2003)\n\nLet Y^{(n)} be the number of distinct values arising from a Gibbs-type prior:\n\nLet \\sigma &lt; 0 and V_{n, y}(\\sigma, H) be the weights of a Dirichlet multinomial, then Y^{(n)} \\rightarrow H a.s.\nLet \\sigma = 0 and V_{n, y}(\\alpha) be the weights of a Dirichlet process, then Y^{(n)} / \\log(n) \\rightarrow \\alpha a.s.\nLet \\sigma \\in (0,1) and V_{n, y}(\\sigma, \\gamma) be the weights of a \\sigma-stable PK, then Y^{(n)} / n^\\sigma \\rightarrow \\gamma a.s.\n\nMoreover, consider a generic set of weights V_{n, y} and let \nc_\\sigma(n) =\n\\begin{cases}\n1, & \\sigma &lt;0,\\\\[2mm]\n\\log(n), & \\sigma = 0,\\\\[1mm]\nn^{\\sigma}, & \\sigma \\in (0,1)\n\\end{cases}\n Then, as n \\rightarrow \\infty: \n\\frac{Y^{(n)}}{c_\\sigma(n)} \\overset{\\textup{a.s.}}{\\longrightarrow} S_{\\sigma}.\n The r.v. S_{\\sigma} is the \\sigma-diversity and its distribution coincides with the prior for H, \\alpha, and \\gamma."
  },
  {
    "objectID": "post/SISBayes/GRASPA_slides.html#historical-summary-and-extensions",
    "href": "post/SISBayes/GRASPA_slides.html#historical-summary-and-extensions",
    "title": "A Bayesian theory for estimation of biodiversity",
    "section": "Historical summary and extensions",
    "text": "Historical summary and extensions\n\nThe fundamental biodiversity number \\alpha is a growth-adjusted richness: \n\\frac{Y^{(n)}}{\\log n} \\stackrel{\\mathrm{a.s.}}{\\longrightarrow} \\alpha, \\qquad n\\to \\infty\n\nThe \\alpha from the neutral theory, DP and Fisher log-series is the same for large n: \n\\underbrace{y = \\hat{\\alpha}^\\mathrm{Fisher}\\log\\Big(1 + \\frac{n}{\\hat{\\alpha}^\\mathrm{Fisher}}\\Big)}_{\\text{Solution from Fisher}} \\ \\stackrel{\\text{Under large } n}{\\approx} \\ \\underbrace{\\sum_{j = 1}^{n} \\frac{\\hat{\\alpha}^{\\text{DP}}}{\\hat{\\alpha}^{\\text{DP}} + j - 1} = y}_{\\text{MLE from Hubbell and DP}}\n\nThere are a lot of possible extensions. This is a very biased list:\n\nThe choice \\sigma \\in (0, 1) allows for polynomial growth (Lijoi et al. 2007b; Pitman and Yor 1997) while \\sigma &lt; 0 leads to a finite richness (Gnedin 2010; Lijoi et al. 2020).\nMultiple samples, a.k.a. partial exchangeability (Camerlenghi et al. 2019; Franzolini et al. 2025)\nEnriched processes, taxonomical data (Rigon et al. 2025a; Rigon et al. 2025b; Zito et al. 2023).\nIncidence data and feature models (Ghilotti et al. 2025).\nWhat about covariates?"
  },
  {
    "objectID": "post/SISBayes/GRASPA_slides.html#specimens-and-bins-in-barcode-of-life",
    "href": "post/SISBayes/GRASPA_slides.html#specimens-and-bins-in-barcode-of-life",
    "title": "A Bayesian theory for estimation of biodiversity",
    "section": "Specimens and BINs in Barcode of Life",
    "text": "Specimens and BINs in Barcode of Life"
  },
  {
    "objectID": "post/SISBayes/GRASPA_slides.html#how-can-we-link-covariates-with-biodiversity",
    "href": "post/SISBayes/GRASPA_slides.html#how-can-we-link-covariates-with-biodiversity",
    "title": "A Bayesian theory for estimation of biodiversity",
    "section": "How can we link covariates with biodiversity?",
    "text": "How can we link covariates with biodiversity?\n\nFor outcome Y_i and covariates \\bm{x}_i = (x_{i1}, \\ldots, x_{ip}) \\in \\mathbb{R}^p, i = 1, \\ldots, N, a generalized linear model (GLM) extends the linear model in non-Gaussian settings via three ingredients:\nThe exponential dispersion family directing each Y_i: \nf(y_i; \\theta_i, \\phi) = \\exp\\left\\{\\frac{y_i \\theta_i - b(\\theta_i)}{a_i(\\phi)} + c(y_i, \\phi)\\right\\},\n where \\theta_i \\in \\Theta is the natural parameter, \\phi &gt; 0 is the dispersion, and a_i,b,c are given functions.\nThe linear predictor \\eta_i = \\bm{x}_i^\\top \\bm{\\beta}, with \\bm{\\beta} \\in \\mathbb{R}^p.\nA monotonic link function g that links \\eta_i with the mean of Y_i, namely \\mu(\\theta_i) = \\mu_i: \n\\mathbb{E}(Y_i \\mid \\bm{x}_i) = \\mu_i = g^{-1}(\\eta_i).\n The link is canonical when g is such that \\theta_i = \\eta_i.\nIs there an exponential family that is related to a sample-specific diversity \\alpha_i?"
  },
  {
    "objectID": "post/SISBayes/GRASPA_slides.html#the-hubbell-regression",
    "href": "post/SISBayes/GRASPA_slides.html#the-hubbell-regression",
    "title": "A Bayesian theory for estimation of biodiversity",
    "section": "The Hubbell regression",
    "text": "The Hubbell regression\n\nThe number of distinct species from a DP is an exponential family! \n\\mathbb{P}(Y^{(n)} = y; \\alpha) = \\frac{\\alpha^y}{(\\alpha)_n}|s(n, y)|, \\qquad y \\in\\{ 1, \\ldots, n\\},\n where (\\alpha)_n = \\Gamma(\\alpha + n)/\\Gamma(\\alpha) and |s(n,y)| are signless Stirling numbers of the first kind.\n\n\n\n\nThe Hubbell regression is a GLM where:\n\nThe distinct species Y_i^{(n_i)} for i=1,\\dots,N are independent draws from the distribution \nf(y_i; \\theta_i) = \\exp \\left\\{y_i \\theta_i - \\log[\\Gamma(e^{\\theta_i} + n_i) - \\Gamma(e^{\\theta_i})] + \\log|s(n_i, y_i)|\\right\\},\n with \\alpha_i = e^{\\theta_i} whose mean and variance functions are \n\\mu_i = \\mu(\\theta_i, n_i) = \\sum_{j=0}^{n_i-1} \\frac{e^{\\theta_i}}{e^{\\theta_i} + j}, \\qquad\nV(\\mu(\\theta_i, n_i)) = \\sum_{j=0}^{n_i-1} \\frac{e^{\\theta_i}}{e^{\\theta_i} + j}\\left(1 - \\frac{e^{\\theta_i}}{e^{\\theta_i} + j}\\right).\n\nThe link function g depends on \\eta_i and n_i and satisfies \\mathbb{E}(Y_i\\mid \\bm{x}_i, n_i) = \\mu_i = g^{-1}(\\bm{x}_i^\\top \\bm{\\beta}, n_i)."
  },
  {
    "objectID": "post/SISBayes/GRASPA_slides.html#the-canonical-link",
    "href": "post/SISBayes/GRASPA_slides.html#the-canonical-link",
    "title": "A Bayesian theory for estimation of biodiversity",
    "section": "The canonical link",
    "text": "The canonical link\n\nUsing the canonical link, we have: \n\\log \\alpha_i = \\theta_i = \\bm{x}_i^\\top \\bm{\\beta} = \\eta_i, \\qquad\n\\mu_i = \\mu(\\bm{x}_i^\\top \\bm{\\beta}, n_i) = \\sum_{j = 0}^{n_i-1} \\frac{e^{\\bm{x}_i^\\top \\bm{\\beta}}}{e^{\\bm{x}_i^\\top \\bm{\\beta}} + j} = g^{-1}(\\bm{x}_i^\\top \\bm{\\beta}, n_i).\n\nCoefficients \\bm{\\beta} have the classic interpretation of log-linear models in terms of \\alpha-diversity\n\n\nIf x_{ip} ↑ by 1 unit → \\alpha_i ↑ by 100(e^{\\beta_p} - 1)\\%\n\n\nThe saturated model corresponds to the case in which \\hat{\\alpha}_i \\approx \\hat{\\alpha}^{\\text{Fisher}}_i. The null model corresponds to the case \\hat{\\alpha}_i = \\hat{\\alpha} for all sites, i.e. there is no variation in biodiversity across sampling sites.\n\n\n\n\nTheorem (Poisson regression with log-log offset)\n\n\nCall \\gamma = 0.5772\\dots the Euler’s constant. Under large values of n, when each n_i \\to \\infty we have \nY^{(n)}_i \\;\\dot{\\sim}\\; 1 + \\mathrm{Poisson}\\left(\\exp\\left\\{\\bm{x}_i^\\top\\bm{\\beta} + \\log(\\gamma + \\log n_i)\\right\\}\\right).\n Informally, we will say that \\hat{\\bm{\\beta}}^{\\mathrm{Hubbell}} \\approx \\hat{\\bm{\\beta}}^{\\mathrm{Poisson}} ."
  },
  {
    "objectID": "post/SISBayes/GRASPA_slides.html#determinants-of-biodiversity-in-the-global-malaise-trap",
    "href": "post/SISBayes/GRASPA_slides.html#determinants-of-biodiversity-in-the-global-malaise-trap",
    "title": "A Bayesian theory for estimation of biodiversity",
    "section": "Determinants of biodiversity in the global malaise trap",
    "text": "Determinants of biodiversity in the global malaise trap\n\nWe run the Hubbel regression model:\n\ncbind(n, y) ~ realm + habitat_type + temperature_2m + total_precipitation + relative_humidity + windspeed + month + year + ns(elev, df = 10) + ns(Latitude, df = 10) + ns(Longitude, df = 10)\n\nWe estimate \\hat{\\sigma} = 0.53 via maximum likelihood, which amounts to a square root growth.\nP-values are corrected for shared species via the sandwich estimator.\n\n\n| Variable                        | Estimate | Std. Error | Pr(&gt;|z|)     | Signif. |\n|---------------------------------|---------:|-----------:|-------------:|---------|\n| (Intercept)                     | -0.019   | 0.489      | 0.969        |         |\n| realm: Australasia              | -0.486   | 0.148      | 0.001        | **      |\n| realm: Indomalayan              | 0.089    | 0.142      | 0.529        |         |\n| realm: Nearctic                 | -0.150   | 0.469      | 0.750        |         |\n| realm: Neotropic                | 0.262    | 0.420      | 0.533        |         |\n| realm: Palearctic               | -0.005   | 0.187      | 0.977        |         |\n| habitat: Grassland              | -0.274   | 0.053      | 0.000        | ***     |\n| habitat: Mixed                  | 0.044    | 0.059      | 0.463        |         |\n| habitat: Tundra                 | -0.334   | 0.177      | 0.059        | .       |\n| habitat: Urban                  | -0.374   | 0.067      | 0.000        | ***     |\n| habitat: Wetland                | -0.087   | 0.069      | 0.210        |         |\n| temperature at 2m               | 0.049    | 0.005      | &lt; 2.2e-16    | ***     |\n| total precipitation             | -0.009   | 0.003      | 0.001        | **      |\n| relative humidity               | 0.008    | 0.002      | 0.000        | ***     |\n| wind speed                      | -0.049   | 0.012      | 0.000        | ***     |"
  },
  {
    "objectID": "post/SISBayes/GRASPA_slides.html#results-latitude-has-a-major-effect",
    "href": "post/SISBayes/GRASPA_slides.html#results-latitude-has-a-major-effect",
    "title": "A Bayesian theory for estimation of biodiversity",
    "section": "Results: latitude has a major effect!",
    "text": "Results: latitude has a major effect!"
  },
  {
    "objectID": "post/SISBayes/GRASPA_slides.html#summary-and-future-directions",
    "href": "post/SISBayes/GRASPA_slides.html#summary-and-future-directions",
    "title": "A Bayesian theory for estimation of biodiversity",
    "section": "Summary and future directions",
    "text": "Summary and future directions\n\nFisher’s \\alpha is the same quantity described by Hubbell’s unified neutral theory of biodiversity, and it has deep connections with the Dirichlet process.\n\n\n\n\nRigon, T., Hsu, C., and Dunson D.B. (2025+). A Bayesian theory for estimation of biodiversity. arXiv:2502.01333\n\n\n\n\nHubbell can be extended to regression model to make it covariate-dependent.\nDifferent growth rates are captured via \\sigma &lt; 0, \\sigma = 0, and \\sigma \\in (0,1).\nFrom here to infinity: next steps\n\nExtend to mixed models, allowing for random effects;\nMore on nonparametrics: GAMs\nFinish the paper! :)\n\nThank you for your attention!"
  },
  {
    "objectID": "post/SISBayes/GRASPA_slides.html#references",
    "href": "post/SISBayes/GRASPA_slides.html#references",
    "title": "A Bayesian theory for estimation of biodiversity",
    "section": "References",
    "text": "References\n\n\n\nCamerlenghi, F., Lijoi, A., Orbanz, P., and Prünster, I. (2019), “Distribution theory for hierarchical processes,” Ann. Statist., 47, 67–92.\n\n\nDe Blasi, P., Favaro, S., Lijoi, A., Mena, R. H., Prunster, I., and Ruggiero, M. (2015), “Are Gibbs-type priors the most natural generalization of the Dirichlet process?” IEEE Trans. Pattern Anal. Mach. Intell., IEEE, 37, 212–229.\n\n\nEwens, W. (1972), “The sampling theory of selectively neutral alleles,” Theoretical Population Biology, 3, 87–112.\n\n\nFerguson, T. S. (1973), “A Bayesian analysis of some nonparametric problems,” Ann. Statist., 1, 209–230.\n\n\nFisher, R. A., Corbet, A. S., and Williams, C. B. (1943), “The relation between the number of species and the number of individuals in a random sample of an animal population,” J. Anim. Ecol., 12, 42–58.\n\n\nFranzolini, B., Lijoi, A., Prünster, I., and Rebaudo, G. (2025), “Multivariate species sampling models,” arXiv:2503.24004.\n\n\nGhilotti, L., Camerlenghi, F., and Rigon, T. (2025), “Bayesian analysis of product feature allocation models,” Journal of the Royal Statistical Society: Series B (Statistical Methodology).\n\n\nGnedin, A. (2010), “Species sampling model with finitely many types,” Electron. Comm. Prob., 15, 79–88.\n\n\nGnedin, A., and Pitman, J. (2005), “Exchangeable Gibbs partitions and Stirling triangles,” Zapiski Nauchnykh Seminarov, POMI, 325, 83–102.\n\n\nHubbell, S. P. (2001), The unified neutral theory of biodiversity and biogeography, Princeton University Press.\n\n\nLijoi, A., Mena, R. H., and Prünster, I. (2007a), “Bayesian nonparametric estimation of the probability of discovering new species,” Biometrika, Oxford University Press, 94, 769–786.\n\n\nLijoi, A., Mena, R. H., and Prünster, I. (2007b), “Controlling the reinforcement in Bayesian non-parametric mixture models,” J. R. Statist. Soc. B, Wiley Online Library, 69, 715–740.\n\n\nLijoi, A., Prünster, I., and Rigon, T. (2020), “The Pitman–Yor multinomial process for mixture modeling,” Biometrika, 107, 891–906.\n\n\nMcCullagh, P. (2016), “Two Early Contributions to the Ewens Saga,” Statist. Sci., 31, 23–26.\n\n\nPitman, J. (1996), “Some developments of the Blackwell-MacQueen urn scheme,” in Statistics, probability and game theory, IMS lecture notes monogr. ser., Inst. Math. Statist., Hayward, CA, pp. 245–267.\n\n\nPitman, J. (2003), “Poisson-Kingman partitions,” Lecture Notes-Monograph Series, 40, 1–34.\n\n\nPitman, J., and Yor, M. (1997), “The two-parameter Poisson-Dirichlet distribution derived from a stable subordinator,” Ann. Probab., 25, 855–900.\n\n\nRigon, T., Hsu, C., and B., D. D. (2025a), “A Bayesian theory for estimation of biodiversity,” arXiv:2502.01333.\n\n\nRigon, T., Petrone, S., and Scarpa, B. (2025b), “Enriched Pitman-Yor processes,” Scand. J. Statist.\n\n\nZito, A., Rigon, T., and Dunson, D. B. (2023), “Inferring taxonomic placement from DNA barcoding allowing discovery of new taxa,” Meth. Ecol. Evol., 14, 529–542."
  },
  {
    "objectID": "post/Poli2025/Poli2025.html",
    "href": "post/Poli2025/Poli2025.html",
    "title": "Project presentation",
    "section": "",
    "text": "My name is Tommaso Rigon and I am a Senior Assistant Professor (RTD-B).\nUniversity of Milano-Bicocca, Department of Economics, Management and Statistics (DEMS), Milan, Italy.\n\nSenior Assistant Professor (RTD-B), 2023 - Present\nJunior Assistant Professor (RTD-A), 2020 - 2023\n\nDuke University, Department of Statistical Science, Durham (NC), U.S.A.\n\nPostdoctoral Associate, 2020 - 2020\nResearch Associate, 2019 - 2020\n\n\n\n\n\n\n\n\nEducation\n\n\n\n\nPh.D. in Statistical Sciences, Bocconi University, Milan, Italy.\nM.Sc. in Statistical Sciences, University of Padova, Padua, Italy.\nB.Sc. in Statistics, Economics & Finance, University of Padova, Padua, Italy."
  },
  {
    "objectID": "post/Poli2025/Poli2025.html#panelists",
    "href": "post/Poli2025/Poli2025.html#panelists",
    "title": "A generalized Bayes project",
    "section": "",
    "text": "Alessandra Guglielmi\n(PoliMi)\n\n\n\n\n\n\n\n\nAlessandro Fassò\n(UniBg)\n\n\n\n\n\n\n\n\nMaria Grazia Valsecchi\n(Unimib)"
  },
  {
    "objectID": "post/Poli2025/Poli2025.html#phd-life-over-time",
    "href": "post/Poli2025/Poli2025.html#phd-life-over-time",
    "title": "A generalized Bayes project",
    "section": "PhD life over time",
    "text": "PhD life over time\n\nHow did you end up here? Was it a grand plan, a happy accident, or the result of saying “yes” one too many times? We would love to hear any anecdotes about what first sparked your interest in your current research area.\n\n\nHow has academia changed at the PhD and postdoc levels compared to when you went through it? Are things better, worse, or just… different with more Zoom?"
  },
  {
    "objectID": "post/Poli2025/Poli2025.html#seeking-wisdom",
    "href": "post/Poli2025/Poli2025.html#seeking-wisdom",
    "title": "A generalized Bayes project",
    "section": "Seeking wisdom",
    "text": "Seeking wisdom\n\nWhat is your advice for a young investigator? Are there common traps you have seen in your work with students?\n\n\nWhat do you think about the often-repeated quip: “publish or perish”? Is it a painful truth or an exaggeration?\n\n\nWe all have times when we feel daunted by the mathematical challenges associated with our work. How do you respond to feelings of inadequacy or intimidation in the face of difficult problems?\n\n\nWhen is it okay to abandon a project? Not every idea is a brilliant one, and some projects slowly (or quickly) reveal themselves to be dead ends. How do you decide when to pull the plug, both as a junior researcher and, hopefully one day, as a senior one?"
  },
  {
    "objectID": "post/Poli2025/Poli2025.html#the-mentor-the-mentee-and-their-delicate-dance",
    "href": "post/Poli2025/Poli2025.html#the-mentor-the-mentee-and-their-delicate-dance",
    "title": "A generalized Bayes project",
    "section": "The mentor, the mentee, and their delicate dance",
    "text": "The mentor, the mentee, and their delicate dance\n\nWhat are the qualities of a good mentor? In your view, what makes someone a great supervisor or mentor?\nWhat advice do you have for balancing criticism and support as a mentor? How do you cultivate a supporting but rigorous academic environment?\n\n\nWhat are some methods for establishing independence from your advisor(s) as your progress in your PhD?\n\n\nWhat role has community played in your career and in your professional successes? Any anecdotes about relationships formed early in your career that you maintain today?"
  },
  {
    "objectID": "post/Poli2025/Poli2025.html#next-generation-challenges",
    "href": "post/Poli2025/Poli2025.html#next-generation-challenges",
    "title": "A generalized Bayes project",
    "section": "Next generation challenges",
    "text": "Next generation challenges\n\nWhat should statisticians do to avoid being replaced by “data scientists”?\n\n\nIn light of recent developments in AI and large language models (ChatGPT and friends), how should the teaching of Statistics, especially at the bachelor level, evolve? Should we be rethinking what we teach, how we teach it, or both? What about theses and written projects?"
  },
  {
    "objectID": "post/Poli2025/Statalk2025_slides.html#panelists",
    "href": "post/Poli2025/Statalk2025_slides.html#panelists",
    "title": "A generalized Bayes project",
    "section": "Panelists",
    "text": "Panelists\n\n\n\n\n\n\n\nAlessandra Guglielmi\n(PoliMi)\n\n\n\n\n\n\n\n\nAlessandro Fassò\n(UniBg)\n\n\n\n\n\n\n\n\nMaria Grazia Valsecchi\n(Unimib)"
  },
  {
    "objectID": "post/Poli2025/Statalk2025_slides.html#phd-life-over-time",
    "href": "post/Poli2025/Statalk2025_slides.html#phd-life-over-time",
    "title": "A generalized Bayes project",
    "section": "PhD life over time",
    "text": "PhD life over time\n\nHow did you end up here? Was it a grand plan, a happy accident, or the result of saying “yes” one too many times? We would love to hear any anecdotes about what first sparked your interest in your current research area.\n\n\n\nHow has academia changed at the PhD and postdoc levels compared to when you went through it? Are things better, worse, or just… different with more Zoom?"
  },
  {
    "objectID": "post/Poli2025/Statalk2025_slides.html#seeking-wisdom",
    "href": "post/Poli2025/Statalk2025_slides.html#seeking-wisdom",
    "title": "A generalized Bayes project",
    "section": "Seeking wisdom",
    "text": "Seeking wisdom\n\nWhat is your advice for a young investigator? Are there common traps you have seen in your work with students?\n\n\n\nWhat do you think about the often-repeated quip: “publish or perish”? Is it a painful truth or an exaggeration?\n\n\n\n\nWe all have times when we feel daunted by the mathematical challenges associated with our work. How do you respond to feelings of inadequacy or intimidation in the face of difficult problems?\n\n\n\n\nWhen is it okay to abandon a project? Not every idea is a brilliant one, and some projects slowly (or quickly) reveal themselves to be dead ends. How do you decide when to pull the plug, both as a junior researcher and, hopefully one day, as a senior one?"
  },
  {
    "objectID": "post/Poli2025/Statalk2025_slides.html#the-mentor-the-mentee-and-their-delicate-dance",
    "href": "post/Poli2025/Statalk2025_slides.html#the-mentor-the-mentee-and-their-delicate-dance",
    "title": "A generalized Bayes project",
    "section": "The mentor, the mentee, and their delicate dance",
    "text": "The mentor, the mentee, and their delicate dance\n\nWhat are the qualities of a good mentor? In your view, what makes someone a great supervisor or mentor?\nWhat advice do you have for balancing criticism and support as a mentor? How do you cultivate a supporting but rigorous academic environment?\n\n\n\nWhat are some methods for establishing independence from your advisor(s) as your progress in your PhD?\n\n\n\n\nWhat role has community played in your career and in your professional successes? Any anecdotes about relationships formed early in your career that you maintain today?"
  },
  {
    "objectID": "post/Poli2025/Statalk2025_slides.html#next-generation-challenges",
    "href": "post/Poli2025/Statalk2025_slides.html#next-generation-challenges",
    "title": "A generalized Bayes project",
    "section": "Next generation challenges",
    "text": "Next generation challenges\n\nWhat should statisticians do to avoid being replaced by “data scientists”?\n\n\n\nIn light of recent developments in AI and large language models (ChatGPT and friends), how should the teaching of Statistics, especially at the bachelor level, evolve? Should we be rethinking what we teach, how we teach it, or both? What about theses and written projects?"
  },
  {
    "objectID": "post/Poli2025/Poli2025.html#about-myself",
    "href": "post/Poli2025/Poli2025.html#about-myself",
    "title": "A generalized Bayes project",
    "section": "",
    "text": "My name is Tommaso Rigon and I am a Senior Assistant Professor (RTD-B) at the University of Milano-Bicocca, at the Department of Economics, Management and Statistics.\nUniversity of Milano-Bicocca, Department of Economics, Management and Statistics (DEMS), Milan, Italy.\n\nSenior Assistant Professor (RTD-B), 2023 - Present\nJunior Assistant Professor (RTD-A), 2020 - 2023\n\nDuke University, Department of Statistical Science, Durham (NC), U.S.A.\n\nPostdoctoral Associate, 2020 - 2020\nResearch Associate, 2019 - 2020\n\n\n\n\n\n\n\n\n\nPh.D. in Statistical Sciences, Bocconi University, Milan, Italy.\nM.Sc. in Statistical Sciences, University of Padova, Padua, Italy.\nB.Sc. in Statistics, Economics & Finance, University of Padova, Padua, Italy."
  },
  {
    "objectID": "post/Poli2025/Poli2025.html#about-the-project",
    "href": "post/Poli2025/Poli2025.html#about-the-project",
    "title": "Tutor: Tommaso Rigon",
    "section": "About the project",
    "text": "About the project"
  },
  {
    "objectID": "post/Poli2025/Poli2025.html#requirements-and-expected-outcome",
    "href": "post/Poli2025/Poli2025.html#requirements-and-expected-outcome",
    "title": "Tutor: Tommaso Rigon",
    "section": "Requirements and expected outcome",
    "text": "Requirements and expected outcome"
  },
  {
    "objectID": "post/Poli2025/Statalk2025_slides.html#about-myself",
    "href": "post/Poli2025/Statalk2025_slides.html#about-myself",
    "title": "A generalized Bayes project",
    "section": "About myself",
    "text": "About myself\n\nMy name is Tommaso Rigon and I am a Senior Assistant Professor (RTD-B) at the University of Milano-Bicocca, at the Department of Economics, Management and Statistics.\nUniversity of Milano-Bicocca, Department of Economics, Management and Statistics (DEMS), Milan, Italy.\n\nSenior Assistant Professor (RTD-B), 2023 - Present\nJunior Assistant Professor (RTD-A), 2020 - 2023\n\nDuke University, Department of Statistical Science, Durham (NC), U.S.A.\n\nPostdoctoral Associate, 2020 - 2020\nResearch Associate, 2019 - 2020\n\n\n\n\n\n\nPh.D. in Statistical Sciences, Bocconi University, Milan, Italy.\nM.Sc. in Statistical Sciences, University of Padova, Padua, Italy.\nB.Sc. in Statistics, Economics & Finance, University of Padova, Padua, Italy."
  },
  {
    "objectID": "post/Poli2025/Statalk2025_slides.html#about-the-project",
    "href": "post/Poli2025/Statalk2025_slides.html#about-the-project",
    "title": "Tutor: Tommaso Rigon",
    "section": "About the project",
    "text": "About the project"
  },
  {
    "objectID": "post/Poli2025/Statalk2025_slides.html#requirements-and-expected-outcome",
    "href": "post/Poli2025/Statalk2025_slides.html#requirements-and-expected-outcome",
    "title": "Tutor: Tommaso Rigon",
    "section": "Requirements and expected outcome",
    "text": "Requirements and expected outcome"
  },
  {
    "objectID": "post/Poli2025/Statalk2025_slides.html#short-bio",
    "href": "post/Poli2025/Statalk2025_slides.html#short-bio",
    "title": "Tutor: Tommaso Rigon",
    "section": "Short Bio",
    "text": "Short Bio\n\nMy name is Tommaso Rigon and I am a Senior Assistant Professor (RTD-B).\nUniversity of Milano-Bicocca, Department of Economics, Management and Statistics (DEMS), Milan, Italy.\n\nSenior Assistant Professor (RTD-B), 2023 - Present\nJunior Assistant Professor (RTD-A), 2020 - 2023\n\nDuke University, Department of Statistical Science, Durham (NC), U.S.A.\n\nPostdoctoral Associate, 2020 - 2020\nResearch Associate, 2019 - 2020\n\n\n\n\n\nEducation\n\n\n\nPh.D. in Statistical Sciences, Bocconi University, Milan, Italy.\nM.Sc. in Statistical Sciences, University of Padova, Padua, Italy.\nB.Sc. in Statistics, Economics & Finance, University of Padova, Padua, Italy."
  },
  {
    "objectID": "post/Poli2025/Polimi2025_slides.html#short-bio",
    "href": "post/Poli2025/Polimi2025_slides.html#short-bio",
    "title": "Tutor: Tommaso Rigon",
    "section": "Short Bio",
    "text": "Short Bio\n\nMy name is Tommaso Rigon and I am a Senior Assistant Professor (RTD-B).\nUniversity of Milano-Bicocca, Department of Economics, Management and Statistics (DEMS), Milan, Italy.\n\nSenior Assistant Professor (RTD-B), 2023 - Present\nJunior Assistant Professor (RTD-A), 2020 - 2023\n\nDuke University, Department of Statistical Science, Durham (NC), U.S.A.\n\nPostdoctoral Associate, 2020 - 2020\nResearch Associate, 2019 - 2020\n\n\n\n\n\nEducation\n\n\n\nPh.D. in Statistical Sciences, Bocconi University, Milan, Italy.\nM.Sc. in Statistical Sciences, University of Padova, Padua, Italy.\nB.Sc. in Statistics, Economics & Finance, University of Padova, Padua, Italy."
  },
  {
    "objectID": "post/Poli2025/Polimi2025_slides.html#about-the-project",
    "href": "post/Poli2025/Polimi2025_slides.html#about-the-project",
    "title": "Tutor: Tommaso Rigon",
    "section": "About the project",
    "text": "About the project"
  },
  {
    "objectID": "post/Poli2025/Polimi2025_slides.html#requirements-and-expected-outcome",
    "href": "post/Poli2025/Polimi2025_slides.html#requirements-and-expected-outcome",
    "title": "Tutor: Tommaso Rigon",
    "section": "Requirements and expected outcome",
    "text": "Requirements and expected outcome\n\n\n\n\nAgnoletto, D., Rigon, T., and Dunson, D. B. (2025), “Bayesian inference for generalized linear models via quasi-posteriors,” Biometrika, 112.\n\n\nMorris, C. N. (1982), “Natural exponential families with quadratic variance functions,” Annals of Statistics, 10, 65–80.\n\n\nPolson, N. G., Scott, J. G., and Windle, J. (2013), “Bayesian inference for logistic models using polya-gamma latent variables,” Journal of the American Statistical Association, Taylor {&} Francis, 108, 1–42."
  },
  {
    "objectID": "post/Poli2025/Poli2025.html#short-bio",
    "href": "post/Poli2025/Poli2025.html#short-bio",
    "title": "Project presentation",
    "section": "",
    "text": "My name is Tommaso Rigon and I am a Senior Assistant Professor (RTD-B).\nUniversity of Milano-Bicocca, Department of Economics, Management and Statistics (DEMS), Milan, Italy.\n\nSenior Assistant Professor (RTD-B), 2023 - Present\nJunior Assistant Professor (RTD-A), 2020 - 2023\n\nDuke University, Department of Statistical Science, Durham (NC), U.S.A.\n\nPostdoctoral Associate, 2020 - 2020\nResearch Associate, 2019 - 2020\n\n\n\n\n\n\n\n\nEducation\n\n\n\n\nPh.D. in Statistical Sciences, Bocconi University, Milan, Italy.\nM.Sc. in Statistical Sciences, University of Padova, Padua, Italy.\nB.Sc. in Statistics, Economics & Finance, University of Padova, Padua, Italy."
  },
  {
    "objectID": "post/Poli2025/Polimi2025_slides.html#hyperbolic-secant-regression",
    "href": "post/Poli2025/Polimi2025_slides.html#hyperbolic-secant-regression",
    "title": "Tutor: Tommaso Rigon",
    "section": "Hyperbolic secant regression",
    "text": "Hyperbolic secant regression\n\n\n\nBackground\n\n\nLet Y_1,\\dots,Y_n be independent random variable distributed according to a hyperbolic secant distribution, whose density is \nf(y_i; \\theta_i) = \\frac{\\exp\\{\\theta_i y_i - \\log{\\cos(\\theta_i)} \\}}{2 \\cosh(\\pi y_i / 2)}, \\qquad y_i \\in \\mathbb{R}, \\; \\theta_i \\in \\left(-\\frac{\\pi}{2}, \\frac{\\pi}{2}\\right).\n This distribution is discussed in Morris (1982) and is an instance of highly tractable exponential family having a quadratic variance function.\n\n\n\n\nThis distribution can be employed to build a novel generalized linear model (GLM), which automatically incorporates heteroskedasticity and exhibits heavier tails than the Gaussian law.\nThere are multiple application areas for such a regression technique, including (but not limited to) financial data."
  },
  {
    "objectID": "post/Poli2025/Polimi2025_slides.html#hyperbolic-secant-regression-i",
    "href": "post/Poli2025/Polimi2025_slides.html#hyperbolic-secant-regression-i",
    "title": "Tutor: Tommaso Rigon",
    "section": "Hyperbolic secant regression I",
    "text": "Hyperbolic secant regression I\n\n\n\nBackground\n\n\n\nLet Y_1,\\dots,Y_n be independent random variable distributed according to a hyperbolic secant distribution, whose density is \nf(y_i; \\theta_i) = \\frac{\\exp\\{\\theta_i y_i - \\log{\\cos(\\theta_i)} \\}}{2 \\cosh(\\pi y_i / 2)}, \\qquad y_i \\in \\mathbb{R}, \\; \\theta_i \\in \\left(-\\frac{\\pi}{2}, \\frac{\\pi}{2}\\right).\n This distribution is discussed in Morris (1982) and is an instance of highly tractable exponential family having a quadratic variance function.\n\n\n\n\n\nThis distribution can be employed build a novel generalized linear model (GLM), which automatically incorporates heteroskedasticity and with heavier tails that the Gaussian law.\nThere are multiple application areas of such a regression technique, including (but not limited to) financial data."
  },
  {
    "objectID": "post/Poli2025/Polimi2025_slides.html#expected-outcome-and-research-gap",
    "href": "post/Poli2025/Polimi2025_slides.html#expected-outcome-and-research-gap",
    "title": "Tutor: Tommaso Rigon",
    "section": "Expected outcome and research gap",
    "text": "Expected outcome and research gap\n\n\n\nResearch gap\n\n\n\nA systematic Bayesian investigation of such a GLM is entirely missing. In principle, this GLM framework can be combined with modern Bayesian tool for regression, such as:\n\nVariable selection and regularization, e.g. using shrinkage priors or spike-and-slab priors\nRandom effects (parametric and non-parametric).\nGeneralized Additive Models (GAMs), e.g. using Gaussian Processes.\n\nOverdispersion may require novel generalized Bayes techniques (Agnoletto et al. 2025).\nIt is possible that a Polya-gamma data augmentation scheme can be employed by adapting the results in Polson et al. (2013). If not, Hamiltonian Monte Carlo (HMC) is certainly feasible.\n\n\n\n\n\n\n\nExpected outcome\n\n\nThe group is expected to implement and develop one or more of the above research gaps.\nI expect that the group is proficient in R programming (C++ programming is a big plus but probably not necessary). Most of the work relates to the practical implementation of the above ideas."
  },
  {
    "objectID": "post/Poli2025/Polimi2025_slides.html#references",
    "href": "post/Poli2025/Polimi2025_slides.html#references",
    "title": "Tutor: Tommaso Rigon",
    "section": "References",
    "text": "References\n\n\n\n\nAgnoletto, D., Rigon, T., and Dunson, D. B. (2025), “Bayesian inference for generalized linear models via quasi-posteriors,” Biometrika, 112.\n\n\nMorris, C. N. (1982), “Natural exponential families with quadratic variance functions,” Annals of Statistics, 10, 65–80.\n\n\nPolson, N. G., Scott, J. G., and Windle, J. (2013), “Bayesian inference for logistic models using polya-gamma latent variables,” Journal of the American Statistical Association, Taylor {&} Francis, 108, 1–42."
  },
  {
    "objectID": "post/Poli2025/Poli2025.html#hyperbolic-secant-regression",
    "href": "post/Poli2025/Poli2025.html#hyperbolic-secant-regression",
    "title": "Project presentation",
    "section": "Hyperbolic secant regression",
    "text": "Hyperbolic secant regression\n\n\n\n\n\n\nBackground\n\n\n\nLet Y_1,\\dots,Y_n be independent random variable distributed according to a hyperbolic secant distribution, whose density is \nf(y_i; \\theta_i) = \\frac{\\exp\\{\\theta_i y_i - \\log{\\cos(\\theta_i)} \\}}{2 \\cosh(\\pi y_i / 2)}, \\qquad y_i \\in \\mathbb{R}, \\; \\theta_i \\in \\left(-\\frac{\\pi}{2}, \\frac{\\pi}{2}\\right).\n This distribution is discussed in Morris (1982) and is an instance of highly tractable exponential family having a quadratic variance function.\n\n\n\nThis distribution can be employed to build a novel generalized linear model (GLM), which automatically incorporates heteroskedasticity and exhibits heavier tails than the Gaussian law.\nThere are multiple application areas for such a regression technique, including (but not limited to) financial data."
  },
  {
    "objectID": "post/Poli2025/Poli2025.html#expected-outcome-and-research-gap",
    "href": "post/Poli2025/Poli2025.html#expected-outcome-and-research-gap",
    "title": "Tutor: Tommaso Rigon",
    "section": "Expected outcome and research gap",
    "text": "Expected outcome and research gap\n\n\n\n\n\n\nResearch gap\n\n\n\n\nA systematic Bayesian investigation of such a GLM is entirely missing. In principle, this GLM framework can be combined with modern Bayesian tool for regression, such as:\n\nVariable selection and regularization, e.g. using shrinkage priors or spike-and-slab priors\nRandom effects (parametric and non-parametric).\nGeneralized Additive Models (GAMs), e.g. using Gaussian Processes\n\nOverdispersion may require novel generalized Bayes techniques (Agnoletto et al. 2025).\nIt is possible that a Polya-gamma data augmentation scheme can be employed by adapting the results in Polson et al. (2013). If not, Hamiltonian Monte Carlo (HMC) is certainly feasible.\n\n\n\n\n\n\n\n\n\nExpected outcome\n\n\n\nThe group is expected to implement and develop one or more of the above research gaps.\nI expect that the group is proficient in R programming (C++ programming is a big plus but probably not necessary). Most of the work relates to the practical implementation of the above ideas."
  },
  {
    "objectID": "post/Poli2025/Polimi2025_slides.html#objectives-and-expectations",
    "href": "post/Poli2025/Polimi2025_slides.html#objectives-and-expectations",
    "title": "Tutor: Tommaso Rigon",
    "section": "Objectives and expectations",
    "text": "Objectives and expectations\n\n\n\nResearch gap\n\n\n\nA systematic Bayesian investigation of such a GLM is entirely lacking. In principle, this GLM framework can be combined with modern Bayesian tools for regression, such as:\n\nVariable selection and regularization, e.g., shrinkage priors or spike-and-slab priors.\nRandom effects (parametric and nonparametric).\nGeneralized Additive Models (GAMs), e.g., Gaussian Processes.\n\nOverdispersion may require novel generalized Bayes techniques (Agnoletto et al. 2025).\nA Polya-gamma data augmentation scheme may be applicable by adapting the results in Polson et al. (2013). If not, Hamiltonian Monte Carlo (HMC) remains a feasible option.\n\n\n\n\n\n\n\nExpected outcome\n\n\nThe group is expected to implement and develop 2–3 of the research gaps outlined above.\nI expect the group to be proficient in R programming (knowledge of C++ is a strong asset, though probably not essential). Most of the work will focus on the practical implementation of these ideas."
  },
  {
    "objectID": "post/Poli2025/Poli2025.html#objectives-and-expectations",
    "href": "post/Poli2025/Poli2025.html#objectives-and-expectations",
    "title": "Project presentation",
    "section": "Objectives and expectations",
    "text": "Objectives and expectations\n\n\n\n\n\n\nResearch gap\n\n\n\n\nA systematic Bayesian investigation of such a GLM is entirely lacking. In principle, this GLM framework can be combined with modern Bayesian tools for regression, such as:\n\nVariable selection and regularization, e.g., shrinkage priors or spike-and-slab priors.\nRandom effects (parametric and nonparametric).\nGeneralized Additive Models (GAMs), e.g., Gaussian Processes.\n\nOverdispersion may require novel generalized Bayes techniques (Agnoletto et al. 2025).\nA Polya-gamma data augmentation scheme may be applicable by adapting the results in Polson et al. (2013). If not, Hamiltonian Monte Carlo (HMC) remains a feasible option.\n\n\n\n\n\n\n\n\n\nExpected outcome\n\n\n\nThe group is expected to implement and develop 2–3 of the research gaps outlined above.\nI expect the group to be proficient in R programming (knowledge of C++ is a strong asset, though probably not essential). Most of the work will focus on the practical implementation of these ideas."
  },
  {
    "objectID": "post/abaco26/abaco26.html",
    "href": "post/abaco26/abaco26.html",
    "title": "ABACO26",
    "section": "",
    "text": "Davide Agnoletto (Duke University)\n\n\n\n\n\n\n\n\nDavid Dunson (Duke University)"
  },
  {
    "objectID": "post/abaco26/abaco26.html#warm-thanks",
    "href": "post/abaco26/abaco26.html#warm-thanks",
    "title": "ABACO26",
    "section": "",
    "text": "Davide Agnoletto (Duke University)\n\n\n\n\n\n\n\n\nDavid Dunson (Duke University)"
  },
  {
    "objectID": "post/abaco26/abaco26.html#foundations",
    "href": "post/abaco26/abaco26.html#foundations",
    "title": "ABACO26",
    "section": "Foundations",
    "text": "Foundations\n\nDe Finetti’s representation Theorem (De Finetti 1937) it provides the fundamental justification to the two approaches to Bayesian statistics: the hypothetical approach and the predictive approach.\n\n\n\n\n\n\n\nDe Finetti’s representation theorem\n\n\n\nLet (Y_n)_{n\\ge 1}, be a sequence of exchangeable random variables. Then there exists a unique probability measure \\Pi such that, for any n\\ge 1 and A_1,\\dots,A_n \n\\mathbb{P}(Y_1 \\in A_1,\\ldots,Y_n \\in A_n) = \\int_{\\mathcal{P}} \\prod_{i=1}^n p(A_i)\\,\\Pi(\\mathrm{d}p).\n\n\n\n\nIn a hierarchical formulation, we will say that (Y_n)_{n \\ge 1} is exchangeable if and only if \n\\begin{aligned}\nY_i \\mid P &\\overset{\\textup{iid}}{\\sim} P, \\qquad i \\ge 1, \\\\\nP &\\sim \\Pi,\n\\end{aligned}\n where P is a random probability measure and \\Pi is the prior law."
  },
  {
    "objectID": "post/abaco26/abaco26.html#hypothetical-approach",
    "href": "post/abaco26/abaco26.html#hypothetical-approach",
    "title": "ABACO26",
    "section": "Hypothetical approach",
    "text": "Hypothetical approach\n\nThe hypothetical approach represents the the most common way to operate within the Bayesian community.\nIn a parametric setting, \\Pi has support on a class \\Theta\\subseteq\\mathbb{R}^p with p&lt;\\infty, such that \\boldsymbol{\\theta}\\in\\Theta indexes the class of distributions \\mathcal{P}_{\\boldsymbol{\\theta}}=\\{P_{\\boldsymbol{\\theta}} : \\boldsymbol{\\theta} \\in \\Theta\\subseteq\\mathbb{R}^p\\}.\nBayes’ rule takes the well-known formulation: \n\\pi(\\boldsymbol{\\theta}\\mid y_{1:n}) \\propto \\pi(\\boldsymbol{\\theta}) \\prod_{i=1}^n p_{\\boldsymbol{\\theta}}(y_i),\n where \\pi and p_{\\boldsymbol{\\theta}} denote the probability density functions associated with \\Pi and P_{\\boldsymbol{\\theta}}, respectively.\nHowever, when the link between observations and parameter of interest cannot be easily expressed through a distribution function, the traditional hypothetical approach fails.\n\n\nSolution: generalized posterior distributions, sometimes called Gibbs-posteriors.\nThis is a lively recent topic, see for instance: Chernozhukov and Hong (2003); Bissiri et al. (2016) Heide et al. (2020); Grünwald and Mehta (2020); Knoblauch et al. (2022); Matsubara et al. (2022); Matsubara et al. (2023); Jewson and Rossell (2022); Rigon et al. (2023); Agnoletto et al. (2025)."
  },
  {
    "objectID": "post/abaco26/abaco26.html#generalizations-of-the-hypothetical-approach",
    "href": "post/abaco26/abaco26.html#generalizations-of-the-hypothetical-approach",
    "title": "ABACO26",
    "section": "Generalizations of the hypothetical approach",
    "text": "Generalizations of the hypothetical approach\n\nBissiri et al. (2016) showed that the generalized posterior \n\\pi_\\omega(\\boldsymbol{\\theta} \\mid y_{1:n}) \\propto \\pi(\\boldsymbol{\\theta}) \\exp\\left\\{ - \\omega \\sum_{i=1}^n \\ell(\\boldsymbol{\\theta}; y_i) \\right\\},\n is the only coherent update of the prior beliefs about \n\\boldsymbol{\\theta}^* = \\arg\\min_{\\boldsymbol{\\theta}\\in\\Theta} \\int_{\\mathcal{Y}} \\ell(\\boldsymbol{\\theta}; y)\\, F_0(\\mathrm{d}y),\n where \\ell(\\boldsymbol{\\theta}, y) is a loss function, \\omega is the loss-scale, and F_0 is the unknown true sampling distribution.\n\n\nLearning the loss scale \\omega from the data is a delicate task. Assuming a prior for \\omega can lead to degenerate estimates if not accompanied by additional adjustments to the loss function.\nHowever, there are several solutions for its calibration: Holmes and Walker (2017); Lyddon et al. (2019); Syring and Martin (2019); Matsubara et al. (2023)."
  },
  {
    "objectID": "post/abaco26/abaco26.html#generalized-bayes-for-glms",
    "href": "post/abaco26/abaco26.html#generalized-bayes-for-glms",
    "title": "ABACO26",
    "section": "Generalized Bayes for GLMs",
    "text": "Generalized Bayes for GLMs\n\nGeneralized linear models (GLMs) are routinely used to model a wide variety of data.\nThe Bayesian approach for GLMs is also incredibly popular, e.g. because of the possibility of naturally incorporating random effects, complex penalizations, prior information, and more.\nHowever, they often incur misspecification, and this could compromise inferential conclusions.\n\n\nA common case is overdispersion, i.e., when proportion or count observations show larger variability than the one assumed by the model.\nTraditional solutions have important drawbacks:\n\nModel-based: may lead to computational bottlenecks and can result again in misspecification.\nNonparametric: increased computational cost and loss of efficiency and interpretability.\n\n\n\nWe rely on a semi-parametric approach, making only assumptions on the mean and variance of the response while preserving computational tractability."
  },
  {
    "objectID": "post/abaco26/abaco26.html#second-order-assumptions",
    "href": "post/abaco26/abaco26.html#second-order-assumptions",
    "title": "ABACO26",
    "section": "Second order assumptions",
    "text": "Second order assumptions\n\nLet Y_i \\in \\mathcal{Y} denote a response variable, \\boldsymbol{x}_i \\in \\mathbb{R}^p be a vector of covariates for i = 1, \\ldots, n, and \\boldsymbol{\\beta} \\in \\mathbb{R}^p be the parameter of interest.\nStandard GLMs assume that observations y_i are independent realizations of Y_i \\mid \\boldsymbol{x}_i, whose distribution belongs to the exponential dispersion family.\n\n\n\n\n\n\n\nWe assume the second-order conditions: \n\\mathbb{E}\\{Y_i\\} = \\mu_i = g^{-1}(\\boldsymbol{x}_i^\\top \\boldsymbol{\\beta}), \\quad\n\\mathrm{var}\\{Y_i\\} = \\psi\\, V(\\mu_i),\n where g(\\cdot) is a link function, V(\\cdot)&gt;0 is a variance function, and \\psi \\in (0,\\infty) is a dispersion parameter.\nWe let (\\boldsymbol{\\beta}_0, \\psi_0) be the true values for the parameters (\\boldsymbol{\\beta}, \\psi) and we assume the data are generated under F_0(\\mathrm{d}y \\mid \\boldsymbol{x}) = F(\\mathrm{d}y \\mid \\boldsymbol{x}, \\boldsymbol{\\beta}_0, \\psi_0).\n\n\n\n\nAlthough the mean and variance functions need to be carefully chosen to fit the data, the resulting inferences are robust to misspecification of higher-order moments."
  },
  {
    "objectID": "post/abaco26/abaco26.html#quasi-likelihood",
    "href": "post/abaco26/abaco26.html#quasi-likelihood",
    "title": "ABACO26",
    "section": "Quasi-likelihood",
    "text": "Quasi-likelihood\n\nUnder the second-order assumptions, it is possible to specify the so-called log-quasi-likelihood function (Wedderburn 1974): \n\\ell_Q(\\boldsymbol{\\beta}; \\mathbf{y}, \\mathbf{X}, \\psi) = \\sum_{i=1}^n \\ell_Q(\\boldsymbol{\\beta}; y_i, \\boldsymbol{x}_i, \\psi) = \\sum_{i=1}^n \\int_a^{\\mu_i(\\boldsymbol{\\beta})} \\frac{y_i - t}{\\psi V(t)} \\, \\mathrm{d}t,\n where a is an arbitrary constant that does not depend on \\boldsymbol{\\beta}.\nThe above integral can be written in closed form for many choices of variance functions, including those associated with exponential family distributions.\n\n\nQuasi-likelihoods retain many properties of genuine likelihoods, such as unbiased estimating equations and the information identity: \n\\mathbb{E}\\left\\{ \\nabla \\ell_Q(\\boldsymbol{\\beta}; \\mathbf{Y}, \\mathbf{X}, \\psi) \\right\\} = 0, \\qquad\n\\mathbb{E}\\left\\{ -\\nabla^2 \\ell_Q(\\boldsymbol{\\beta}; \\mathbf{Y}, \\mathbf{X}, \\psi) \\right\\} =\n\\mathbb{E}\\left\\{ \\nabla \\ell_Q \\nabla \\ell_Q^\\top \\right\\},\n where \\nabla denotes the gradient with respect to \\boldsymbol{\\beta}.\n\n\nUnder mild conditions, the maximum quasi-likelihood is consistent and has the smallest asymptotic variance among estimators derived from linear estimating equations (McCullagh 1983)."
  },
  {
    "objectID": "post/abaco26/abaco26.html#quasi-posteriors-i",
    "href": "post/abaco26/abaco26.html#quasi-posteriors-i",
    "title": "ABACO26",
    "section": "Quasi-posteriors I",
    "text": "Quasi-posteriors I\n\n\n\n\n\n\nLet \\exp\\{\\ell_Q(\\boldsymbol{\\beta}; \\mathbf{y}, \\mathbf{X}, \\psi)\\} be the quasi-likelihood function and p(\\boldsymbol{\\beta}) be the prior distribution for \\boldsymbol{\\beta}. We define the quasi-posterior distribution for \\boldsymbol{\\beta} as: \np_Q(\\boldsymbol{\\beta} \\mid \\mathbf{y}, \\mathbf{X}, \\psi) \\propto p(\\boldsymbol{\\beta}) \\exp \\left\\{ \\ell_Q(\\boldsymbol{\\beta}; \\mathbf{y}, \\mathbf{X}, \\psi) \\right\\} =\np(\\boldsymbol{\\beta}) \\exp \\left\\{ \\frac{1}{\\psi} \\sum_{i=1}^n \\int_a^{\\mu_i(\\boldsymbol{\\beta})} \\frac{y_i - t}{V(t)} \\, \\mathrm{d}t \\right\\}\n\n\n\n\n\nThe quasi-posterior is a rational update of a belief distribution within the generalized Bayesian framework, with loss function: \n\\ell(\\boldsymbol{\\beta}; \\mathbf{y}, \\mathbf{X}) = - \\sum_{i=1}^n \\int_a^{\\mu_i(\\boldsymbol{\\beta})} \\frac{y_i - t}{V(t)} \\, \\mathrm{d}t.\n\nThe dispersion parameter \\psi plays the role of a loss-scale parameter for the quasi-posterior."
  },
  {
    "objectID": "post/abaco26/abaco26.html#quasi-posteriors-ii",
    "href": "post/abaco26/abaco26.html#quasi-posteriors-ii",
    "title": "ABACO26",
    "section": "Quasi-posteriors II",
    "text": "Quasi-posteriors II\n\nThe quasi-posterior represents subjective uncertainty about the unknown parameter value: \n\\boldsymbol{\\beta}^* = \\arg\\min_{\\boldsymbol{\\beta}} \\int_{\\mathcal{Y}} \\ell(\\boldsymbol{\\beta}; \\mathbf{y}, \\mathbf{X}) \\, F_0(d\\mathbf{y} \\mid \\mathbf{X}),\n which is assumed to be unique (Bissiri et al. 2016).\nThe definition of \\boldsymbol{\\beta}^* does not automatically guarantee that \\boldsymbol{\\beta}^* = \\boldsymbol{\\beta}_0.\n\n\n\n\n\n\n\nTheorem (Agnoletto, R., Dunson, 2025)\n\n\n\nAssume the second-order conditions are well-specified, and suppose the target of inference \\boldsymbol{\\beta}^* is unique. Then, for quasi-posteriors, \\boldsymbol{\\beta}^* must coincide with the true value \\boldsymbol{\\beta}_0: \n\\boldsymbol{\\beta}^* = \\arg\\min_{\\boldsymbol{\\beta}} \\int_{\\mathcal{Y}}\n\\sum_{i=1}^n \\int_a^{\\mu_i(\\boldsymbol{\\beta})} \\frac{t - y_i}{V(t)} \\, \\mathrm{d}t \\, F_0(d\\mathbf{y} \\mid \\mathbf{X}) = \\boldsymbol{\\beta}_0."
  },
  {
    "objectID": "post/abaco26/abaco26.html#calibration-of-the-dispersion-i",
    "href": "post/abaco26/abaco26.html#calibration-of-the-dispersion-i",
    "title": "ABACO26",
    "section": "Calibration of the dispersion I",
    "text": "Calibration of the dispersion I\n\nBased on a comparison with the Bayesian bootstrap, Lyddon et al. (2019) propose calibrate the dispersion \\psi setting it equal to: \n\\psi_{\\text{LLB}} =\n\\frac{\\mathrm{tr}\\{j(\\boldsymbol{\\beta}_0)\\}}{\\mathrm{tr}\\{j(\\boldsymbol{\\beta}_0) h(\\boldsymbol{\\beta}_0)^{-1} j(\\boldsymbol{\\beta}_0)\\}},\n where we define \nj(\\boldsymbol{\\beta}) := \\lim_{n \\to \\infty} \\frac{1}{n} \\mathbb{E}\\left[\\nabla^2 \\ell(\\boldsymbol{\\beta}; \\mathbf{Y}, \\mathbf{X})\\right], \\qquad  h(\\boldsymbol{\\beta}) := \\lim_{n \\to \\infty} \\frac{1}{n} \\mathbb{E}\\left[\\nabla \\ell(\\boldsymbol{\\beta}; \\mathbf{Y}, \\mathbf{X}) \\nabla \\ell(\\boldsymbol{\\beta}; \\mathbf{Y}, \\mathbf{X})^\\top\\right].\n\n\n\n\n\n\n\n\nProposition (Agnoletto, R., Dunson, 2025)\n\n\n\nUnder the second order conditions, namely if \\mathbb{E}(Y_i) = g^{-1}(\\mathbf{x}_i^\\top \\boldsymbol{\\beta}_0) and \\mathrm{var}(Y_i) = \\psi_0 V\\{\\mu_i(\\boldsymbol{\\beta}_0)\\}, then for quasi posteriors with loss \\ell(\\boldsymbol{\\beta}; \\mathbf{y}, \\mathbf{X}) = -\\sum_{i=1}^n \\int_a^{\\mu_i(\\boldsymbol{\\beta})} \\frac{y_i - t}{V(t)} \\mathrm{d}t, we have \n\\psi_{\\text{LLB}} = \\psi_0."
  },
  {
    "objectID": "post/abaco26/abaco26.html#calibration-of-the-dispersion-ii",
    "href": "post/abaco26/abaco26.html#calibration-of-the-dispersion-ii",
    "title": "ABACO26",
    "section": "Calibration of the dispersion II",
    "text": "Calibration of the dispersion II\n\n\n\n\n\n\nTheorem (Agnoletto, R., Dunson, 2025)\n\n\n\nAssume the second-order conditions are correctly specified. Let S_1, S_2, \\ldots \\subseteq \\mathbb{R}^p be a sequence of convex credible sets of level \\rho \\in (0,1). Then, under mild conditions and setting \\psi = \\psi_0: \n\\mathbb{P}(\\boldsymbol{\\beta}_0 \\in S_n \\mid \\mathbf{y}, \\mathbf{X}, \\psi_0) \\to \\rho \\quad \\text{as } n \\to \\infty.\n\n\n\n\nAs \\psi_0 is typically unknown, we can use the classical method of moments estimator: \n\\widehat{\\psi} = \\frac{1}{n - p} \\sum_{i=1}^n \\frac{(y_i - \\widehat{\\mu}_i)^2}{V(\\widehat{\\mu}_i)},\n where \\widehat{\\mu}_i = \\mu_i(\\widehat{\\boldsymbol{\\beta}}), which is fast and consistent (McCullagh and Nelder 1989)."
  },
  {
    "objectID": "post/abaco26/abaco26.html#simulation-results-i",
    "href": "post/abaco26/abaco26.html#simulation-results-i",
    "title": "ABACO26",
    "section": "Simulation results I",
    "text": "Simulation results I\n\nData are generated from a distribution with \\mathbb{E}(Y_i) = \\mu_i(\\boldsymbol{\\beta}_0)= \\exp(\\mathbf{x}_i^\\top \\boldsymbol{\\beta}_0) and \\mathrm{var}(Y_i) = \\psi_0 \\mu_i(\\boldsymbol{\\beta}_0) —not a Poisson!— with parameters \\beta_0 = (3.5,\\; 1.5,\\; -1.0,\\; 0.5), and \\psi_0 = 3.5.\nWe computed posterior means and 95\\% credible intervals.\nThe sample size is n = 100; estimates are averages over multiple simulated datasets.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPoisson\nNegative Binomial\nDFD-Bayes\nQuasi-posterior\nQuasi-posterior + alternative \\hat{\\psi}\n\n\n\n\n\\beta_1\nMean\n3.50 (0.035)\n3.49 (0.047)\n57.46 (105.41)\n3.50 (0.035)\n3.50 (0.035)\n\n\n\nCover.\n0.715\n0.920\n0.201\n0.945\n0.970\n\n\n\\beta_2\nMean\n1.50 (0.020)\n1.51 (0.040)\n5.26 (6.31)\n1.50 (0.020)\n1.50 (0.020)\n\n\n\nCover.\n0.675\n0.960\n0.454\n0.945\n0.990\n\n\n\\beta_3\nMean\n-1.00 (0.020)\n-1.01 (0.034)\n-3.98 (6.53)\n-1.00 (0.020)\n-1.00 (0.020)\n\n\n\nCover.\n0.715\n0.995\n0.479\n0.950\n0.965\n\n\n\\beta_4\nMean\n0.50 (0.018)\n0.50 (0.037)\n2.55 (7.37)\n0.50 (0.018)\n0.50 (0.018)\n\n\n\nCover.\n0.655\n0.965\n0.526\n0.950\n0.970"
  },
  {
    "objectID": "post/abaco26/abaco26.html#simulation-results-ii",
    "href": "post/abaco26/abaco26.html#simulation-results-ii",
    "title": "ABACO26",
    "section": "Simulation results II",
    "text": "Simulation results II\n\nData are generated from a distribution with \\mathbb{E}(Y_i) = \\mu_i(\\boldsymbol{\\beta}_0)= \\exp(\\mathbf{x}_i^\\top \\boldsymbol{\\beta}_0) and \\mathrm{var}(Y_i) = \\psi_0 \\mu_i(\\boldsymbol{\\beta}_0) —not a Poisson!— with parameters \\beta_0 = (3.5,\\; 1.5,\\; -1.0,\\; 0.5), and \\psi_0 = 3.5.\nWe computed posterior means and 95\\% credible intervals.\nThe sample size is n = 1000; estimates are averages over multiple simulated datasets.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPoisson\nNegative Binomial\nDFD-Bayes\nQuasi-posterior\nQuasi-posterior + alternative \\hat{\\psi}\n\n\n\n\n\\beta_1\nMean\n3.50 (0.010)\n3.50 (0.015)\n4.64 (8.45)\n3.50 (0.010)\n3.50 (0.010)\n\n\n\nCover.\n0.690\n0.835\n0.070\n0.945\n0.955\n\n\n\\beta_2\nMean\n1.50 (0.005)\n1.50 (0.012)\n1.87 (1.37)\n1.50 (0.005)\n1.50 (0.005)\n\n\n\nCover.\n0.665\n0.910\n0.510\n0.950\n0.960\n\n\n\\beta_3\nMean\n-1.00 (0.005)\n-1.00 (0.010)\n-1.21 (0.81)\n-1.00 (0.005)\n-1.00 (0.005)\n\n\n\nCover.\n0.680\n0.960\n0.690\n0.955\n0.950\n\n\n\\beta_4\nMean\n0.50 (0.005)\n0.50 (0.009)\n0.54 (0.35)\n0.50 (0.005)\n0.50 (0.005)\n\n\n\nCover.\n0.715\n0.950\n0.810\n0.950\n0.940"
  },
  {
    "objectID": "post/abaco26/abaco26.html#thank-you",
    "href": "post/abaco26/abaco26.html#thank-you",
    "title": "ABACO26",
    "section": "Thank you!",
    "text": "Thank you!\n\n\n\n\n\nThe main paper is:\nAgnoletto, D., Rigon, T., and Dunson D.B. (2025+). Nonparametric predictive inference for discrete data via Metropolis-adjusted Dirichlet sequences. arXiv:2507.08629"
  },
  {
    "objectID": "post/abaco26/abaco26.html#references",
    "href": "post/abaco26/abaco26.html#references",
    "title": "ABACO26",
    "section": "References",
    "text": "References\n\n\nAgnoletto, D., Rigon, T., and Dunson, D. B. (2025), “Bayesian inference for generalized linear models via quasi-posteriors,” Biometrika, 112.\n\n\nAldous, D. J. (1985), “Exchangeability and related topics, ecole d’eté de saint-flour XIII, lectures notes n 1117,” Springer Verlag.\n\n\nBerti, P., Dreassi, E., Leisen, F., Pratelli, L., and Rigo, P. (2023b), “Kernel based dirichlet sequences,” Bernoulli, Bernoulli Society for Mathematical Statistics; Probability, 29, 1321–1342.\n\n\nBerti, P., Dreassi, E., Leisen, F., Pratelli, L., and Rigo, P. (2023a), “Bayesian predictive inference without a prior,” Statistica Sinica, Academia Sinica, Institute of Statistical Science, 34, 2405–2429.\n\n\nBerti, P., Dreassi, E., Pratelli, L., and Rigo, P. (2021), “A class of models for Bayesian predictive inference,” Bernoulli, 27, 702–726.\n\n\nBerti, P., Pratelli, L., and Rigo, P. (2004), “Limit theorems for a class of identically distributed random variables,” 32, 2029–2052.\n\n\nBissiri, P. G., Holmes, C. C., and Walker, S. G. (2016), “A general framework for updating belief distributions,” Journal of the Royal Statistical Society: Series B (Statistical Methodology), Wiley Online Library, 78, 1103–1130.\n\n\nCanale, A., and Dunson, D. B. (2011), “Bayesian kernel mixtures for counts,” Journal of the American Statistical Association, Taylor & Francis, 106, 1528–1539.\n\n\nChernozhukov, V., and Hong, H. (2003), “An MCMC approach to classical estimation,” Journal of econometrics, Elsevier, 115, 293–346.\n\n\nCui, F., and Walker, S. G. (2024), “Martingale posterior distributions for log-concave density functions,” arXiv preprint arXiv:2401.14515.\n\n\nDe Finetti, B. (1937), “La prévision: Ses lois logiques, ses sources subjectives,” in Annales de l’institut henri poincaré, pp. 1–68.\n\n\nFong, E., Holmes, C. C., and Walker, S. G. (2023), “Martingale posterior distributions,” Journal of the Royal Statistical Society Series B: Statistical Methodology, 85, 1357–1391.\n\n\nFong, E., and Yiu, A. (2024b), “Bayesian quantile estimation and regression with martingale posteriors,” arXiv preprint arXiv:2406.03358.\n\n\nFong, E., and Yiu, A. (2024a), “Asymptotics for parametric martingale posteriors,” arXiv preprint arXiv:2410.17692.\n\n\nFortini, S., Ladelli, L., and Regazzini, E. (2000), “Exchangeability, predictive distributions and parametric models,” Sankhya: The Indian Journal of Statistics, Series A, 62, 86–109.\n\n\nFortini, S., and Petrone, S. (2012), “Predictive construction of priors in bayesian nonparametrics,” Brazilian Journal of Probability and Statistics, 26, 423–449.\n\n\nFortini, S., and Petrone, S. (2020), “Quasi-Bayes properties of a procedure for sequential learning in mixture models,” Journal of the Royal Statistical Society Series B: Statistical Methodology, Oxford University Press, 82, 1087–1114.\n\n\nFortini, S., and Petrone, S. (2024), “Exchangeability, prediction and predictive modeling in bayesian statistics,” arXiv preprint arXiv:2402.10126.\n\n\nFortini, S., Petrone, S., and Sariev, H. (2021), “Predictive constructions based on measure-valued pólya urn processes,” Mathematics, MDPI, 9, 2845.\n\n\nGrünwald, P. D., and Mehta, N. A. (2020), “Fast rates for general unbounded loss functions: from ERM to generalized Bayes,” The Journal of Machine Learning Research, JMLRORG, 21, 2040–2119.\n\n\nHeide, R. de, Kirichenko, A., Grunwald, P., and Mehta, N. (2020), “Safe-bayesian generalized linear regression,” in Proceedings of the twenty third international conference on artificial intelligence and statistics, PMLR, pp. 2623–2633.\n\n\nHolmes, C. C., and Walker, S. G. (2017), “Assigning a value to a power likelihood in a general Bayesian model,” Biometrika, Oxford University Press, 104, 497–503.\n\n\nJewson, J., and Rossell, D. (2022), “General bayesian loss function selection and the use of improper models,” Journal of the Royal Statistical Society Series B: Statistical Methodology, Oxford University Press, 84, 1640–1665.\n\n\nKnoblauch, J., Jewson, J., and Damoulas, T. (2022), “An optimization-centric view on bayes’ rule: Reviewing and generalizing variational inference,” Journal of Machine Learning Research, 23, 1–109.\n\n\nLyddon, S. P., Holmes, C. C., and Walker, S. G. (2019), “General Bayesian updating and the loss-likelihood bootstrap,” Biometrika, Oxford University Press, 106, 465–478.\n\n\nMartin, R., and Tokdar, S. T. (2009), “Asymptotic properties of predictive recursion: Robustness and rate of convergence,” Electornic Journal of Statistics, 3, 1455–1472.\n\n\nMatsubara, T., Knoblauch, J., Briol, F.-X., and Oates, C. J. (2022), “Robust generalised bayesian inference for intractable likelihoods,” Journal of the Royal Statistical Society Series B: Statistical Methodology, Oxford University Press, 84, 997–1022.\n\n\nMatsubara, T., Knoblauch, J., Briol, F.-X., and Oates, C. J. (2023), “Generalized bayesian inference for discrete intractable likelihood,” Journal of the American Statistical Association, Taylor & Francis, 1–11.\n\n\nRigon, T., Herring, A. H., and Dunson, D. B. (2023), “A generalized Bayes framework for probabilistic clustering,” Biometrika, Oxford University Press, 10, 559–578.\n\n\nSyring, N., and Martin, R. (2019), “Calibrating general posterior credible regions,” Biometrika, Oxford University Press, 106, 479–486."
  },
  {
    "objectID": "post/abaco26/abaco26_slides.html#warm-thanks",
    "href": "post/abaco26/abaco26_slides.html#warm-thanks",
    "title": "ABACO26",
    "section": "Warm thanks",
    "text": "Warm thanks\n\n\nDavide Agnoletto (Duke University)\n\n\n\n\n\n\n\n\nDavid Dunson (Duke University)"
  },
  {
    "objectID": "post/abaco26/abaco26_slides.html#foundations",
    "href": "post/abaco26/abaco26_slides.html#foundations",
    "title": "ABACO26",
    "section": "Foundations",
    "text": "Foundations\n\nDe Finetti’s representation Theorem (De Finetti 1937) it provides the fundamental justification to the two approaches to Bayesian statistics: the hypothetical approach and the predictive approach.\n\n\n\n\n\nDe Finetti’s representation theorem\n\n\nLet (Y_n)_{n\\ge 1}, be a sequence of exchangeable random variables. Then there exists a unique probability measure \\Pi such that, for any n\\ge 1 and A_1,\\dots,A_n \n\\mathbb{P}(Y_1 \\in A_1,\\ldots,Y_n \\in A_n) = \\int_{\\mathcal{P}} \\prod_{i=1}^n p(A_i)\\,\\Pi(\\mathrm{d}p).\n\n\n\n\n\n\nIn a hierarchical formulation, we will say that (Y_n)_{n \\ge 1} is exchangeable if and only if \n\\begin{aligned}\nY_i \\mid P &\\overset{\\textup{iid}}{\\sim} P, \\qquad i \\ge 1, \\\\\nP &\\sim \\Pi,\n\\end{aligned}\n where P is a random probability measure and \\Pi is the prior law."
  },
  {
    "objectID": "post/abaco26/abaco26_slides.html#hypothetical-approach",
    "href": "post/abaco26/abaco26_slides.html#hypothetical-approach",
    "title": "ABACO26",
    "section": "Hypothetical approach",
    "text": "Hypothetical approach\n\nThe hypothetical approach represents the the most common way to operate within the Bayesian community.\nIn a parametric setting, \\Pi has support on a class \\Theta\\subseteq\\mathbb{R}^p with p&lt;\\infty, such that \\boldsymbol{\\theta}\\in\\Theta indexes the class of distributions \\mathcal{P}_{\\boldsymbol{\\theta}}=\\{P_{\\boldsymbol{\\theta}} : \\boldsymbol{\\theta} \\in \\Theta\\subseteq\\mathbb{R}^p\\}.\nBayes’ rule takes the well-known formulation: \n\\pi(\\boldsymbol{\\theta}\\mid y_{1:n}) \\propto \\pi(\\boldsymbol{\\theta}) \\prod_{i=1}^n p_{\\boldsymbol{\\theta}}(y_i),\n where \\pi and p_{\\boldsymbol{\\theta}} denote the probability density functions associated with \\Pi and P_{\\boldsymbol{\\theta}}, respectively.\nHowever, when the link between observations and parameter of interest cannot be easily expressed through a distribution function, the traditional hypothetical approach fails.\n\n\n\nSolution: generalized posterior distributions, sometimes called Gibbs-posteriors.\nThis is a lively recent topic, see for instance: Chernozhukov and Hong (2003); Bissiri et al. (2016) Heide et al. (2020); Grünwald and Mehta (2020); Knoblauch et al. (2022); Matsubara et al. (2022); Matsubara et al. (2023); Jewson and Rossell (2022); Rigon et al. (2023); Agnoletto et al. (2025)."
  },
  {
    "objectID": "post/abaco26/abaco26_slides.html#generalizations-of-the-hypothetical-approach",
    "href": "post/abaco26/abaco26_slides.html#generalizations-of-the-hypothetical-approach",
    "title": "ABACO26",
    "section": "Generalizations of the hypothetical approach",
    "text": "Generalizations of the hypothetical approach\n\nBissiri et al. (2016) showed that the generalized posterior \n\\pi_\\omega(\\boldsymbol{\\theta} \\mid y_{1:n}) \\propto \\pi(\\boldsymbol{\\theta}) \\exp\\left\\{ - \\omega \\sum_{i=1}^n \\ell(\\boldsymbol{\\theta}; y_i) \\right\\},\n is the only coherent update of the prior beliefs about \n\\boldsymbol{\\theta}^* = \\arg\\min_{\\boldsymbol{\\theta}\\in\\Theta} \\int_{\\mathcal{Y}} \\ell(\\boldsymbol{\\theta}; y)\\, F_0(\\mathrm{d}y),\n where \\ell(\\boldsymbol{\\theta}, y) is a loss function, \\omega is the loss-scale, and F_0 is the unknown true sampling distribution.\n\n\n\nLearning the loss scale \\omega from the data is a delicate task. Assuming a prior for \\omega can lead to degenerate estimates if not accompanied by additional adjustments to the loss function.\nHowever, there are several solutions for its calibration: Holmes and Walker (2017); Lyddon et al. (2019); Syring and Martin (2019); Matsubara et al. (2023)."
  },
  {
    "objectID": "post/abaco26/abaco26_slides.html#generalized-bayes-for-glms",
    "href": "post/abaco26/abaco26_slides.html#generalized-bayes-for-glms",
    "title": "ABACO26",
    "section": "Generalized Bayes for GLMs",
    "text": "Generalized Bayes for GLMs\n\nGeneralized linear models (GLMs) are routinely used to model a wide variety of data.\nThe Bayesian approach for GLMs is also incredibly popular, e.g. because of the possibility of naturally incorporating random effects, complex penalizations, prior information, and more.\nHowever, they often incur misspecification, and this could compromise inferential conclusions.\n\n\n\nA common case is overdispersion, i.e., when proportion or count observations show larger variability than the one assumed by the model.\nTraditional solutions have important drawbacks:\n\nModel-based: may lead to computational bottlenecks and can result again in misspecification.\nNonparametric: increased computational cost and loss of efficiency and interpretability.\n\n\n\n\n\nWe rely on a semi-parametric approach, making only assumptions on the mean and variance of the response while preserving computational tractability."
  },
  {
    "objectID": "post/abaco26/abaco26_slides.html#second-order-assumptions",
    "href": "post/abaco26/abaco26_slides.html#second-order-assumptions",
    "title": "ABACO26",
    "section": "Second order assumptions",
    "text": "Second order assumptions\n\nLet Y_i \\in \\mathcal{Y} denote a response variable, \\boldsymbol{x}_i \\in \\mathbb{R}^p be a vector of covariates for i = 1, \\ldots, n, and \\boldsymbol{\\beta} \\in \\mathbb{R}^p be the parameter of interest.\nStandard GLMs assume that observations y_i are independent realizations of Y_i \\mid \\boldsymbol{x}_i, whose distribution belongs to the exponential dispersion family.\n\n\n\n\n\nWe assume the second-order conditions: \n\\mathbb{E}\\{Y_i\\} = \\mu_i = g^{-1}(\\boldsymbol{x}_i^\\top \\boldsymbol{\\beta}), \\quad\n\\mathrm{var}\\{Y_i\\} = \\psi\\, V(\\mu_i),\n where g(\\cdot) is a link function, V(\\cdot)&gt;0 is a variance function, and \\psi \\in (0,\\infty) is a dispersion parameter.\nWe let (\\boldsymbol{\\beta}_0, \\psi_0) be the true values for the parameters (\\boldsymbol{\\beta}, \\psi) and we assume the data are generated under F_0(\\mathrm{d}y \\mid \\boldsymbol{x}) = F(\\mathrm{d}y \\mid \\boldsymbol{x}, \\boldsymbol{\\beta}_0, \\psi_0).\n\n\n\n\n\n\nAlthough the mean and variance functions need to be carefully chosen to fit the data, the resulting inferences are robust to misspecification of higher-order moments."
  },
  {
    "objectID": "post/abaco26/abaco26_slides.html#quasi-likelihood",
    "href": "post/abaco26/abaco26_slides.html#quasi-likelihood",
    "title": "ABACO26",
    "section": "Quasi-likelihood",
    "text": "Quasi-likelihood\n\nUnder the second-order assumptions, it is possible to specify the so-called log-quasi-likelihood function (Wedderburn 1974): \n\\ell_Q(\\boldsymbol{\\beta}; \\mathbf{y}, \\mathbf{X}, \\psi) = \\sum_{i=1}^n \\ell_Q(\\boldsymbol{\\beta}; y_i, \\boldsymbol{x}_i, \\psi) = \\sum_{i=1}^n \\int_a^{\\mu_i(\\boldsymbol{\\beta})} \\frac{y_i - t}{\\psi V(t)} \\, \\mathrm{d}t,\n where a is an arbitrary constant that does not depend on \\boldsymbol{\\beta}.\nThe above integral can be written in closed form for many choices of variance functions, including those associated with exponential family distributions.\n\n\n\nQuasi-likelihoods retain many properties of genuine likelihoods, such as unbiased estimating equations and the information identity: \n\\mathbb{E}\\left\\{ \\nabla \\ell_Q(\\boldsymbol{\\beta}; \\mathbf{Y}, \\mathbf{X}, \\psi) \\right\\} = 0, \\qquad\n\\mathbb{E}\\left\\{ -\\nabla^2 \\ell_Q(\\boldsymbol{\\beta}; \\mathbf{Y}, \\mathbf{X}, \\psi) \\right\\} =\n\\mathbb{E}\\left\\{ \\nabla \\ell_Q \\nabla \\ell_Q^\\top \\right\\},\n where \\nabla denotes the gradient with respect to \\boldsymbol{\\beta}.\n\n\n\n\nUnder mild conditions, the maximum quasi-likelihood is consistent and has the smallest asymptotic variance among estimators derived from linear estimating equations (McCullagh 1983)."
  },
  {
    "objectID": "post/abaco26/abaco26_slides.html#quasi-posteriors-i",
    "href": "post/abaco26/abaco26_slides.html#quasi-posteriors-i",
    "title": "ABACO26",
    "section": "Quasi-posteriors I",
    "text": "Quasi-posteriors I\n\n\n\nLet \\exp\\{\\ell_Q(\\boldsymbol{\\beta}; \\mathbf{y}, \\mathbf{X}, \\psi)\\} be the quasi-likelihood function and p(\\boldsymbol{\\beta}) be the prior distribution for \\boldsymbol{\\beta}. We define the quasi-posterior distribution for \\boldsymbol{\\beta} as: \np_Q(\\boldsymbol{\\beta} \\mid \\mathbf{y}, \\mathbf{X}, \\psi) \\propto p(\\boldsymbol{\\beta}) \\exp \\left\\{ \\ell_Q(\\boldsymbol{\\beta}; \\mathbf{y}, \\mathbf{X}, \\psi) \\right\\} =\np(\\boldsymbol{\\beta}) \\exp \\left\\{ \\frac{1}{\\psi} \\sum_{i=1}^n \\int_a^{\\mu_i(\\boldsymbol{\\beta})} \\frac{y_i - t}{V(t)} \\, \\mathrm{d}t \\right\\}\n\n\n\n\n\nThe quasi-posterior is a rational update of a belief distribution within the generalized Bayesian framework, with loss function: \n\\ell(\\boldsymbol{\\beta}; \\mathbf{y}, \\mathbf{X}) = - \\sum_{i=1}^n \\int_a^{\\mu_i(\\boldsymbol{\\beta})} \\frac{y_i - t}{V(t)} \\, \\mathrm{d}t.\n\nThe dispersion parameter \\psi plays the role of a loss-scale parameter for the quasi-posterior."
  },
  {
    "objectID": "post/abaco26/abaco26_slides.html#quasi-posteriors-ii",
    "href": "post/abaco26/abaco26_slides.html#quasi-posteriors-ii",
    "title": "ABACO26",
    "section": "Quasi-posteriors II",
    "text": "Quasi-posteriors II\n\nThe quasi-posterior represents subjective uncertainty about the unknown parameter value: \n\\boldsymbol{\\beta}^* = \\arg\\min_{\\boldsymbol{\\beta}} \\int_{\\mathcal{Y}} \\ell(\\boldsymbol{\\beta}; \\mathbf{y}, \\mathbf{X}) \\, F_0(d\\mathbf{y} \\mid \\mathbf{X}),\n which is assumed to be unique (Bissiri et al. 2016).\nThe definition of \\boldsymbol{\\beta}^* does not automatically guarantee that \\boldsymbol{\\beta}^* = \\boldsymbol{\\beta}_0.\n\n\n\n\n\nTheorem (Agnoletto, R., Dunson, 2025)\n\n\nAssume the second-order conditions are well-specified, and suppose the target of inference \\boldsymbol{\\beta}^* is unique. Then, for quasi-posteriors, \\boldsymbol{\\beta}^* must coincide with the true value \\boldsymbol{\\beta}_0: \n\\boldsymbol{\\beta}^* = \\arg\\min_{\\boldsymbol{\\beta}} \\int_{\\mathcal{Y}}\n\\sum_{i=1}^n \\int_a^{\\mu_i(\\boldsymbol{\\beta})} \\frac{t - y_i}{V(t)} \\, \\mathrm{d}t \\, F_0(d\\mathbf{y} \\mid \\mathbf{X}) = \\boldsymbol{\\beta}_0."
  },
  {
    "objectID": "post/abaco26/abaco26_slides.html#calibration-of-the-dispersion-i",
    "href": "post/abaco26/abaco26_slides.html#calibration-of-the-dispersion-i",
    "title": "ABACO26",
    "section": "Calibration of the dispersion I",
    "text": "Calibration of the dispersion I\n\nBased on a comparison with the Bayesian bootstrap, Lyddon et al. (2019) propose calibrate the dispersion \\psi setting it equal to: \n\\psi_{\\text{LLB}} =\n\\frac{\\mathrm{tr}\\{j(\\boldsymbol{\\beta}_0)\\}}{\\mathrm{tr}\\{j(\\boldsymbol{\\beta}_0) h(\\boldsymbol{\\beta}_0)^{-1} j(\\boldsymbol{\\beta}_0)\\}},\n where we define \nj(\\boldsymbol{\\beta}) := \\lim_{n \\to \\infty} \\frac{1}{n} \\mathbb{E}\\left[\\nabla^2 \\ell(\\boldsymbol{\\beta}; \\mathbf{Y}, \\mathbf{X})\\right], \\qquad  h(\\boldsymbol{\\beta}) := \\lim_{n \\to \\infty} \\frac{1}{n} \\mathbb{E}\\left[\\nabla \\ell(\\boldsymbol{\\beta}; \\mathbf{Y}, \\mathbf{X}) \\nabla \\ell(\\boldsymbol{\\beta}; \\mathbf{Y}, \\mathbf{X})^\\top\\right].\n\n\n\n\n\nProposition (Agnoletto, R., Dunson, 2025)\n\n\nUnder the second order conditions, namely if \\mathbb{E}(Y_i) = g^{-1}(\\mathbf{x}_i^\\top \\boldsymbol{\\beta}_0) and \\mathrm{var}(Y_i) = \\psi_0 V\\{\\mu_i(\\boldsymbol{\\beta}_0)\\}, then for quasi posteriors with loss \\ell(\\boldsymbol{\\beta}; \\mathbf{y}, \\mathbf{X}) = -\\sum_{i=1}^n \\int_a^{\\mu_i(\\boldsymbol{\\beta})} \\frac{y_i - t}{V(t)} \\mathrm{d}t, we have \n\\psi_{\\text{LLB}} = \\psi_0."
  },
  {
    "objectID": "post/abaco26/abaco26_slides.html#calibration-of-the-dispersion-ii",
    "href": "post/abaco26/abaco26_slides.html#calibration-of-the-dispersion-ii",
    "title": "ABACO26",
    "section": "Calibration of the dispersion II",
    "text": "Calibration of the dispersion II\n\n\n\nTheorem (Agnoletto, R., Dunson, 2025)\n\n\nAssume the second-order conditions are correctly specified. Let S_1, S_2, \\ldots \\subseteq \\mathbb{R}^p be a sequence of convex credible sets of level \\rho \\in (0,1). Then, under mild conditions and setting \\psi = \\psi_0: \n\\mathbb{P}(\\boldsymbol{\\beta}_0 \\in S_n \\mid \\mathbf{y}, \\mathbf{X}, \\psi_0) \\to \\rho \\quad \\text{as } n \\to \\infty.\n\n\n\n\n\nAs \\psi_0 is typically unknown, we can use the classical method of moments estimator: \n\\widehat{\\psi} = \\frac{1}{n - p} \\sum_{i=1}^n \\frac{(y_i - \\widehat{\\mu}_i)^2}{V(\\widehat{\\mu}_i)},\n where \\widehat{\\mu}_i = \\mu_i(\\widehat{\\boldsymbol{\\beta}}), which is fast and consistent (McCullagh and Nelder 1989)."
  },
  {
    "objectID": "post/abaco26/abaco26_slides.html#simulation-results-i",
    "href": "post/abaco26/abaco26_slides.html#simulation-results-i",
    "title": "ABACO26",
    "section": "Simulation results I",
    "text": "Simulation results I\n\nData are generated from a distribution with \\mathbb{E}(Y_i) = \\mu_i(\\boldsymbol{\\beta}_0)= \\exp(\\mathbf{x}_i^\\top \\boldsymbol{\\beta}_0) and \\mathrm{var}(Y_i) = \\psi_0 \\mu_i(\\boldsymbol{\\beta}_0) —not a Poisson!— with parameters \\beta_0 = (3.5,\\; 1.5,\\; -1.0,\\; 0.5), and \\psi_0 = 3.5.\nWe computed posterior means and 95\\% credible intervals.\nThe sample size is n = 100; estimates are averages over multiple simulated datasets.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPoisson\nNegative Binomial\nDFD-Bayes\nQuasi-posterior\nQuasi-posterior + alternative \\hat{\\psi}\n\n\n\n\n\\beta_1\nMean\n3.50 (0.035)\n3.49 (0.047)\n57.46 (105.41)\n3.50 (0.035)\n3.50 (0.035)\n\n\n\nCover.\n0.715\n0.920\n0.201\n0.945\n0.970\n\n\n\\beta_2\nMean\n1.50 (0.020)\n1.51 (0.040)\n5.26 (6.31)\n1.50 (0.020)\n1.50 (0.020)\n\n\n\nCover.\n0.675\n0.960\n0.454\n0.945\n0.990\n\n\n\\beta_3\nMean\n-1.00 (0.020)\n-1.01 (0.034)\n-3.98 (6.53)\n-1.00 (0.020)\n-1.00 (0.020)\n\n\n\nCover.\n0.715\n0.995\n0.479\n0.950\n0.965\n\n\n\\beta_4\nMean\n0.50 (0.018)\n0.50 (0.037)\n2.55 (7.37)\n0.50 (0.018)\n0.50 (0.018)\n\n\n\nCover.\n0.655\n0.965\n0.526\n0.950\n0.970"
  },
  {
    "objectID": "post/abaco26/abaco26_slides.html#simulation-results-ii",
    "href": "post/abaco26/abaco26_slides.html#simulation-results-ii",
    "title": "ABACO26",
    "section": "Simulation results II",
    "text": "Simulation results II\n\nData are generated from a distribution with \\mathbb{E}(Y_i) = \\mu_i(\\boldsymbol{\\beta}_0)= \\exp(\\mathbf{x}_i^\\top \\boldsymbol{\\beta}_0) and \\mathrm{var}(Y_i) = \\psi_0 \\mu_i(\\boldsymbol{\\beta}_0) —not a Poisson!— with parameters \\beta_0 = (3.5,\\; 1.5,\\; -1.0,\\; 0.5), and \\psi_0 = 3.5.\nWe computed posterior means and 95\\% credible intervals.\nThe sample size is n = 1000; estimates are averages over multiple simulated datasets.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPoisson\nNegative Binomial\nDFD-Bayes\nQuasi-posterior\nQuasi-posterior + alternative \\hat{\\psi}\n\n\n\n\n\\beta_1\nMean\n3.50 (0.010)\n3.50 (0.015)\n4.64 (8.45)\n3.50 (0.010)\n3.50 (0.010)\n\n\n\nCover.\n0.690\n0.835\n0.070\n0.945\n0.955\n\n\n\\beta_2\nMean\n1.50 (0.005)\n1.50 (0.012)\n1.87 (1.37)\n1.50 (0.005)\n1.50 (0.005)\n\n\n\nCover.\n0.665\n0.910\n0.510\n0.950\n0.960\n\n\n\\beta_3\nMean\n-1.00 (0.005)\n-1.00 (0.010)\n-1.21 (0.81)\n-1.00 (0.005)\n-1.00 (0.005)\n\n\n\nCover.\n0.680\n0.960\n0.690\n0.955\n0.950\n\n\n\\beta_4\nMean\n0.50 (0.005)\n0.50 (0.009)\n0.54 (0.35)\n0.50 (0.005)\n0.50 (0.005)\n\n\n\nCover.\n0.715\n0.950\n0.810\n0.950\n0.940"
  },
  {
    "objectID": "post/abaco26/abaco26_slides.html#thank-you",
    "href": "post/abaco26/abaco26_slides.html#thank-you",
    "title": "ABACO26",
    "section": "Thank you!",
    "text": "Thank you!\n\nThe main paper is:\nAgnoletto, D., Rigon, T., and Dunson D.B. (2025+). Nonparametric predictive inference for discrete data via Metropolis-adjusted Dirichlet sequences. arXiv:2507.08629"
  },
  {
    "objectID": "post/abaco26/abaco26_slides.html#references",
    "href": "post/abaco26/abaco26_slides.html#references",
    "title": "ABACO26",
    "section": "References",
    "text": "References\n\n\nAgnoletto, D., Rigon, T., and Dunson, D. B. (2025), “Bayesian inference for generalized linear models via quasi-posteriors,” Biometrika, 112.\n\n\nAldous, D. J. (1985), “Exchangeability and related topics, ecole d’eté de saint-flour XIII, lectures notes n 1117,” Springer Verlag.\n\n\nBerti, P., Dreassi, E., Leisen, F., Pratelli, L., and Rigo, P. (2023b), “Kernel based dirichlet sequences,” Bernoulli, Bernoulli Society for Mathematical Statistics; Probability, 29, 1321–1342.\n\n\nBerti, P., Dreassi, E., Leisen, F., Pratelli, L., and Rigo, P. (2023a), “Bayesian predictive inference without a prior,” Statistica Sinica, Academia Sinica, Institute of Statistical Science, 34, 2405–2429.\n\n\nBerti, P., Dreassi, E., Pratelli, L., and Rigo, P. (2021), “A class of models for Bayesian predictive inference,” Bernoulli, 27, 702–726.\n\n\nBerti, P., Pratelli, L., and Rigo, P. (2004), “Limit theorems for a class of identically distributed random variables,” 32, 2029–2052.\n\n\nBissiri, P. G., Holmes, C. C., and Walker, S. G. (2016), “A general framework for updating belief distributions,” Journal of the Royal Statistical Society: Series B (Statistical Methodology), Wiley Online Library, 78, 1103–1130.\n\n\nCanale, A., and Dunson, D. B. (2011), “Bayesian kernel mixtures for counts,” Journal of the American Statistical Association, Taylor & Francis, 106, 1528–1539.\n\n\nChernozhukov, V., and Hong, H. (2003), “An MCMC approach to classical estimation,” Journal of econometrics, Elsevier, 115, 293–346.\n\n\nCui, F., and Walker, S. G. (2024), “Martingale posterior distributions for log-concave density functions,” arXiv preprint arXiv:2401.14515.\n\n\nDe Finetti, B. (1937), “La prévision: Ses lois logiques, ses sources subjectives,” in Annales de l’institut henri poincaré, pp. 1–68.\n\n\nFong, E., Holmes, C. C., and Walker, S. G. (2023), “Martingale posterior distributions,” Journal of the Royal Statistical Society Series B: Statistical Methodology, 85, 1357–1391.\n\n\nFong, E., and Yiu, A. (2024b), “Bayesian quantile estimation and regression with martingale posteriors,” arXiv preprint arXiv:2406.03358.\n\n\nFong, E., and Yiu, A. (2024a), “Asymptotics for parametric martingale posteriors,” arXiv preprint arXiv:2410.17692.\n\n\nFortini, S., Ladelli, L., and Regazzini, E. (2000), “Exchangeability, predictive distributions and parametric models,” Sankhya: The Indian Journal of Statistics, Series A, 62, 86–109.\n\n\nFortini, S., and Petrone, S. (2012), “Predictive construction of priors in bayesian nonparametrics,” Brazilian Journal of Probability and Statistics, 26, 423–449.\n\n\nFortini, S., and Petrone, S. (2020), “Quasi-Bayes properties of a procedure for sequential learning in mixture models,” Journal of the Royal Statistical Society Series B: Statistical Methodology, Oxford University Press, 82, 1087–1114.\n\n\nFortini, S., and Petrone, S. (2024), “Exchangeability, prediction and predictive modeling in bayesian statistics,” arXiv preprint arXiv:2402.10126.\n\n\nFortini, S., Petrone, S., and Sariev, H. (2021), “Predictive constructions based on measure-valued pólya urn processes,” Mathematics, MDPI, 9, 2845.\n\n\nGrünwald, P. D., and Mehta, N. A. (2020), “Fast rates for general unbounded loss functions: from ERM to generalized Bayes,” The Journal of Machine Learning Research, JMLRORG, 21, 2040–2119.\n\n\nHeide, R. de, Kirichenko, A., Grunwald, P., and Mehta, N. (2020), “Safe-bayesian generalized linear regression,” in Proceedings of the twenty third international conference on artificial intelligence and statistics, PMLR, pp. 2623–2633.\n\n\nHolmes, C. C., and Walker, S. G. (2017), “Assigning a value to a power likelihood in a general Bayesian model,” Biometrika, Oxford University Press, 104, 497–503.\n\n\nJewson, J., and Rossell, D. (2022), “General bayesian loss function selection and the use of improper models,” Journal of the Royal Statistical Society Series B: Statistical Methodology, Oxford University Press, 84, 1640–1665.\n\n\nKnoblauch, J., Jewson, J., and Damoulas, T. (2022), “An optimization-centric view on bayes’ rule: Reviewing and generalizing variational inference,” Journal of Machine Learning Research, 23, 1–109.\n\n\nLyddon, S. P., Holmes, C. C., and Walker, S. G. (2019), “General Bayesian updating and the loss-likelihood bootstrap,” Biometrika, Oxford University Press, 106, 465–478.\n\n\nMartin, R., and Tokdar, S. T. (2009), “Asymptotic properties of predictive recursion: Robustness and rate of convergence,” Electornic Journal of Statistics, 3, 1455–1472.\n\n\nMatsubara, T., Knoblauch, J., Briol, F.-X., and Oates, C. J. (2022), “Robust generalised bayesian inference for intractable likelihoods,” Journal of the Royal Statistical Society Series B: Statistical Methodology, Oxford University Press, 84, 997–1022.\n\n\nMatsubara, T., Knoblauch, J., Briol, F.-X., and Oates, C. J. (2023), “Generalized bayesian inference for discrete intractable likelihood,” Journal of the American Statistical Association, Taylor & Francis, 1–11.\n\n\nRigon, T., Herring, A. H., and Dunson, D. B. (2023), “A generalized Bayes framework for probabilistic clustering,” Biometrika, Oxford University Press, 10, 559–578.\n\n\nSyring, N., and Martin, R. (2019), “Calibrating general posterior credible regions,” Biometrika, Oxford University Press, 106, 479–486."
  },
  {
    "objectID": "post/abaco26/abaco26_slides.html#motivation",
    "href": "post/abaco26/abaco26_slides.html#motivation",
    "title": "ABACO26",
    "section": "Motivation",
    "text": "Motivation\n\nOur contribution: Nonparametric predictive inference for discrete data via Metropolis-adjusted Dirichlet sequences.\nA better estimator for counts distributions would be obtained by replacing \\mathbb{1}(\\cdot) with a kernel that allows the borrowing of information between nearby values.\nBayesian nonparametric modeling of counts distributions is a challenging task.\n\n\n\nEstimating nonparametric mixtures of discrete kernels (Canale and Dunson 2011; Canale and Prünster 2017) can be cumbersome in practice.\n\n\n\n\nAlternatively, one can directly specify a DP prior on the data generator as \n  Y_i\\mid P \\sim P,\\quad P\\sim\\mathrm{DP}(\\alpha,P_0),\n for Y_i\\in\\mathcal{Y}=\\{0,1,\\ldots\\}, i=1,\\ldots,n, where \\alpha is the precision parameter and P_0 a base parametric distribution, such as a Poisson.\n\n\n\n\nHowever, the corresponding posterior lacks smoothing, leading to poor performance.\n\n\n\n\nWithin the hypothetical approach, it is not clear how to specify a nonparametric process having the same simplicity and flexibility as the DP prior while allowing smoothing."
  },
  {
    "objectID": "post/abaco26/abaco26_slides.html#predictive-approach",
    "href": "post/abaco26/abaco26_slides.html#predictive-approach",
    "title": "ABACO26",
    "section": "Predictive approach",
    "text": "Predictive approach\n\nTaking a predictive approach, one can implicitly characterize the prior via de Finetti theorem by specifying the sequence of predictive distributions of an exchangeable sequence: \nP_n(A) := \\mathbb{P}(Y_{n+1}\\in A\\mid y_{1:n}), \\qquad n \\ge 1.\n This leads to an exchangeable sequences iff the conditions in Fortini et al. (2000) are satisfied.\n\n\n\nExample: the predictive construction of a Dirichlet process prior is such that Y_1\\sim P_0 and Y_{n+1}\\mid y_{1:n}\\sim P_n for n\\ge 1, where \nP_n(A) = \\frac{\\alpha}{\\alpha+n} P_0(A) + \\frac{1}{\\alpha+n}\\sum_{i=1}^n\\mathbb{1}(y_i\\in A),\n for any measurable set A.\n\n\n\n\nThe possibility of specifying a sequence of one-step-ahead predictive distributions is appealing:\n\nit bypasses direct elicitation of the prior;\nit explicitly connects prediction and inference (see the next slide);"
  },
  {
    "objectID": "post/abaco26/abaco26_slides.html#generalizations-of-the-predictive-approach",
    "href": "post/abaco26/abaco26_slides.html#generalizations-of-the-predictive-approach",
    "title": "ABACO26",
    "section": "Generalizations of the predictive approach",
    "text": "Generalizations of the predictive approach\n\nDefining a sequence of predictive laws P_n that guarantees exchangeability—i.e., satisfies the two-step-ahead conditions of Fortini et al. (2000)—is a difficult task in practice.\n\n\n\nSolution: replace exchangeability with the weaker requirement that (Y_n)_{n\\ge1} is conditionally identically distributed (CID), also known as martingale posteriors.\nThe CID condition requires \n\\mathbb{P}(Y_{n+k} \\in \\cdot \\mid y_{1:n}) = \\mathbb{P}(Y_{n+1} \\in \\cdot \\mid y_{1:n}) = P_n(\\cdot), \\qquad \\text{for all} \\quad  k \\ge 1, \\; n \\ge 1.\n It is sufficient to verify this condition for k = 1 in order to ensure its validity for all k  \\ge 1.\nEquivalently, Fong et al. (2023) express the above condition in a way that emphasizes the martingale property of the predictive distributions \n\\mathbb{E}\\{P_{n+1}(\\cdot) \\mid y_{1:n}\\} = P_n(\\cdot), \\qquad n \\ge 1.\n\n\n\n\n\nThis is another lively recent topic: Fortini and Petrone (2020); Berti et al. (2021); Fortini et al. (2021); Berti et al. (2023a); Berti et al. (2023b); Fong et al. (2023); Fong and Yiu (2024a); Fong and Yiu (2024b); Cui and Walker (2024); Fortini and Petrone (2024) and more…"
  },
  {
    "objectID": "post/abaco26/abaco26_slides.html#metropolis-adjusted-dirichlet-mad-sequences",
    "href": "post/abaco26/abaco26_slides.html#metropolis-adjusted-dirichlet-mad-sequences",
    "title": "ABACO26",
    "section": "Metropolis-adjusted Dirichlet (MAD) sequences",
    "text": "Metropolis-adjusted Dirichlet (MAD) sequences\n\nWe assume that K_{n}(\\cdot\\mid y_n) is a Metropolis-Hastings kernel centered in y_n having pmf: \n  k_{n}(y\\mid y_n) = \\gamma_{n}(y,y_n) k_*(y\\mid y_n) + \\mathbb{1}(y=y_n)\\Big[\\sum_{z\\in\\mathcal{Y}}\\big\\{1-\\gamma_{n}(z,y_n)\\big\\}k_*(z\\mid y_n)\\Big],\n with acceptance probability \n  \\gamma_{n}(y,y_n) = \\gamma(y, y_n, P_{n-1}) =\n  \\min\\left\\{1,\\frac{p_{n-1}(y) k_*(y_n\\mid y)}{p_{n-1}(y_n) k_*(y\\mid y_n)}\\right\\},\n where p_{n-1} is the probability mass functions associated to P_{n-1} and k_*(\\cdot\\mid y_n) is the pmf of a discrete base kernel centered at y_n.\nWe refer to P_n above as the Metropolis-adjusted Dirichlet (MAD) distribution with weights (w_n)_{n\\ge1}, base kernel k_* and initial distribution P_0. We call (Y_n)_{n \\ge 1} a MAD sequence.\n\n\n\n\n\n\nTheorem (Agnoletto, R. and Dunson, 2025)\n\n\nLet (Y_n)_{n\\ge1} be a MAD sequence. Then, for every set of weights (w_n)_{n \\ge 1}, discrete base kernel k_*, and initial distribution P_0, the sequence (Y_n)_{n\\ge1} is conditionally identically distributed (CID)."
  },
  {
    "objectID": "post/abaco26/abaco26_slides.html#mad-sequences-are-cid",
    "href": "post/abaco26/abaco26_slides.html#mad-sequences-are-cid",
    "title": "ABACO26",
    "section": "MAD sequences are cid",
    "text": "MAD sequences are cid\n\nK_{n}(\\cdot\\mid y_n) denotes a Metropolis-Hastings kernel centered in y_n having pmf: \n  k_{n}(y\\mid y_n) = \\gamma_{n}(y,y_n) k_*(y\\mid y_n) + \\mathbb{1}(y=y_n)\\Big[\\sum_{z\\in\\mathcal{Y}}\\big\\{1-\\gamma_{n}(z,y_n)\\big\\}k_*(z\\mid y_n)\\Big],\n where \n  \\gamma_{n}(y,y_n) = \\gamma(y, y_n, P_{n-1}) =\n  \\min\\left\\{1,\\frac{p_{n-1}(y) k_*(y_n\\mid y)}{p_{n-1}(y_n) k_*(y\\mid y_n)}\\right\\},\n p_{n-1} is the probability mass functions associated to P_{n-1} and k_*(\\cdot\\mid y_n) is a discrete base kernel centered in y_n.\n\n\n\nWe refer to P_n as the Metropolis-adjusted Dirichlet (MAD) distribution with weights (w_n)_{n\\ge1}, base kernel k_* and initial distribution P_0. The corresponding sequence (Y_n)_{n\\ge1} is a MAD sequence.\n\n\n\n\n\nTheorem\n\n\nLet (Y_n)_{n\\ge1} be a MAD sequence. Then, for every set of weights (w_n)_{n \\ge 1}, discrete base kernel k_*, and initial distribution P_0, the sequence (Y_n)_{n\\ge1} is conditionally identically distributed.\n\n\n\n\n\nA sequence (Y_n)_{n\\ge1} is conditionally identically distributed (cid) if, \\mathbb{P}-a.s., \n  \\mathbb{P}\\{Y_{n+k}\\in\\cdot\\mid y_{1:n}\\} = \\mathbb{P}\\{Y_{n+1}\\in\\cdot\\mid y_{1:n}\\}=P_n(\\cdot)\n  \\qquad k\\ge1.\n\nThis is equivalent to the martingale condition \n  \\mathbb{E}\\{P_{n+1}(\\cdot)\\mid y_{1:n}\\} = P_{n}(\\cdot)."
  },
  {
    "objectID": "post/abaco26/abaco26_slides.html#bayesian-properties-of-cid-sequences",
    "href": "post/abaco26/abaco26_slides.html#bayesian-properties-of-cid-sequences",
    "title": "ABACO26",
    "section": "Bayesian properties of CID sequences",
    "text": "Bayesian properties of CID sequences\n\n\n\n\nCorollary (Aldous 1985; Berti et al. 2004)\n\n\nConsider a MAD sequence (Y_n)_{n\\ge1}. Then, \\mathbb{P}-a.s.,\n(a) The sequence is asymptotically exchangeable, that is \n    (Y_{n+1}, Y_{n+2}, \\ldots) \\overset{\\textup{d}}{\\longrightarrow} (Z_1, Z_2, \\ldots), \\qquad n \\rightarrow \\infty,\n where (Z_1,Z_2,\\ldots) is an exchangeable sequence with directing random probability measure P;\n(b) the corresponding sequence of predictive distributions (P_n)_{n\\ge1} weakly converge to a random probability measures P (a.s. \\mathbb{P}).\n\n\n\n\n\nThe ordering dependence will vanish asymptotically and, informally, Y_i \\mid P \\overset{\\mathrm{iid}}{\\sim} P for large n.\nThe random probability measure P exists and is defined as the limit of the predictive distributions."
  },
  {
    "objectID": "post/abaco26/abaco26_slides.html#predictive-inference",
    "href": "post/abaco26/abaco26_slides.html#predictive-inference",
    "title": "ABACO26",
    "section": "Predictive inference",
    "text": "Predictive inference\n\nThus, an asymptotic equivalent of de Finetti’s theorem holds: each MAD sequence has a corresponding unique prior on P.\nThe prior and posterior on P exists but are not available explicitly.\n\n\n\n\n\n\nCorollary\n\n\nLet \\theta = P(f) = \\sum_{y \\in \\mathcal{Y}} f(y) p(y) and analogously \\theta_n = P_n(f) = \\sum_{y \\in \\mathcal{Y}} f(y) p_n(y) be any functional of interest. Consider a MAD sequence (Y_n)_{n\\ge1}. Then, \\mathbb{P}-a.s., for every n\\ge1 and every integrable function f:\\mathcal{Y}\\rightarrow\\mathbb{R}, we have \n\\mathbb{E}(\\theta \\mid y_{1:n}) = \\mathbb{E}\\{P(f) \\mid y_{1:n}\\} = P_n(f) = \\theta_n\n\n\n\n\n\n\nMoreover, we also have that \\theta = \\mathbb{E}\\{P(f)\\}=P_0(f) = \\theta_0 for every integrable function f, so that P_0 retains the role of a base measure as for standard Dirichlet sequences, providing an initial guess at P.\n\n\n\n\nUncertainty quantification for \\theta=P(f) is carried out by predictive resampling (Fong et al. 2023)."
  },
  {
    "objectID": "post/abaco26/abaco26_slides.html#illustrative-example",
    "href": "post/abaco26/abaco26_slides.html#illustrative-example",
    "title": "ABACO26",
    "section": "Illustrative example",
    "text": "Illustrative example\n\n\n\n\n\n\nLeft plot: posterior mean of a DP. Right plot: posterior mean of the proposed MAD sequence."
  },
  {
    "objectID": "post/abaco26/abaco26_slides.html#on-the-choice-of-the-base-kernel",
    "href": "post/abaco26/abaco26_slides.html#on-the-choice-of-the-base-kernel",
    "title": "ABACO26",
    "section": "On the choice of the base kernel",
    "text": "On the choice of the base kernel\n\nIn principle, any discrete distribution can be chosen as the base kernel k_*. However, it is natural to consider choices that allow the kernel to be centered at y_n while permitting control over the variance.\n\n\n\nWe consider a rounded Gaussian distribution centered in y_n, with pmf \n  k_*(y\\mid y_n, \\sigma) = \\frac{\\int_{y-1/2}^{y+1/2}\\mathcal{N}(t\\mid y_n, \\sigma^2) \\mathrm{d}t}{\\sum_{z\\in\\mathcal{Y}} \\int_{z-1/2}^{z+1/2}\\mathcal{N}(t\\mid y_n, \\sigma^2) \\mathrm{d}t},\n for n\\ge1, where \\mathcal{N}(\\cdot\\mid y_n,\\sigma^2) denotes a normal density function with mean y_n and variance \\sigma^2."
  },
  {
    "objectID": "post/abaco26/abaco26_slides.html#role-of-the-weights-in-controlling-posterior-variability",
    "href": "post/abaco26/abaco26_slides.html#role-of-the-weights-in-controlling-posterior-variability",
    "title": "ABACO26",
    "section": "Role of the weights in controlling posterior variability",
    "text": "Role of the weights in controlling posterior variability\n\nIt can be shown that the distribution of P(A)\\mid y_{1:n} is approximated by \\mathcal{N}(P_n(A), \\Sigma_n r_n^{-1}) for n large, where the variance is \n  \\Sigma_n r_n^{-1}\n  \\approx \\mathbb{E}\\{[P_{n+1}(A)-P_n(A)]^2\\mid y_{1:n}\\}\\sum_{k&gt;n+1}w_k^2.\n\n\n\n\nWeights that decay to zero quickly induce fast learning and convergence to the asymptotic exchangeable regime.\nBut small values of w_n leads to poor learning and underestimation of the posterior variability.\n\n\n\n\nPossible choices are w_n=(\\alpha+n)^{-1}, w_n=(\\alpha+n)^{-2/3} (Martin and Tokdar 2009), and w_n=(2 - n^{-1})(n+1)^{-1} (Fong et al. 2023).\nWe consider adaptive weights \n  w_n=(\\alpha+n)^{-\\lambda_n}, \\qquad \\lambda_n=\\lambda+(1+\\lambda)\\exp\\bigg\\{-\\frac{1}{N_*}n\\bigg\\},\n with \\lambda\\in(0.5,1], N_*&gt;0."
  },
  {
    "objectID": "post/abaco26/abaco26_slides.html#multivariate-count-and-binary-data",
    "href": "post/abaco26/abaco26_slides.html#multivariate-count-and-binary-data",
    "title": "ABACO26",
    "section": "Multivariate count and binary data",
    "text": "Multivariate count and binary data\n\nExtending MAD sequences for multivariate data is straightforward using a factorized base kernel \nk_*(\\bm y\\mid\\bm y_n) = \\prod_{j=1}^d k_*(y_j\\mid y_{n,j}),\n with \\bm y=(y_1,\\ldots,y_d) and \\bm y_n=(y_{n,1},\\ldots,y_{n,d}).\n\n\n\nMAD sequences can be employed for modeling multivariate binary data using an appropriate base kernel.\n\n\n\n\nA natural step further is to use MAD sequences for nonparametric regression and classification."
  },
  {
    "objectID": "post/abaco26/abaco26_slides.html#simulations-i",
    "href": "post/abaco26/abaco26_slides.html#simulations-i",
    "title": "ABACO26",
    "section": "Simulations I",
    "text": "Simulations I\nOut-of-sample prediction accuracy evaluated in terms of MSE and AUC for regression and classification, respectively.\n\n\n\n\n\n\n\n\n\n\n\n\nRegression (MSE)\n\n\nClassification (AUC)\n\n\n\n\n\n\nn=40\nn=80\n\nn=150\nn=300\n\n\nGLM\n120.77 [51.51]\n94.93 [8.37]\n\n0.796 [0.014]\n0.809 [0.007]\n\n\nBART\n101.17 [12.69]\n74.17 [10.00]\n\n0.863 [0.026]\n0.932 [0.009]\n\n\nRF\n99.98 [7.45]\n87.75 [6.53]\n\n0.882 [0.025]\n0.913 [0.015]\n\n\nDP\n1450.21 [5.53]\n1395.61 [8.72]\n\n0.644 [0.011]\n0.724 [0.012]\n\n\nMAD-1\n91.07 [10.35]\n73.96 [7.60]\n\n0.873 [0.014]\n0.899 [0.008]\n\n\nMAD-2/3\n88.83 [13.00]\n73.18 [9.58]\n\n0.869 [0.015]\n0.899 [0.009]\n\n\nMAD-dpm\n87.41 [12.36]\n72.07 [9.48]\n\n0.872 [0.014]\n0.901 [0.008]\n\n\nMAD-ada\n90.61 [10.28]\n73.45 [7.69]\n\n0.874 [0.014]\n0.900 [0.008]"
  },
  {
    "objectID": "post/abaco26/abaco26_slides.html#simulations-ii",
    "href": "post/abaco26/abaco26_slides.html#simulations-ii",
    "title": "ABACO26",
    "section": "Simulations II",
    "text": "Simulations II"
  },
  {
    "objectID": "post/abaco26/abaco26_slides.html#application",
    "href": "post/abaco26/abaco26_slides.html#application",
    "title": "ABACO26",
    "section": "Application",
    "text": "Application\n\nWe analyze the occurrence rates of 4 species corvids in Finland in year 2009 across different temperatures and habitats."
  },
  {
    "objectID": "post/abaco26/abaco26_slides.html#application-ii",
    "href": "post/abaco26/abaco26_slides.html#application-ii",
    "title": "ABACO26",
    "section": "Application II",
    "text": "Application II"
  },
  {
    "objectID": "post/abaco26/abaco26_slides.html#thank-you-1",
    "href": "post/abaco26/abaco26_slides.html#thank-you-1",
    "title": "ABACO26",
    "section": "Thank you!",
    "text": "Thank you!\n\nThe main paper is:\nAgnoletto, D., Rigon, T., and Dunson D.B. (2025+). Bayesian inference for generalized linear models via quasi-posteriors. Biometrika, to appear."
  },
  {
    "objectID": "post/abaco26/abaco26_slides.html#references-1",
    "href": "post/abaco26/abaco26_slides.html#references-1",
    "title": "ABACO26",
    "section": "References",
    "text": "References\n\n\nBerti, P., Dreassi, E., Leisen, F., Pratelli, L., and Rigo, P. (2023a), “Bayesian predictive inference without a prior,” Statistica Sinica, Academia Sinica, Institute of Statistical Science, 34, 2405–2429.\n\n\nBerti, P., Dreassi, E., Leisen, F., Pratelli, L., and Rigo, P. (2023b), “Kernel based Dirichlet sequences,” Bernoulli, Bernoulli Society for Mathematical Statistics; Probability, 29, 1321–1342.\n\n\nBerti, P., Dreassi, E., Pratelli, L., and Rigo, P. (2021), “A class of models for Bayesian predictive inference,” Bernoulli, 27, 702–726.\n\n\nBissiri, P. G., Holmes, C. C., and Walker, S. G. (2016), “A general framework for updating belief distributions,” Journal of the Royal Statistical Society: Series B (Statistical Methodology), Wiley Online Library, 78, 1103–1130.\n\n\nCanale, A., and Dunson, D. B. (2011), “Bayesian kernel mixtures for counts,” Journal of the American Statistical Association, Taylor & Francis, 106, 1528–1539.\n\n\nCanale, A., and Prünster, I. (2017), “Robustifying Bayesian nonparametric mixtures for count data,” Biometrics, 73, 174–184.\n\n\nChernozhukov, V., and Hong, H. (2003), “An MCMC approach to classical estimation,” Journal of econometrics, Elsevier, 115, 293–346.\n\n\nCui, F., and Walker, S. G. (2024), “Martingale posterior distributions for log-concave density functions,” arXiv preprint arXiv:2401.14515.\n\n\nFinetti, B. de (1937), “La prévision: Ses lois logiques, ses sources subjectives,” in Annales de l’institut henri poincaré, pp. 1–68.\n\n\nFong, E., Holmes, C. C., and Walker, S. G. (2023), “Martingale posterior distributions,” Journal of the Royal Statistical Society Series B: Statistical Methodology, 85, 1357–1391.\n\n\nFong, E., and Yiu, A. (2024b), “Bayesian quantile estimation and regression with martingale posteriors,” arXiv preprint arXiv:2406.03358.\n\n\nFong, E., and Yiu, A. (2024a), “Asymptotics for parametric martingale posteriors,” arXiv preprint arXiv:2410.17692.\n\n\nFortini, S., and Petrone, S. (2020), “Quasi-Bayes properties of a procedure for sequential learning in mixture models,” Journal of the Royal Statistical Society Series B: Statistical Methodology, Oxford University Press, 82, 1087–1114.\n\n\nFortini, S., and Petrone, S. (2025), “Exchangeability, prediction and predictive modeling in bayesian statistics,” Statistical Science, Institute of Mathematical Statistics, 40, 40–67.\n\n\nFortini, S., Petrone, S., and Sariev, H. (2021), “Predictive constructions based on measure-valued Pólya urn processes,” Mathematics, MDPI, 9, 2845.\n\n\nGrünwald, P. D., and Mehta, N. A. (2020), “Fast rates for general unbounded loss functions: from ERM to generalized Bayes,” The Journal of Machine Learning Research, JMLRORG, 21, 2040–2119.\n\n\nHeide, R. de, Kirichenko, A., Grunwald, P., and Mehta, N. (2020), “Safe-bayesian generalized linear regression,” in Proceedings of the twenty third international conference on artificial intelligence and statistics, PMLR, pp. 2623–2633.\n\n\nHolmes, C. C., and Walker, S. G. (2017), “Assigning a value to a power likelihood in a general Bayesian model,” Biometrika, Oxford University Press, 104, 497–503.\n\n\nJewson, J., and Rossell, D. (2022), “General bayesian loss function selection and the use of improper models,” Journal of the Royal Statistical Society Series B: Statistical Methodology, Oxford University Press, 84, 1640–1665.\n\n\nKnoblauch, J., Jewson, J., and Damoulas, T. (2022), “An optimization-centric view on bayes’ rule: Reviewing and generalizing variational inference,” Journal of Machine Learning Research, 23, 1–109.\n\n\nLyddon, S. P., Holmes, C. C., and Walker, S. G. (2019), “General Bayesian updating and the loss-likelihood bootstrap,” Biometrika, Oxford University Press, 106, 465–478.\n\n\nMartin, R., and Tokdar, S. T. (2009), “Asymptotic properties of predictive recursion: Robustness and rate of convergence,” Electornic Journal of Statistics, 3, 1455–1472.\n\n\nMatsubara, T., Knoblauch, J., Briol, F.-X., and Oates, C. J. (2022), “Robust generalised bayesian inference for intractable likelihoods,” Journal of the Royal Statistical Society Series B: Statistical Methodology, Oxford University Press, 84, 997–1022.\n\n\nMatsubara, T., Knoblauch, J., Briol, F.-X., and Oates, C. J. (2023), “Generalized bayesian inference for discrete intractable likelihood,” Journal of the American Statistical Association, Taylor & Francis, 1–11.\n\n\nRigon, T., Herring, A. H., and Dunson, D. B. (2023), “A generalized Bayes framework for probabilistic clustering,” Biometrika, Oxford University Press, 10, 559–578.\n\n\nSyring, N., and Martin, R. (2019), “Calibrating general posterior credible regions,” Biometrika, Oxford University Press, 106, 479–486."
  },
  {
    "objectID": "post/abaco26/abaco26_slides.html#appendix",
    "href": "post/abaco26/abaco26_slides.html#appendix",
    "title": "ABACO26",
    "section": "Appendix",
    "text": "Appendix"
  },
  {
    "objectID": "post/abaco26/abaco26_slides.html#generalization-of-the-predictive-approach",
    "href": "post/abaco26/abaco26_slides.html#generalization-of-the-predictive-approach",
    "title": "ABACO26",
    "section": "Generalization of the predictive approach",
    "text": "Generalization of the predictive approach"
  },
  {
    "objectID": "post/abaco26/abaco26_slides.html#foundations-1",
    "href": "post/abaco26/abaco26_slides.html#foundations-1",
    "title": "ABACO26",
    "section": "Foundations",
    "text": "Foundations\n\nDe Finetti’s representation Theorem (De Finetti 1937) has a central role in Bayesian statistics because it provides the fundamental justification to the two approaches to Bayesian statistics:\n\nthe hypothetical approach;\nthe predictive approach.\n\n\n\n\n\n\nDe Finetti’s representation theorem\n\n\nLet (Y_n)_{n\\ge 1}, Y_n\\in\\mathcal{Y}, be a sequence of exchangeable random variables with probability law P. Then there exists a unique probability measure \\Pi such that, for any n\\ge 1,\n\nP(y_1,\\ldots,y_n) = \\int_{\\mathcal{F}} \\prod_{i=1}^n F(y_i)\\,\\Pi(\\mathrm{d}F).\n Furthermore the sequence of the empirical distributions (F_n)_n converges weakly to a random distribution function \\tilde{P} distributed according to \\Pi."
  },
  {
    "objectID": "post/abaco26/abaco26_slides.html#empirical-consistency",
    "href": "post/abaco26/abaco26_slides.html#empirical-consistency",
    "title": "ABACO26",
    "section": "Empirical consistency",
    "text": "Empirical consistency\n\nIllustrative example. Hellinger distance of MAD-ada (red), MAD-1 (blue), and DP (black) predictive distributions with respect to the density of the true data generator."
  },
  {
    "objectID": "post/abaco26/abaco26_slides.html#predictive-resampling-for-mad-sequences",
    "href": "post/abaco26/abaco26_slides.html#predictive-resampling-for-mad-sequences",
    "title": "ABACO26",
    "section": "Predictive resampling for MAD sequences",
    "text": "Predictive resampling for MAD sequences\nAlgorithm (Fortini and Petrone 2020):\n\nCompute P_n(\\cdot) from the observed data y_{1:n}\nSet N\\gg n\nFor j = 1,\\ldots,B\n\nFor i=n+1,\\ldots,N\n\nSample Y_i\\mid y_{1:i-1}\\sim P_{i-1}\nUpdate P_i(\\cdot) = (1-w_i)P_{i-1}(\\cdot) + w_i K_{i-1}(\\cdot\\mid y_i)\n\nEnd For\n\nEnd For\nReturn P_N^{(1)}(\\cdot),\\ldots,P_N^{(B)}(\\cdot), an iid sample from the distribution of P_N(\\cdot)\\mid y_{1:n}"
  },
  {
    "objectID": "post/abaco26/abaco26_slides.html#on-the-choice-of-the-base-kernel-1",
    "href": "post/abaco26/abaco26_slides.html#on-the-choice-of-the-base-kernel-1",
    "title": "ABACO26",
    "section": "On the choice of the base kernel",
    "text": "On the choice of the base kernel\n\n\n\\mathrm{var}\\{P_{n+1}(y)\\mid y_{1:n}\\}, \\text{ with } y\\in\\{20,40,60,80\\}\n \n\\mathrm{cor}\\{P_{n+1}(40),P_{n+1}(y)\\mid y_{1:n}\\}, \\text{ with } y\\in\\{10, 41, 80\\}"
  },
  {
    "objectID": "post/abaco26/abaco26_slides.html#asymptotic-posterior-distributions-i",
    "href": "post/abaco26/abaco26_slides.html#asymptotic-posterior-distributions-i",
    "title": "ABACO26",
    "section": "Asymptotic posterior distributions I",
    "text": "Asymptotic posterior distributions I\n\n\n\n\nTheorem\n\n\nLet (Y_n)_{n\\ge1} be a MAD sequence and let (r_n)_{n\\ge1} be a monotone sequence of positive numbers such that r_n\\overset{\\cdot}{=}(\\sum_{k&gt;n}w_k^2)^{-1} as n\\rightarrow\\infty. Assume \\sqrt{r_n}\\sup_{k\\ge n}w_k\\rightarrow 0, \\sum_{k\\ge1}r_k^2w_{k+1}^4&lt;\\infty, and that, for every H\\ge1 and all measurable sets A_1,\\ldots,A_H, the random matrix \\bm\\Sigma = \\bm\\Sigma(A_1,\\ldots,A_H) = [\\Sigma_{jt}] is positive definite. Then, for every H\\ge1 and every A_1,\\ldots,A_H, \n    \\sqrt{r_n}\\left[\\begin{array}{c}\n     P(A_1)-P_n(A_1)\\\\\n     \\cdots\\\\\n     P(A_H)-P_n(A_H)\n    \\end{array}\\right] \\Big|\\;\nY_{1:n}\n    \\overset{d}{\\longrightarrow} \\mathcal{N}_H(\\bm0,\\bm\\Sigma)\n \\mathbb{P}-a.s. for n\\rightarrow\\infty."
  },
  {
    "objectID": "post/abaco26/abaco26_slides.html#asymptotic-posterior-distributions-ii",
    "href": "post/abaco26/abaco26_slides.html#asymptotic-posterior-distributions-ii",
    "title": "ABACO26",
    "section": "Asymptotic posterior distributions II",
    "text": "Asymptotic posterior distributions II\n\n\n\n\nTheorem\n\n\nFor every n\\ge1 and all measurable sets A_1,\\ldots,A_H, let \\bm\\Sigma_n=\\bm\\Sigma_n(A_1,\\ldots,A_H) = [\\Sigma_{n,jt}]. Then, under the assumptions of previous Theorem, \\mathbb{P}-a.s., \\bm\\Sigma_n\\rightarrow\\bm\\Sigma and \n    \\sqrt{r_n}\\bm\\Sigma_n^{-1/2}\\left[\\begin{array}{c}\n     P(A_1)-P_n(A_1)\\\\\n     \\cdots\\\\\n     P(A_H)-P_n(A_H)\n    \\end{array}\\right] \\Big|\\;\ny_{1:n}\n    \\overset{d}{\\longrightarrow} \\mathcal{N}_H(\\bm0,\\bm I)\n for n\\rightarrow\\infty, where \\bm I denotes the identity matrix."
  },
  {
    "objectID": "post/abaco26/abaco26.html#predictive-approach",
    "href": "post/abaco26/abaco26.html#predictive-approach",
    "title": "ABACO26",
    "section": "Predictive approach",
    "text": "Predictive approach\n\nTaking a predictive approach, one can implicitly characterize the prior via de Finetti theorem by specifying the sequence of predictive distributions of an exchangeable sequence: \nP_n(A) := \\mathbb{P}(Y_{n+1}\\in A\\mid y_{1:n}), \\qquad n \\ge 1.\n This leads to an exchangeable sequences iff the conditions in Fortini et al. (2000) are satisfied.\n\n\nExample: the predictive construction of a Dirichlet process prior is such that Y_1\\sim P_0 and Y_{n+1}\\mid y_{1:n}\\sim P_n for n\\ge 1, where \nP_n(A) = \\frac{\\alpha}{\\alpha+n} P_0(A) + \\frac{1}{\\alpha+n}\\sum_{i=1}^n\\mathbb{1}(y_i\\in A),\n for any measurable set A.\n\n\nThe possibility of specifying a sequence of one-step-ahead predictive distributions is appealing:\n\nit bypasses direct elicitation of the prior;\nit explicitly connects prediction and inference (see the next slide);"
  },
  {
    "objectID": "post/abaco26/abaco26.html#generalizations-of-the-predictive-approach",
    "href": "post/abaco26/abaco26.html#generalizations-of-the-predictive-approach",
    "title": "ABACO26",
    "section": "Generalizations of the predictive approach",
    "text": "Generalizations of the predictive approach\n\nDefining a sequence of predictive laws P_n that guarantees exchangeability—i.e., satisfies the two-step-ahead conditions of Fortini et al. (2000)—is a difficult task in practice.\n\n\nSolution: replace exchangeability with the weaker requirement that (Y_n)_{n\\ge1} is conditionally identically distributed (CID), also known as martingale posteriors.\nThe CID condition requires \n\\mathbb{P}(Y_{n+k} \\in \\cdot \\mid y_{1:n}) = \\mathbb{P}(Y_{n+1} \\in \\cdot \\mid y_{1:n}) = P_n(\\cdot), \\qquad \\text{for all} \\quad  k \\ge 1, \\; n \\ge 1.\n It is sufficient to verify this condition for k = 1 in order to ensure its validity for all k  \\ge 1.\nEquivalently, Fong et al. (2023) express the above condition in a way that emphasizes the martingale property of the predictive distributions \n\\mathbb{E}\\{P_{n+1}(\\cdot) \\mid y_{1:n}\\} = P_n(\\cdot), \\qquad n \\ge 1.\n\n\n\nThis is another lively recent topic: Fortini and Petrone (2020); Berti et al. (2021); Fortini et al. (2021); Berti et al. (2023a); Berti et al. (2023b); Fong et al. (2023); Fong and Yiu (2024a); Fong and Yiu (2024b); Cui and Walker (2024); Fortini and Petrone (2024) and more…"
  },
  {
    "objectID": "post/abaco26/abaco26.html#motivation",
    "href": "post/abaco26/abaco26.html#motivation",
    "title": "ABACO26",
    "section": "Motivation",
    "text": "Motivation\n\nA better estimator for counts distributions would be obtained by replacing \\mathbb{1}(\\cdot) with a kernel that allows the borrowing of information between nearby values.\nBayesian nonparametric modeling of counts distributions is a challenging task.\n\n\nEstimating nonparametric mixtures of discrete kernels (Canale and Dunson 2011; Canale and Prünster 2017) can be cumbersome in practice.\n\n\nAlternatively, one can directly specify a DP prior on the data generator as \n  Y_i\\mid P \\sim P,\\quad P\\sim\\mathrm{DP}(\\alpha,P_0),\n for Y_i\\in\\mathcal{Y}=\\{0,1,\\ldots\\}, i=1,\\ldots,n, where \\alpha is the precision parameter and P_0 a base parametric distribution, such as a Poisson.\n\n\nHowever, the corresponding posterior lacks smoothing, leading to poor performance.\n\n\nWithin the hypothetical approach, it is not clear how to specify a nonparametric process having the same simplicity and flexibility as the DP prior while allowing smoothing."
  },
  {
    "objectID": "post/abaco26/abaco26.html#metropolis-adjusted-dirichlet-mad-sequences",
    "href": "post/abaco26/abaco26.html#metropolis-adjusted-dirichlet-mad-sequences",
    "title": "ABACO26",
    "section": "Metropolis-adjusted Dirichlet (MAD) sequences",
    "text": "Metropolis-adjusted Dirichlet (MAD) sequences\n\nWe assume that K_{n}(\\cdot\\mid y_n) is a Metropolis-Hastings kernel centered in y_n having pmf: \n  k_{n}(y\\mid y_n) = \\gamma_{n}(y,y_n) k_*(y\\mid y_n) + \\mathbb{1}(y=y_n)\\Big[\\sum_{z\\in\\mathcal{Y}}\\big\\{1-\\gamma_{n}(z,y_n)\\big\\}k_*(z\\mid y_n)\\Big],\n with acceptance probability \n  \\gamma_{n}(y,y_n) = \\gamma(y, y_n, P_{n-1}) =\n  \\min\\left\\{1,\\frac{p_{n-1}(y) k_*(y_n\\mid y)}{p_{n-1}(y_n) k_*(y\\mid y_n)}\\right\\},\n where p_{n-1} is the probability mass functions associated to P_{n-1} and k_*(\\cdot\\mid y_n) is the pmf of a discrete base kernel centered at y_n.\nWe refer to P_n above as the Metropolis-adjusted Dirichlet (MAD) distribution with weights (w_n)_{n\\ge1}, base kernel k_* and initial distribution P_0. We call (Y_n)_{n \\ge 1} a MAD sequence.\n\n\n\n\n\n\n\nTheorem (Agnoletto, R. and Dunson, 2025)\n\n\n\nLet (Y_n)_{n\\ge1} be a MAD sequence. Then, for every set of weights (w_n)_{n \\ge 1}, discrete base kernel k_*, and initial distribution P_0, the sequence (Y_n)_{n\\ge1} is conditionally identically distributed (CID)."
  },
  {
    "objectID": "post/abaco26/abaco26.html#mad-sequences-are-cid",
    "href": "post/abaco26/abaco26.html#mad-sequences-are-cid",
    "title": "ABACO26",
    "section": "MAD sequences are cid",
    "text": "MAD sequences are cid\n\nIf w_n = (\\alpha +n)^{-1}, then the MAD distribution becomes \n  P_n(\\cdot) = \\frac{\\alpha}{\\alpha + n}P_0(\\cdot) + \\frac{1}{\\alpha + n}\\sum_{i=1}^nK_i(\\cdot\\mid y_i).\n\n\n\n\n\n\n\n\nTheorem\n\n\n\nLet (Y_n)_{n\\ge1} be a MAD sequence. Then, for every set of weights (w_n)_{n \\ge 1}, discrete base kernel k_*, and initial distribution P_0, the sequence (Y_n)_{n\\ge1} is conditionally identically distributed.\n\n\n\nA sequence (Y_n)_{n\\ge1} is conditionally identically distributed (cid) if, \\mathbb{P}-a.s., \n  \\mathbb{P}\\{Y_{n+k}\\in\\cdot\\mid y_{1:n}\\} = \\mathbb{P}\\{Y_{n+1}\\in\\cdot\\mid y_{1:n}\\}=P_n(\\cdot)\n  \\qquad k\\ge1.\n\nThis is equivalent to the martingale condition \n  \\mathbb{E}\\{P_{n+1}(\\cdot)\\mid y_{1:n}\\} = P_{n}(\\cdot)."
  },
  {
    "objectID": "post/abaco26/abaco26.html#bayesian-properties-of-cid-sequences",
    "href": "post/abaco26/abaco26.html#bayesian-properties-of-cid-sequences",
    "title": "ABACO26",
    "section": "Bayesian properties of CID sequences",
    "text": "Bayesian properties of CID sequences\n\n\n\n\n\n\nCorollary (Aldous 1985; Berti et al. 2004)\n\n\n\nConsider a MAD sequence (Y_n)_{n\\ge1}. Then, \\mathbb{P}-a.s.,\n(a) The sequence is asymptotically exchangeable, that is \n    (Y_{n+1}, Y_{n+2}, \\ldots) \\overset{\\textup{d}}{\\longrightarrow} (Z_1, Z_2, \\ldots), \\qquad n \\rightarrow \\infty,\n where (Z_1,Z_2,\\ldots) is an exchangeable sequence with directing random probability measure P;\n(b) the corresponding sequence of predictive distributions (P_n)_{n\\ge1} weakly converge to a random probability measures P (a.s. \\mathbb{P}).\n\n\n\nThe ordering dependence will vanish asymptotically and, informally, Y_i \\mid P \\overset{\\mathrm{iid}}{\\sim} P for large n.\nThe random probability measure P exists and is defined as the limit of the predictive distributions."
  },
  {
    "objectID": "post/abaco26/abaco26.html#predictive-inference",
    "href": "post/abaco26/abaco26.html#predictive-inference",
    "title": "ABACO26",
    "section": "Predictive inference",
    "text": "Predictive inference\n\nThus, an asymptotic equivalent of de Finetti’s theorem holds: each MAD sequence has a corresponding unique prior on P.\nThe prior and posterior on P exists but are not available explicitly.\n\n\n\n\n\n\n\nCorollary\n\n\n\nLet \\theta = P(f) = \\sum_{y \\in \\mathcal{Y}} f(y) p(y) and analogously \\theta_n = P_n(f) = \\sum_{y \\in \\mathcal{Y}} f(y) p_n(y) be any functional of interest. Consider a MAD sequence (Y_n)_{n\\ge1}. Then, \\mathbb{P}-a.s., for every n\\ge1 and every integrable function f:\\mathcal{Y}\\rightarrow\\mathbb{R}, we have \n\\mathbb{E}(\\theta \\mid y_{1:n}) = \\mathbb{E}\\{P(f) \\mid y_{1:n}\\} = P_n(f) = \\theta_n\n\n\n\n\nMoreover, we also have that \\theta = \\mathbb{E}\\{P(f)\\}=P_0(f) = \\theta_0 for every integrable function f, so that P_0 retains the role of a base measure as for standard Dirichlet sequences, providing an initial guess at P.\n\n\nUncertainty quantification for \\theta=P(f) is carried out by predictive resampling (Fong et al. 2023)."
  },
  {
    "objectID": "post/abaco26/abaco26.html#illustrative-example",
    "href": "post/abaco26/abaco26.html#illustrative-example",
    "title": "ABACO26",
    "section": "Illustrative example",
    "text": "Illustrative example\n\n\n\n\n\n\nLeft plot: posterior mean of a DP. Right plot: posterior mean of the proposed MAD sequence."
  },
  {
    "objectID": "post/abaco26/abaco26.html#on-the-choice-of-the-base-kernel",
    "href": "post/abaco26/abaco26.html#on-the-choice-of-the-base-kernel",
    "title": "ABACO26",
    "section": "On the choice of the base kernel",
    "text": "On the choice of the base kernel\n\nIn principle, any discrete distribution can be chosen as the base kernel k_*. However, it is natural to consider choices that allow the kernel to be centered at y_n while permitting control over the variance.\n\n\nWe consider a rounded Gaussian distribution centered in y_n, with pmf \n  k_*(y\\mid y_n, \\sigma) = \\frac{\\int_{y-1/2}^{y+1/2}\\mathcal{N}(t\\mid y_n, \\sigma^2) \\mathrm{d}t}{\\sum_{z\\in\\mathcal{Y}} \\int_{z-1/2}^{z+1/2}\\mathcal{N}(t\\mid y_n, \\sigma^2) \\mathrm{d}t},\n for n\\ge1, where \\mathcal{N}(\\cdot\\mid y_n,\\sigma^2) denotes a normal density function with mean y_n and variance \\sigma^2."
  },
  {
    "objectID": "post/abaco26/abaco26.html#role-of-the-weights-in-controlling-posterior-variability",
    "href": "post/abaco26/abaco26.html#role-of-the-weights-in-controlling-posterior-variability",
    "title": "ABACO26",
    "section": "Role of the weights in controlling posterior variability",
    "text": "Role of the weights in controlling posterior variability\n\nIt can be shown that the distribution of P(A)\\mid y_{1:n} is approximated by \\mathcal{N}(P_n(A), \\Sigma_n r_n^{-1}) for n large, where the variance is \n  \\Sigma_n r_n^{-1}\n  \\approx \\mathbb{E}\\{[P_{n+1}(A)-P_n(A)]^2\\mid y_{1:n}\\}\\sum_{k&gt;n+1}w_k^2.\n\n\n\nWeights that decay to zero quickly induce fast learning and convergence to the asymptotic exchangeable regime.\nBut small values of w_n leads to poor learning and underestimation of the posterior variability.\n\n\nPossible choices are w_n=(\\alpha+n)^{-1}, w_n=(\\alpha+n)^{-2/3} (Martin and Tokdar 2009), and w_n=(2 - n^{-1})(n+1)^{-1} (Fong et al. 2023).\nWe consider adaptive weights \n  w_n=(\\alpha+n)^{-\\lambda_n}, \\qquad \\lambda_n=\\lambda+(1+\\lambda)\\exp\\bigg\\{-\\frac{1}{N_*}n\\bigg\\},\n with \\lambda\\in(0.5,1], N_*&gt;0."
  },
  {
    "objectID": "post/abaco26/abaco26.html#multivariate-count-and-binary-data",
    "href": "post/abaco26/abaco26.html#multivariate-count-and-binary-data",
    "title": "ABACO26",
    "section": "Multivariate count and binary data",
    "text": "Multivariate count and binary data\n\nExtending MAD sequences for multivariate data is straightforward using a factorized base kernel \nk_*(\\bm y\\mid\\bm y_n) = \\prod_{j=1}^d k_*(y_j\\mid y_{n,j}),\n with \\bm y=(y_1,\\ldots,y_d) and \\bm y_n=(y_{n,1},\\ldots,y_{n,d}).\n\n\nMAD sequences can be employed for modeling multivariate binary data using an appropriate base kernel.\n\n\nA natural step further is to use MAD sequences for nonparametric regression and classification."
  },
  {
    "objectID": "post/abaco26/abaco26.html#simulations-i",
    "href": "post/abaco26/abaco26.html#simulations-i",
    "title": "ABACO26",
    "section": "Simulations I",
    "text": "Simulations I\nOut-of-sample prediction accuracy evaluated in terms of MSE and AUC for regression and classification, respectively.\n\n\n\n\n\n\n\n\n\n\n\n\nRegression (MSE)\n\n\nClassification (AUC)\n\n\n\n\n\n\nn=40\nn=80\n\nn=150\nn=300\n\n\nGLM\n120.77 [51.51]\n94.93 [8.37]\n\n0.796 [0.014]\n0.809 [0.007]\n\n\nBART\n101.17 [12.69]\n74.17 [10.00]\n\n0.863 [0.026]\n0.932 [0.009]\n\n\nRF\n99.98 [7.45]\n87.75 [6.53]\n\n0.882 [0.025]\n0.913 [0.015]\n\n\nDP\n1450.21 [5.53]\n1395.61 [8.72]\n\n0.644 [0.011]\n0.724 [0.012]\n\n\nMAD-1\n91.07 [10.35]\n73.96 [7.60]\n\n0.873 [0.014]\n0.899 [0.008]\n\n\nMAD-2/3\n88.83 [13.00]\n73.18 [9.58]\n\n0.869 [0.015]\n0.899 [0.009]\n\n\nMAD-dpm\n87.41 [12.36]\n72.07 [9.48]\n\n0.872 [0.014]\n0.901 [0.008]\n\n\nMAD-ada\n90.61 [10.28]\n73.45 [7.69]\n\n0.874 [0.014]\n0.900 [0.008]"
  },
  {
    "objectID": "post/abaco26/abaco26.html#simulations-ii",
    "href": "post/abaco26/abaco26.html#simulations-ii",
    "title": "ABACO26",
    "section": "Simulations II",
    "text": "Simulations II"
  },
  {
    "objectID": "post/abaco26/abaco26.html#application",
    "href": "post/abaco26/abaco26.html#application",
    "title": "ABACO26",
    "section": "Application",
    "text": "Application\n\nWe analyze the occurrence rates of 4 species corvids in Finland in year 2009 across different temperatures and habitats."
  },
  {
    "objectID": "post/abaco26/abaco26.html#application-ii",
    "href": "post/abaco26/abaco26.html#application-ii",
    "title": "ABACO26",
    "section": "Application II",
    "text": "Application II"
  },
  {
    "objectID": "post/abaco26/abaco26.html#appendix",
    "href": "post/abaco26/abaco26.html#appendix",
    "title": "ABACO26",
    "section": "Appendix",
    "text": "Appendix"
  },
  {
    "objectID": "post/abaco26/abaco26.html#empirical-consistency",
    "href": "post/abaco26/abaco26.html#empirical-consistency",
    "title": "ABACO26",
    "section": "Empirical consistency",
    "text": "Empirical consistency\n\n\n\nIllustrative example. Hellinger distance of MAD-ada (red), MAD-1 (blue), and DP (black) predictive distributions with respect to the density of the true data generator."
  },
  {
    "objectID": "post/abaco26/abaco26.html#predictive-resampling-for-mad-sequences",
    "href": "post/abaco26/abaco26.html#predictive-resampling-for-mad-sequences",
    "title": "ABACO26",
    "section": "Predictive resampling for MAD sequences",
    "text": "Predictive resampling for MAD sequences\nAlgorithm (Fortini and Petrone 2020):\n\nCompute P_n(\\cdot) from the observed data y_{1:n}\nSet N\\gg n\nFor j = 1,\\ldots,B\n\nFor i=n+1,\\ldots,N\n\nSample Y_i\\mid y_{1:i-1}\\sim P_{i-1}\nUpdate P_i(\\cdot) = (1-w_i)P_{i-1}(\\cdot) + w_i K_{i-1}(\\cdot\\mid y_i)\n\nEnd For\n\nEnd For\nReturn P_N^{(1)}(\\cdot),\\ldots,P_N^{(B)}(\\cdot), an iid sample from the distribution of P_N(\\cdot)\\mid y_{1:n}"
  },
  {
    "objectID": "post/abaco26/abaco26.html#on-the-choice-of-the-base-kernel-1",
    "href": "post/abaco26/abaco26.html#on-the-choice-of-the-base-kernel-1",
    "title": "ABACO26",
    "section": "On the choice of the base kernel",
    "text": "On the choice of the base kernel\n\n\n\n\n\n\n\\mathrm{var}\\{P_{n+1}(y)\\mid y_{1:n}\\}, \\text{ with } y\\in\\{20,40,60,80\\}\n \n\\mathrm{cor}\\{P_{n+1}(40),P_{n+1}(y)\\mid y_{1:n}\\}, \\text{ with } y\\in\\{10, 41, 80\\}"
  },
  {
    "objectID": "post/abaco26/abaco26.html#asymptotic-posterior-distributions-i",
    "href": "post/abaco26/abaco26.html#asymptotic-posterior-distributions-i",
    "title": "ABACO26",
    "section": "Asymptotic posterior distributions I",
    "text": "Asymptotic posterior distributions I\n\n\n\n\n\n\nTheorem\n\n\n\nLet (Y_n)_{n\\ge1} be a MAD sequence and let (r_n)_{n\\ge1} be a monotone sequence of positive numbers such that r_n\\overset{\\cdot}{=}(\\sum_{k&gt;n}w_k^2)^{-1} as n\\rightarrow\\infty. Assume \\sqrt{r_n}\\sup_{k\\ge n}w_k\\rightarrow 0, \\sum_{k\\ge1}r_k^2w_{k+1}^4&lt;\\infty, and that, for every H\\ge1 and all measurable sets A_1,\\ldots,A_H, the random matrix \\bm\\Sigma = \\bm\\Sigma(A_1,\\ldots,A_H) = [\\Sigma_{jt}] is positive definite. Then, for every H\\ge1 and every A_1,\\ldots,A_H, \n    \\sqrt{r_n}\\left[\\begin{array}{c}\n     P(A_1)-P_n(A_1)\\\\\n     \\cdots\\\\\n     P(A_H)-P_n(A_H)\n    \\end{array}\\right] \\Big|\\;\nY_{1:n}\n    \\overset{d}{\\longrightarrow} \\mathcal{N}_H(\\bm0,\\bm\\Sigma)\n \\mathbb{P}-a.s. for n\\rightarrow\\infty."
  },
  {
    "objectID": "post/abaco26/abaco26.html#asymptotic-posterior-distributions-ii",
    "href": "post/abaco26/abaco26.html#asymptotic-posterior-distributions-ii",
    "title": "ABACO26",
    "section": "Asymptotic posterior distributions II",
    "text": "Asymptotic posterior distributions II\n\n\n\n\n\n\nTheorem\n\n\n\nFor every n\\ge1 and all measurable sets A_1,\\ldots,A_H, let \\bm\\Sigma_n=\\bm\\Sigma_n(A_1,\\ldots,A_H) = [\\Sigma_{n,jt}]. Then, under the assumptions of previous Theorem, \\mathbb{P}-a.s., \\bm\\Sigma_n\\rightarrow\\bm\\Sigma and \n    \\sqrt{r_n}\\bm\\Sigma_n^{-1/2}\\left[\\begin{array}{c}\n     P(A_1)-P_n(A_1)\\\\\n     \\cdots\\\\\n     P(A_H)-P_n(A_H)\n    \\end{array}\\right] \\Big|\\;\ny_{1:n}\n    \\overset{d}{\\longrightarrow} \\mathcal{N}_H(\\bm0,\\bm I)\n for n\\rightarrow\\infty, where \\bm I denotes the identity matrix."
  },
  {
    "objectID": "post/abaco26/abaco26_slides.html#an-alternative-look-at-the-posterior-distribution-of-p",
    "href": "post/abaco26/abaco26_slides.html#an-alternative-look-at-the-posterior-distribution-of-p",
    "title": "ABACO26",
    "section": "An alternative look at the posterior distribution of P",
    "text": "An alternative look at the posterior distribution of P"
  },
  {
    "objectID": "post/abaco26/abaco26_slides.html#an-alternative-look-at-the-posterior-distribution",
    "href": "post/abaco26/abaco26_slides.html#an-alternative-look-at-the-posterior-distribution",
    "title": "ABACO26",
    "section": "An alternative look at the posterior distribution",
    "text": "An alternative look at the posterior distribution"
  },
  {
    "objectID": "post/abaco26/abaco26_slides.html#connecting-inference-and-prediction",
    "href": "post/abaco26/abaco26_slides.html#connecting-inference-and-prediction",
    "title": "ABACO26",
    "section": "Connecting inference and prediction",
    "text": "Connecting inference and prediction\n\nThe posterior P \\mid y_{1:n} is usually obtained through Bayes theorem, but this is not the only way.\nWe can characterize both prior and posterior of P through the predictive distributions P_n, which indeed contains all the necessary information.\n\n\n\nIf (Y_n)_{n \\ge 1} is exchangeable, then the prior and posterior mean of P coincide with the predictive: \nP_0(A) = \\mathbb{P}(Y_1\\in A) = \\mathbb{E}\\{P(A)\\}, \\qquad P_n(A) = \\mathbb{P}(Y_{n+1}\\in A\\mid y_{1:n}) = \\mathbb{E}(P(A) \\mid y_{1:n}), \\qquad n \\ge 1.\n However, a deeper result hold, which can be regarded as a corollary Finetti theorem (Fortini and Petrone 2012).\n\n\n\n\n\n\n\nDe Finetti’s representation theorem\n\n\nLet (Y_n)_{n \\ge 1} be an exchangeable sequence. Then the sequence of predictive distributions (P_n)_{n \\ge 1} converges weakly (a.s. \\mathbb{P}) to a random probability measure P distributed according to \\Pi."
  },
  {
    "objectID": "post/abaco26/abaco26_slides.html#connecting-inference-and-prediction-i",
    "href": "post/abaco26/abaco26_slides.html#connecting-inference-and-prediction-i",
    "title": "ABACO26",
    "section": "Connecting inference and prediction I",
    "text": "Connecting inference and prediction I\n\nThe posterior P \\mid y_{1:n} is usually obtained through Bayes theorem, but this is not the only way.\nWe can characterize both prior and posterior of P through the predictive distributions P_n, which indeed contains all the necessary information.\n\n\n\nIf (Y_n)_{n \\ge 1} is exchangeable, then the prior and posterior mean of P coincide with the predictive: \nP_0(A) = \\mathbb{P}(Y_1\\in A) = \\mathbb{E}\\{P(A)\\}, \\qquad P_n(A) = \\mathbb{P}(Y_{n+1}\\in A\\mid y_{1:n}) = \\mathbb{E}(P(A) \\mid y_{1:n}), \\qquad n \\ge 1.\n\n\n\n\n\nA deeper result holds, which is a corollary of Finetti theorem (Fortini and Petrone 2012).\n\n\n\n\n\nDe Finetti’s representation theorem (predictive form)\n\n\nLet (Y_n)_{n \\ge 1} be an exchangeable sequence with predictive distributions (P_n)_{n \\ge 1}. Then P_n converges weakly (a.s. \\mathbb{P}) to a random probability measure P distributed according to \\Pi as n \\to \\infty."
  },
  {
    "objectID": "post/abaco26/abaco26_slides.html#connecting-inference-and-prediction-ii",
    "href": "post/abaco26/abaco26_slides.html#connecting-inference-and-prediction-ii",
    "title": "ABACO26",
    "section": "Connecting inference and prediction II",
    "text": "Connecting inference and prediction II\n\nIn other words, the sequence of predictive distributions P_n converges to a random probability measure P with prior distribution \\Pi. The source of randomness is the data\n\nY_1, Y_2, Y_3, \\dots\n Intuitively, before observing the data, our predictions eventually reflect the prior.\n\n\n\nGiven y_{1:n}, the sequence P_{n+m} converges weakly (a.s. \\mathbb{P}) as m \\to \\infty to a random probability measure with posterior distribution \\Pi(\\cdot \\mid y_{1:n}). The source of randomness is the future data\n\nY_{n+1}, Y_{n+2}, Y_{n+3}, \\dots .\n\nThis provides a natural alternative interpretation of the posterior distribution P \\mid y_{1:n} and a practical algorithm for sampling from it, called predictive resampling.\nIntuitively, posterior uncertainty arises from lack of knowledge about future observations. If we knew them, the posterior would collapse to a point mass (Bayesian consistency).\n\n\n\n\nThis reasoning is at the heart of martingale posteriors (Fong et al. 2023)."
  },
  {
    "objectID": "post/abaco26/abaco26.html#connecting-inference-and-prediction-i",
    "href": "post/abaco26/abaco26.html#connecting-inference-and-prediction-i",
    "title": "ABACO26",
    "section": "Connecting inference and prediction I",
    "text": "Connecting inference and prediction I\n\nThe posterior P \\mid y_{1:n} is usually obtained through Bayes theorem, but this is not the only way.\nWe can characterize both prior and posterior of P through the predictive distributions P_n, which indeed contains all the necessary information.\n\n\nIf (Y_n)_{n \\ge 1} is exchangeable, then the prior and posterior mean of P coincide with the predictive: \nP_0(A) = \\mathbb{P}(Y_1\\in A) = \\mathbb{E}\\{P(A)\\}, \\qquad P_n(A) = \\mathbb{P}(Y_{n+1}\\in A\\mid y_{1:n}) = \\mathbb{E}(P(A) \\mid y_{1:n}), \\qquad n \\ge 1.\n\n\n\nA deeper result holds, which is a corollary of Finetti theorem (Fortini and Petrone 2012).\n\n\n\n\n\n\n\nDe Finetti’s representation theorem (predictive form)\n\n\n\nLet (Y_n)_{n \\ge 1} be an exchangeable sequence with predictive distributions (P_n)_{n \\ge 1}. Then P_n converges weakly (a.s. \\mathbb{P}) to a random probability measure P distributed according to \\Pi as n \\to \\infty."
  },
  {
    "objectID": "post/abaco26/abaco26.html#connecting-inference-and-prediction-ii",
    "href": "post/abaco26/abaco26.html#connecting-inference-and-prediction-ii",
    "title": "ABACO26",
    "section": "Connecting inference and prediction II",
    "text": "Connecting inference and prediction II\n\nIn other words, the sequence of predictive distributions P_n converges to a random probability measure P with prior distribution \\Pi. The source of randomness is the data\n\nY_1, Y_2, Y_3, \\dots\n Intuitively, before observing the data, our predictions eventually reflect the prior.\n\n\nGiven y_{1:n}, the sequence P_{n+m} converges weakly (a.s. \\mathbb{P}) as m \\to \\infty to a random probability measure with posterior distribution \\Pi(\\cdot \\mid y_{1:n}). The source of randomness is the future data\n\nY_{n+1}, Y_{n+2}, Y_{n+3}, \\dots .\n\nThis provides a natural alternative interpretation of the posterior distribution P \\mid y_{1:n} and a practical algorithm for sampling from it, called predictive resampling.\nIntuitively, posterior uncertainty arises from lack of knowledge about future observations. If we knew them, the posterior would collapse to a point mass (Bayesian consistency).\n\n\nThis reasoning is at the heart of martingale posteriors (Fong et al. 2023)."
  },
  {
    "objectID": "post/BISP2025/BISP2025_slides.html#section",
    "href": "post/BISP2025/BISP2025_slides.html#section",
    "title": "BISP14 workshop",
    "section": "",
    "text": "Bissiri, P. G., Holmes, C. C., and Walker, S. G. (2016), “A general framework for updating belief distributions,” Journal of the Royal Statistical Society: Series B (Statistical Methodology), Wiley Online Library, 78, 1103–1130.\n\n\nChernozhukov, V., and Hong, H. (2003), “An MCMC approach to classical estimation,” Journal of econometrics, Elsevier, 115, 293–346.\n\n\nDe Finetti, B. (1937), “La prévision: Ses lois logiques, ses sources subjectives,” in Annales de l’institut henri poincaré, pp. 1–68.\n\n\nGrünwald, P. D., and Mehta, N. A. (2020), “Fast rates for general unbounded loss functions: from ERM to generalized Bayes,” The Journal of Machine Learning Research, JMLRORG, 21, 2040–2119.\n\n\nHeide, R. de, Kirichenko, A., Grunwald, P., and Mehta, N. (2020), “Safe-bayesian generalized linear regression,” in Proceedings of the twenty third international conference on artificial intelligence and statistics, PMLR, pp. 2623–2633.\n\n\nHolmes, C. C., and Walker, S. G. (2017), “Assigning a value to a power likelihood in a general Bayesian model,” Biometrika, Oxford University Press, 104, 497–503.\n\n\nJewson, J., and Rossell, D. (2022), “General bayesian loss function selection and the use of improper models,” Journal of the Royal Statistical Society Series B: Statistical Methodology, Oxford University Press, 84, 1640–1665.\n\n\nKnoblauch, J., Jewson, J., and Damoulas, T. (2022), “An optimization-centric view on bayes’ rule: Reviewing and generalizing variational inference,” Journal of Machine Learning Research, 23, 1–109.\n\n\nLyddon, S. P., Holmes, C. C., and Walker, S. G. (2019), “General Bayesian updating and the loss-likelihood bootstrap,” Biometrika, Oxford University Press, 106, 465–478.\n\n\nMatsubara, T., Knoblauch, J., Briol, F.-X., and Oates, C. J. (2022), “Robust generalised bayesian inference for intractable likelihoods,” Journal of the Royal Statistical Society Series B: Statistical Methodology, Oxford University Press, 84, 997–1022.\n\n\nMatsubara, T., Knoblauch, J., Briol, F.-X., and Oates, C. J. (2023), “Generalized bayesian inference for discrete intractable likelihood,” Journal of the American Statistical Association, Taylor & Francis, 1–11.\n\n\nMcCullagh, P. (1983), “Quasi-likelihood functions,” Annals of Statistics, Institute of Mathematical Statistics, 11, 59–67.\n\n\nMcCullagh, P., and Nelder, J. A. (1989), Generalized linear models, Chapman & Hall.\n\n\nRigon, T., Herring, A. H., and Dunson, D. B. (2023), “A generalized Bayes framework for probabilistic clustering,” Biometrika, Oxford University Press, 10, 559–578.\n\n\nSyring, N., and Martin, R. (2019), “Calibrating general posterior credible regions,” Biometrika, Oxford University Press, 106, 479–486.\n\n\nWedderburn, R. W. (1974), “Quasi-likelihood functions, generalized linear models, and the Gauss-Newton method,” Biometrika, Oxford University Press, 61, 439–447."
  },
  {
    "objectID": "post/abaco26/abaco26_slides.html#nonparametric-modelling-of-count-data",
    "href": "post/abaco26/abaco26_slides.html#nonparametric-modelling-of-count-data",
    "title": "ABACO26",
    "section": "Nonparametric modelling of count data",
    "text": "Nonparametric modelling of count data\n\nBayesian nonparametric modeling of counts distributions is a challenging task. Nonparametric mixtures of discrete kernels (Canale and Dunson 2011) can be cumbersome in practice.\n\n\n\nAlternatively, one could directly specify a DP prior on the data generator as \n  Y_i\\mid P \\overset{\\textup{iid}}{\\sim} P,\\quad P\\sim\\mathrm{DP}(\\alpha, P_0),\n for Y_i\\in\\mathcal{Y}=\\{0,1,\\ldots\\}, i=1,\\ldots,n, where \\alpha is the precision parameter and P_0 a base parametric distribution, such as a Poisson.\n\n\n\n\nThe Dirichlet process is mathematically convenient. However, the corresponding posterior lacks smoothing, which can lead to poor performance.\nWithin the hypothetical approach, it is unclear how to specify a nonparametric process with the same simplicity and flexibility as the DP prior while allowing for smoothing.\n\n\n\n\nOur proposal: a predictive sequence tailored to count data inspired by kernel density estimators."
  },
  {
    "objectID": "post/abaco26/abaco26.html#nonparametric-modelling-of-count-data",
    "href": "post/abaco26/abaco26.html#nonparametric-modelling-of-count-data",
    "title": "ABACO26",
    "section": "Nonparametric modelling of count data",
    "text": "Nonparametric modelling of count data\n\nBayesian nonparametric modeling of counts distributions is a challenging task. Nonparametric mixtures of discrete kernels (Canale and Dunson 2011) can be cumbersome in practice.\n\n\nAlternatively, one could directly specify a DP prior on the data generator as \n  Y_i\\mid P \\overset{\\textup{iid}}{\\sim} P,\\quad P\\sim\\mathrm{DP}(\\alpha, P_0),\n for Y_i\\in\\mathcal{Y}=\\{0,1,\\ldots\\}, i=1,\\ldots,n, where \\alpha is the precision parameter and P_0 a base parametric distribution, such as a Poisson.\n\n\nThe Dirichlet process is mathematically convenient. However, the corresponding posterior lacks smoothing, which can lead to poor performance.\nWithin the hypothetical approach, it is unclear how to specify a nonparametric process with the same simplicity and flexibility as the DP prior while allowing for smoothing.\n\n\nOur proposal: a predictive sequence tailored to count data inspired by kernel density estimators."
  },
  {
    "objectID": "post/abaco26/abaco26_slides.html#illustrative-example-i",
    "href": "post/abaco26/abaco26_slides.html#illustrative-example-i",
    "title": "ABACO26",
    "section": "Illustrative example I",
    "text": "Illustrative example I\n\n\n\n\n\n\nLeft plot: posterior mean of a DP. Right plot: posterior mean of the proposed MAD sequence."
  },
  {
    "objectID": "post/abaco26/abaco26_slides.html#a-recursive-predictive-rule",
    "href": "post/abaco26/abaco26_slides.html#a-recursive-predictive-rule",
    "title": "ABACO26",
    "section": "A recursive predictive rule",
    "text": "A recursive predictive rule\n\nIntuitively, a better estimator would be obtained by replacing the indicator \\mathbb{1}(\\cdot) of the DP predictive scheme with a kernel that allows the borrowing of information between nearby values.\n\n\n\nLet Y_1\\sim P_0 and Y_{n+1} \\mid y_{1:n} \\sim P_n for n\\ge 1, and let K_n(\\cdot \\mid y_n) be a sequence of transition kernels. We define the predictive distribution recursively: \n  P_n(\\cdot) = \\mathbb{P}(Y_{n+1}\\in \\cdot\\mid y_{1:n}) = (1-w_n) P_{n-1}(\\cdot) + w_n K_{n}(\\cdot\\mid y_n), \\qquad n \\ge 1,\n where (w_n)_{n\\ge1} are decreasing weights such that w_n\\in(0,1), \\sum_{n\\ge1}w_n = \\infty, and \\sum_{n\\ge1}w_n^2&lt;\\infty.\nThe choice of weights w_n = (\\alpha +n)^{-1} gives the following DP-like predictive rule \nP_n(\\cdot) = \\frac{\\alpha}{\\alpha + n} P_0(\\cdot) + \\frac{1}{\\alpha + n}\\sum_{i=1}^nK_i(\\cdot\\mid y_i).\n Hence, the predictive law of a DP is a special case whenever K_i(\\cdot \\mid y_i) = \\delta_{y_i}(\\cdot).\n\n\n\n\nThe above sequence, beyond the DP special case, is not exchangeable and it will depend on the order of the data. Moreover, without further restrictions, is not necessarily CID!"
  },
  {
    "objectID": "post/abaco26/abaco26.html#a-recursive-predictive-rule",
    "href": "post/abaco26/abaco26.html#a-recursive-predictive-rule",
    "title": "ABACO26",
    "section": "A recursive predictive rule",
    "text": "A recursive predictive rule\n\nIntuitively, a better estimator would be obtained by replacing the indicator \\mathbb{1}(\\cdot) of the DP predictive scheme with a kernel that allows the borrowing of information between nearby values.\n\n\nLet Y_1\\sim P_0 and Y_{n+1} \\mid y_{1:n} \\sim P_n for n\\ge 1, and let K_n(\\cdot \\mid y_n) be a sequence of transition kernels. We define the predictive distribution recursively: \n  P_n(\\cdot) = \\mathbb{P}(Y_{n+1}\\in \\cdot\\mid y_{1:n}) = (1-w_n) P_{n-1}(\\cdot) + w_n K_{n}(\\cdot\\mid y_n), \\qquad n \\ge 1,\n where (w_n)_{n\\ge1} are decreasing weights such that w_n\\in(0,1), \\sum_{n\\ge1}w_n = \\infty, and \\sum_{n\\ge1}w_n^2&lt;\\infty.\nThe choice of weights w_n = (\\alpha +n)^{-1} gives the following DP-like predictive rule \nP_n(\\cdot) = \\frac{\\alpha}{\\alpha + n} P_0(\\cdot) + \\frac{1}{\\alpha + n}\\sum_{i=1}^nK_i(\\cdot\\mid y_i).\n Hence, the predictive law of a DP is a special case whenever K_i(\\cdot \\mid y_i) = \\delta_{y_i}(\\cdot).\n\n\n\n\n\n\n\nThe above sequence, beyond the DP special case, is not exchangeable and it will depend on the order of the data. Moreover, without further restrictions, is not necessarily CID!"
  },
  {
    "objectID": "post/abaco26/abaco26_slides.html#bayesian-properties-of-cid-sequences-i",
    "href": "post/abaco26/abaco26_slides.html#bayesian-properties-of-cid-sequences-i",
    "title": "ABACO26",
    "section": "Bayesian properties of CID sequences I",
    "text": "Bayesian properties of CID sequences I\n\n\n\n\nCorollary (Aldous 1985; Berti et al. 2004)\n\n\nConsider a MAD sequence (Y_n)_{n\\ge1}. Then, \\mathbb{P}-a.s.,\n(a) The sequence is asymptotically exchangeable, that is \n    (Y_{n+1}, Y_{n+2}, \\ldots) \\overset{\\textup{d}}{\\longrightarrow} (Z_1, Z_2, \\ldots), \\qquad n \\rightarrow \\infty,\n where (Z_1,Z_2,\\ldots) is an exchangeable sequence with directing random probability measure P;\n(b) the corresponding sequence of predictive distributions (P_n)_{n\\ge1} weakly converge to a random probability measures P (a.s. \\mathbb{P}).\n\n\n\n\n\nAn asymptotic equivalent of de Finetti’s theorem holds: each MAD sequence has a corresponding unique prior on P.\nThe ordering dependence will vanish asymptotically and, informally, Y_i \\mid P \\overset{\\mathrm{iid}}{\\sim} P for large n.\nThe random probability measure P exists and is defined as the limit of the predictive distributions. However, it is not available explicitly."
  },
  {
    "objectID": "post/abaco26/abaco26_slides.html#bayesian-properties-of-cid-sequences-ii",
    "href": "post/abaco26/abaco26_slides.html#bayesian-properties-of-cid-sequences-ii",
    "title": "ABACO26",
    "section": "Bayesian properties of CID sequences II",
    "text": "Bayesian properties of CID sequences II\n\n\n\n\nCorollary (Aldous 1985; Berti et al. 2004)\n\n\nLet \\theta = P(f) = \\sum_{y \\in \\mathcal{Y}} f(y) p(y) and analogously \\theta_n = P_n(f) = \\sum_{y \\in \\mathcal{Y}} f(y) p_n(y) be any functional of interest. Consider a MAD sequence (Y_n)_{n\\ge1}.\nThen, \\mathbb{P}-a.s., for every n\\ge1 and every integrable function f:\\mathcal{Y}\\rightarrow\\mathbb{R}, we have \n\\mathbb{E}(\\theta \\mid y_{1:n}) = \\mathbb{E}\\{P(f) \\mid y_{1:n}\\} = P_n(f) = \\theta_n\n\n\n\n\n\n\nBroadly speaking, the posterior mean of any functional of interest of P coincides with the functional of the predictive.\nMoreover, \\mathbb{E}\\{P(f)\\}= P_0(f) = \\theta_0 for every integrable function f, so that P_0 retains the role of a base measure as for standard Dirichlet sequences, providing an initial guess at P.\n\n\n\nUncertainty quantification for \\theta=P(f) is carried out by predictive resampling (Fong et al. 2023)."
  },
  {
    "objectID": "post/abaco26/abaco26.html#bayesian-properties-of-cid-sequences-i",
    "href": "post/abaco26/abaco26.html#bayesian-properties-of-cid-sequences-i",
    "title": "ABACO26",
    "section": "Bayesian properties of CID sequences I",
    "text": "Bayesian properties of CID sequences I\n\n\n\n\n\n\nCorollary (Aldous 1985; Berti et al. 2004)\n\n\n\nConsider a MAD sequence (Y_n)_{n\\ge1}. Then, \\mathbb{P}-a.s.,\n(a) The sequence is asymptotically exchangeable, that is \n    (Y_{n+1}, Y_{n+2}, \\ldots) \\overset{\\textup{d}}{\\longrightarrow} (Z_1, Z_2, \\ldots), \\qquad n \\rightarrow \\infty,\n where (Z_1,Z_2,\\ldots) is an exchangeable sequence with directing random probability measure P;\n(b) the corresponding sequence of predictive distributions (P_n)_{n\\ge1} weakly converge to a random probability measures P (a.s. \\mathbb{P}).\n\n\n\nAn asymptotic equivalent of de Finetti’s theorem holds: each MAD sequence has a corresponding unique prior on P.\nThe ordering dependence will vanish asymptotically and, informally, Y_i \\mid P \\overset{\\mathrm{iid}}{\\sim} P for large n.\nThe random probability measure P exists and is defined as the limit of the predictive distributions. However, it is not available explicitly."
  },
  {
    "objectID": "post/abaco26/abaco26.html#bayesian-properties-of-cid-sequences-ii",
    "href": "post/abaco26/abaco26.html#bayesian-properties-of-cid-sequences-ii",
    "title": "ABACO26",
    "section": "Bayesian properties of CID sequences II",
    "text": "Bayesian properties of CID sequences II\n\n\n\n\n\n\nCorollary (Aldous 1985; Berti et al. 2004)\n\n\n\nLet \\theta = P(f) = \\sum_{y \\in \\mathcal{Y}} f(y) p(y) and analogously \\theta_n = P_n(f) = \\sum_{y \\in \\mathcal{Y}} f(y) p_n(y) be any functional of interest. Consider a MAD sequence (Y_n)_{n\\ge1}.\nThen, \\mathbb{P}-a.s., for every n\\ge1 and every integrable function f:\\mathcal{Y}\\rightarrow\\mathbb{R}, we have \n\\mathbb{E}(\\theta \\mid y_{1:n}) = \\mathbb{E}\\{P(f) \\mid y_{1:n}\\} = P_n(f) = \\theta_n\n\n\n\n\nBroadly speaking, the posterior mean of any functional of interest of P coincides with the functional of the predictive.\nMoreover, \\mathbb{E}\\{P(f)\\}= P_0(f) = \\theta_0 for every integrable function f, so that P_0 retains the role of a base measure as for standard Dirichlet sequences, providing an initial guess at P.\n\n\nUncertainty quantification for \\theta=P(f) is carried out by predictive resampling (Fong et al. 2023)."
  },
  {
    "objectID": "post/abaco26/abaco26_slides.html#illustrative-example-ii",
    "href": "post/abaco26/abaco26_slides.html#illustrative-example-ii",
    "title": "ABACO26",
    "section": "Illustrative example II",
    "text": "Illustrative example II"
  },
  {
    "objectID": "post/abaco26/abaco26.html#illustrative-example-i",
    "href": "post/abaco26/abaco26.html#illustrative-example-i",
    "title": "ABACO26",
    "section": "Illustrative example I",
    "text": "Illustrative example I\n\n\n\n\n\n\nLeft plot: posterior mean of a DP. Right plot: posterior mean of the proposed MAD sequence."
  },
  {
    "objectID": "post/abaco26/abaco26.html#illustrative-example-ii",
    "href": "post/abaco26/abaco26.html#illustrative-example-ii",
    "title": "ABACO26",
    "section": "Illustrative example II",
    "text": "Illustrative example II"
  }
]