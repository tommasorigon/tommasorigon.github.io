<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.7.32">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Tommaso Rigon">
<meta name="dcterms.date" content="2026-02-06">
<meta name="description" content="A joint work with Davide Agnoletto and David Dunson presented at the Workshop on Advances in BAyesian COmputation and modeling organized by Emanuele Aliverti at the University of Padova.">

<title>ABACO26 – Tommaso Rigon</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for citations */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
  margin-bottom: 0em;
}
.hanging-indent div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}</style>


<script src="../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../site_libs/quarto-html/quarto.js" type="module"></script>
<script src="../../site_libs/quarto-html/tabsets/tabsets.js" type="module"></script>
<script src="../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting-92eaa9f3cac64f6982a8a06e2ba947a3.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../site_libs/bootstrap/bootstrap-3111f4bc39e569af9b8dff312645f3ef.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>
<style>html{ scroll-behavior: smooth; }</style>

  <script>window.backupDefine = window.define; window.define = undefined;</script><script src="https://cdn.jsdelivr.net/npm/katex@latest/dist/katex.min.js"></script>
  <script>document.addEventListener("DOMContentLoaded", function () {
 var mathElements = document.getElementsByClassName("math");
 var macros = [];
 for (var i = 0; i < mathElements.length; i++) {
  var texText = mathElements[i].firstChild;
  if (mathElements[i].tagName == "SPAN") {
   katex.render(texText.data, mathElements[i], {
    displayMode: mathElements[i].classList.contains('display'),
    throwOnError: false,
    macros: macros,
    fleqn: false
   });
}}});
  </script>
  <script>window.define = window.backupDefine; window.backupDefine = undefined;</script><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@latest/dist/katex.min.css">

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

</head>

<body class="nav-fixed quarto-light">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg " data-bs-theme="dark">
      <div class="navbar-container container-fluid">
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" role="menu" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll me-auto">
  <li class="nav-item">
    <a class="nav-link" href="../../index.html"> 
<span class="menu-text">Home</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../papers.html"> 
<span class="menu-text">Papers</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../bio.html"> 
<span class="menu-text">Short Bio</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../teaching.html"> 
<span class="menu-text">Teaching &amp; Slides</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../tesi.html"> 
<span class="menu-text">Thesis Information (ITA)</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../post.html"> 
<span class="menu-text">Blogpost</span></a>
  </li>  
</ul>
          </div> <!-- /navcollapse -->
            <div class="quarto-navbar-tools">
</div>
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-full page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#warm-thanks" id="toc-warm-thanks" class="nav-link active" data-scroll-target="#warm-thanks">Warm thanks</a></li>
  <li><a href="#foundations" id="toc-foundations" class="nav-link" data-scroll-target="#foundations">Foundations</a></li>
  <li><a href="#hypothetical-approach" id="toc-hypothetical-approach" class="nav-link" data-scroll-target="#hypothetical-approach">Hypothetical approach</a></li>
  <li><a href="#generalizations-of-the-hypothetical-approach" id="toc-generalizations-of-the-hypothetical-approach" class="nav-link" data-scroll-target="#generalizations-of-the-hypothetical-approach">Generalizations of the hypothetical approach</a></li>
  <li><a href="#predictive-approach" id="toc-predictive-approach" class="nav-link" data-scroll-target="#predictive-approach">Predictive approach</a></li>
  <li><a href="#connecting-inference-and-prediction-i" id="toc-connecting-inference-and-prediction-i" class="nav-link" data-scroll-target="#connecting-inference-and-prediction-i">Connecting inference and prediction I</a></li>
  <li><a href="#connecting-inference-and-prediction-ii" id="toc-connecting-inference-and-prediction-ii" class="nav-link" data-scroll-target="#connecting-inference-and-prediction-ii">Connecting inference and prediction II</a></li>
  <li><a href="#generalizations-of-the-predictive-approach" id="toc-generalizations-of-the-predictive-approach" class="nav-link" data-scroll-target="#generalizations-of-the-predictive-approach">Generalizations of the predictive approach</a></li>
  <li><a href="#nonparametric-modelling-of-count-data" id="toc-nonparametric-modelling-of-count-data" class="nav-link" data-scroll-target="#nonparametric-modelling-of-count-data">Nonparametric modelling of count data</a></li>
  <li><a href="#illustrative-example-i" id="toc-illustrative-example-i" class="nav-link" data-scroll-target="#illustrative-example-i">Illustrative example I</a></li>
  <li><a href="#a-recursive-predictive-rule" id="toc-a-recursive-predictive-rule" class="nav-link" data-scroll-target="#a-recursive-predictive-rule">A recursive predictive rule</a></li>
  <li><a href="#metropolis-adjusted-dirichlet-mad-sequences" id="toc-metropolis-adjusted-dirichlet-mad-sequences" class="nav-link" data-scroll-target="#metropolis-adjusted-dirichlet-mad-sequences">Metropolis-adjusted Dirichlet (MAD) sequences</a></li>
  <li><a href="#bayesian-properties-of-cid-sequences-i" id="toc-bayesian-properties-of-cid-sequences-i" class="nav-link" data-scroll-target="#bayesian-properties-of-cid-sequences-i">Bayesian properties of CID sequences I</a></li>
  <li><a href="#bayesian-properties-of-cid-sequences-ii" id="toc-bayesian-properties-of-cid-sequences-ii" class="nav-link" data-scroll-target="#bayesian-properties-of-cid-sequences-ii">Bayesian properties of CID sequences II</a></li>
  <li><a href="#predictive-resampling-for-mad-sequences" id="toc-predictive-resampling-for-mad-sequences" class="nav-link" data-scroll-target="#predictive-resampling-for-mad-sequences">Predictive resampling for MAD sequences</a></li>
  <li><a href="#on-the-choice-of-the-base-kernel" id="toc-on-the-choice-of-the-base-kernel" class="nav-link" data-scroll-target="#on-the-choice-of-the-base-kernel">On the choice of the base kernel</a></li>
  <li><a href="#uncertainty-quantification-and-calibration" id="toc-uncertainty-quantification-and-calibration" class="nav-link" data-scroll-target="#uncertainty-quantification-and-calibration">Uncertainty quantification and calibration</a></li>
  <li><a href="#illustrative-examples-ii" id="toc-illustrative-examples-ii" class="nav-link" data-scroll-target="#illustrative-examples-ii">Illustrative examples II</a></li>
  <li><a href="#multivariate-count-and-binary-data" id="toc-multivariate-count-and-binary-data" class="nav-link" data-scroll-target="#multivariate-count-and-binary-data">Multivariate count and binary data</a></li>
  <li><a href="#simulations-i" id="toc-simulations-i" class="nav-link" data-scroll-target="#simulations-i">Simulations I</a></li>
  <li><a href="#application" id="toc-application" class="nav-link" data-scroll-target="#application">Application</a></li>
  <li><a href="#application-ii" id="toc-application-ii" class="nav-link" data-scroll-target="#application-ii">Application II</a></li>
  <li><a href="#thank-you" id="toc-thank-you" class="nav-link" data-scroll-target="#thank-you">Thank you!</a></li>
  <li><a href="#references" id="toc-references" class="nav-link" data-scroll-target="#references">References</a></li>
  </ul>
<div class="quarto-alternate-formats"><h2>Other Formats</h2><ul><li><a href="abaco26_slides.html"><i class="bi bi-file-slides"></i>RevealJS</a></li></ul></div></nav>
    </div>
<!-- main -->
<main class="content column-page-left" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title">ABACO26</h1>
<p class="subtitle lead"><em>Nonparametric predictive inference for discrete data via Metropolis-adjusted Dirichlet sequences</em></p>
  <div class="quarto-categories">
    <div class="quarto-category">talk</div>
  </div>
  </div>

<div>
  <div class="description">
    A joint work with <span class="orange">Davide Agnoletto</span> and <span class="blue">David Dunson</span> presented at the Workshop on <em>Advances in BAyesian COmputation and modeling organized</em> by Emanuele Aliverti at the University of Padova.
  </div>
</div>

<div class="quarto-title-meta-author column-page-left">
  <div class="quarto-title-meta-heading">Author</div>
  <div class="quarto-title-meta-heading">Affiliation</div>
  
    <div class="quarto-title-meta-contents">
    <p class="author"><span class="orange">Tommaso Rigon</span> </p>
  </div>
  <div class="quarto-title-meta-contents">
        <p class="affiliation">
            <em>Università degli Studi di Milano-Bicocca</em>
          </p>
      </div>
  </div>

<div class="quarto-title-meta column-page-left">

      
    <div>
    <div class="quarto-title-meta-heading">Published</div>
    <div class="quarto-title-meta-contents">
      <p class="date">February 6, 2026</p>
    </div>
  </div>
  
    
  </div>
  


</header>


<section id="warm-thanks" class="level2">
<h2 class="anchored" data-anchor-id="warm-thanks">Warm thanks</h2>
<div class="columns">
<div class="column" style="width:45%;">
<p><span class="orange">Davide Agnoletto</span> (Duke University)</p>
<div class="quarto-figure quarto-figure-left">
<figure class="figure">
<p><img src="img/davide.jpg" class="img-fluid quarto-figure quarto-figure-left figure-img"></p>
</figure>
</div>
</div><div class="column" style="width:10%;">

</div><div class="column" style="width:45%;">
<p><span class="orange">David Dunson</span> (Duke University)</p>
<div class="quarto-figure quarto-figure-left">
<figure class="figure">
<p><img src="img/david.jpeg" class="img-fluid quarto-figure quarto-figure-left figure-img"></p>
</figure>
</div>
</div>
</div>
</section>
<section id="foundations" class="level2">
<h2 class="anchored" data-anchor-id="foundations">Foundations</h2>
<ul>
<li>De Finetti’s representation Theorem <span class="citation" data-cites="de1937prevision">(<a href="#ref-de1937prevision" role="doc-biblioref">De Finetti 1937</a>)</span> it provides the fundamental justification to the <span class="blue">two approaches</span> to <span class="orange">Bayesian statistics</span>: the <span class="orange">hypothetical</span> approach and the <span class="blue">predictive</span> approach.</li>
</ul>
<div class="callout callout-style-simple callout-warning no-icon callout-titled" title="De Finetti's representation theorem">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
De Finetti’s representation theorem
</div>
</div>
<div class="callout-body-container callout-body">
<p>Let <span class="math inline">(Y_n)_{n\ge 1}</span>, be a sequence of <span class="blue">exchangeable</span> random variables. Then there exists a unique probability measure <span class="math inline">\Pi</span> such that, for any <span class="math inline">n\ge 1</span> and <span class="math inline">A_1,\dots,A_n</span> <span class="math display">
\mathbb{P}(Y_1 \in A_1,\ldots,Y_n \in A_n) = \int_{\mathcal{P}} \prod_{i=1}^n p(A_i)\,\Pi(\mathrm{d}p).
</span></p>
</div>
</div>
<ul>
<li>In a hierarchical formulation, we will say that <span class="math inline">(Y_n)_{n \ge 1}</span> is <span class="bl">exchangeable</span> if and only if <span class="math display">
\begin{aligned}
Y_i \mid P &amp;\overset{\textup{iid}}{\sim} P, \qquad i \ge 1, \\
P &amp;\sim \Pi,
\end{aligned}
</span> where <span class="math inline">P</span> is a <span class="blue">random probability measure</span> and <span class="math inline">\Pi</span> is the <span class="orange">prior law</span>.</li>
</ul>
</section>
<section id="hypothetical-approach" class="level2">
<h2 class="anchored" data-anchor-id="hypothetical-approach">Hypothetical approach</h2>
<ul>
<li><p>The hypothetical approach represents the the <span class="blue">most common</span> way to operate within the Bayesian community.</p></li>
<li><p>In a <span class="orange">parametric</span> setting, <span class="math inline">\Pi</span> has support on a class <span class="math inline">\Theta\subseteq\mathbb{R}^p</span> with <span class="math inline">p&lt;\infty</span>, such that <span class="math inline">\boldsymbol{\theta}\in\Theta</span> indexes the class of distributions <span class="math inline">\mathcal{P}_{\boldsymbol{\theta}}=\{P_{\boldsymbol{\theta}} : \boldsymbol{\theta} \in \Theta\subseteq\mathbb{R}^p\}</span>.</p></li>
<li><p>Bayes’ rule takes the well-known formulation: <span class="math display">
\pi(\boldsymbol{\theta}\mid y_{1:n}) \propto \pi(\boldsymbol{\theta}) \prod_{i=1}^n p_{\boldsymbol{\theta}}(y_i),
</span> where <span class="math inline">\pi</span> and <span class="math inline">p_{\boldsymbol{\theta}}</span> denote the probability density functions associated with <span class="math inline">\Pi</span> and <span class="math inline">P_{\boldsymbol{\theta}}</span>, respectively.</p></li>
<li><p>However, when the link between observations and parameter of interest cannot be easily expressed through a distribution function, the traditional hypothetical approach fails.</p></li>
</ul>
<ul>
<li><p>Solution: <span class="orange">generalized posterior distributions</span>, sometimes called <span class="blue">Gibbs-posteriors</span>.</p></li>
<li><p>This is a <span class="blue">lively recent topic</span>, see for instance: <span class="citation" data-cites="chernozhukov2003mcmc">Chernozhukov and Hong (<a href="#ref-chernozhukov2003mcmc" role="doc-biblioref">2003</a>)</span>; <span class="citation" data-cites="bissiri2016general">Bissiri et al. (<a href="#ref-bissiri2016general" role="doc-biblioref">2016</a>)</span> <span class="citation" data-cites="heide2020safe">Heide et al. (<a href="#ref-heide2020safe" role="doc-biblioref">2020</a>)</span>; <span class="citation" data-cites="grunwald2020fast">Grünwald and Mehta (<a href="#ref-grunwald2020fast" role="doc-biblioref">2020</a>)</span>; <span class="citation" data-cites="knoblauch2022optimization">Knoblauch et al. (<a href="#ref-knoblauch2022optimization" role="doc-biblioref">2022</a>)</span>; <span class="citation" data-cites="matsubara2022robust">Matsubara et al. (<a href="#ref-matsubara2022robust" role="doc-biblioref">2022</a>)</span>; <span class="citation" data-cites="matsubara2023generalized">Matsubara et al. (<a href="#ref-matsubara2023generalized" role="doc-biblioref">2023</a>)</span>; <span class="citation" data-cites="jewson_rossell_2022">Jewson and Rossell (<a href="#ref-jewson_rossell_2022" role="doc-biblioref">2022</a>)</span>; <span class="citation" data-cites="rigon2023generalized">Rigon et al. (<a href="#ref-rigon2023generalized" role="doc-biblioref">2023</a>)</span>; <span class="citation" data-cites="Agnoletto2025">Agnoletto et al. (<a href="#ref-Agnoletto2025" role="doc-biblioref">2025</a>)</span>.</p></li>
</ul>
</section>
<section id="generalizations-of-the-hypothetical-approach" class="level2">
<h2 class="anchored" data-anchor-id="generalizations-of-the-hypothetical-approach">Generalizations of the hypothetical approach</h2>
<ul>
<li><span class="citation" data-cites="bissiri2016general">Bissiri et al. (<a href="#ref-bissiri2016general" role="doc-biblioref">2016</a>)</span> showed that the <span class="blue">generalized posterior</span> <span class="math display">
\pi_\omega(\boldsymbol{\theta} \mid y_{1:n}) \propto \pi(\boldsymbol{\theta}) \exp\left\{ - \omega \sum_{i=1}^n \ell(\boldsymbol{\theta}; y_i) \right\},
</span> is the only coherent update of the prior beliefs about <span class="math display">
\boldsymbol{\theta}^* = \arg\min_{\boldsymbol{\theta}\in\Theta} \int_{\mathcal{Y}} \ell(\boldsymbol{\theta}; y)\, F_0(\mathrm{d}y),
</span> where <span class="math inline">\ell(\boldsymbol{\theta}, y)</span> is a <span class="orange">loss function</span>, <span class="math inline">\omega</span> is the <span class="grey">loss-scale</span>, and <span class="math inline">F_0</span> is the unknown true sampling distribution.</li>
</ul>
<ul>
<li><p>Learning the loss scale <span class="math inline">\omega</span> from the data is a <span class="orange">delicate</span> task. Assuming a prior for <span class="math inline">\omega</span> can lead to degenerate estimates if not accompanied by additional adjustments to the loss function.</p></li>
<li><p>However, there are several solutions for its calibration: <span class="citation" data-cites="holmes2017assigning">Holmes and Walker (<a href="#ref-holmes2017assigning" role="doc-biblioref">2017</a>)</span>; <span class="citation" data-cites="lyddon2019general">Lyddon et al. (<a href="#ref-lyddon2019general" role="doc-biblioref">2019</a>)</span>; <span class="citation" data-cites="syring2019calibrating">Syring and Martin (<a href="#ref-syring2019calibrating" role="doc-biblioref">2019</a>)</span>; <span class="citation" data-cites="matsubara2023generalized">Matsubara et al. (<a href="#ref-matsubara2023generalized" role="doc-biblioref">2023</a>)</span>.</p></li>
</ul>
</section>
<section id="predictive-approach" class="level2">
<h2 class="anchored" data-anchor-id="predictive-approach">Predictive approach</h2>
<ul>
<li>Taking a <span class="blue">predictive approach</span>, one can implicitly <span class="orange">characterize the prior</span> via de Finetti theorem by specifying the sequence of predictive distributions of an exchangeable sequence: <span class="math display">
P_n(A) := \mathbb{P}(Y_{n+1}\in A\mid y_{1:n}), \qquad n \ge 1.
</span> This leads to an exchangeable sequences iff the conditions in <span class="citation" data-cites="fortini2000exchangeability">Fortini et al. (<a href="#ref-fortini2000exchangeability" role="doc-biblioref">2000</a>)</span> are satisfied.</li>
</ul>
<ul>
<li><span class="orange">Example</span>: the predictive construction of a Dirichlet process prior is such that <span class="math inline">Y_1\sim P_0</span> and <span class="math inline">Y_{n+1}\mid y_{1:n}\sim P_n</span> for <span class="math inline">n\ge 1</span>, where <span class="math display">
P_n(A) = \frac{\alpha}{\alpha+n} P_0(A) + \frac{1}{\alpha+n}\sum_{i=1}^n\mathbb{1}(y_i\in A),
</span> for any measurable set <span class="math inline">A</span>.</li>
</ul>
<ul>
<li>The possibility of specifying a sequence of one-step-ahead predictive distributions is appealing:
<ul>
<li>it bypasses direct elicitation of the prior;</li>
<li>it explicitly connects prediction and inference (see the next slide);</li>
</ul></li>
</ul>
</section>
<section id="connecting-inference-and-prediction-i" class="level2">
<h2 class="anchored" data-anchor-id="connecting-inference-and-prediction-i">Connecting inference and prediction I</h2>
<ul>
<li><p>The posterior <span class="math inline">P \mid y_{1:n}</span> is usually obtained through Bayes theorem, but this is not the only way.</p></li>
<li><p>We can <span class="blue">characterize</span> both <span class="blue">prior</span> and <span class="orange">posterior</span> of <span class="math inline">P</span> through the predictive distributions <span class="math inline">P_n</span>, which indeed contains all the necessary information.</p></li>
</ul>
<ul>
<li>If <span class="math inline">(Y_n)_{n \ge 1}</span> is exchangeable, then the <span class="blue">prior</span> and <span class="blue">posterior mean</span> of <span class="math inline">P</span> coincide with the predictive: <span class="math display">
P_0(A) = \mathbb{P}(Y_1\in A) = \mathbb{E}\{P(A)\}, \qquad P_n(A) = \mathbb{P}(Y_{n+1}\in A\mid y_{1:n}) = \mathbb{E}(P(A) \mid y_{1:n}), \qquad n \ge 1.
</span></li>
</ul>
<ul>
<li>A deeper result holds, which is a corollary of Finetti theorem <span class="citation" data-cites="fortini2012predictive">(<a href="#ref-fortini2012predictive" role="doc-biblioref">Fortini and Petrone 2012</a>)</span>.</li>
</ul>
<div class="callout callout-style-simple callout-warning no-icon callout-titled" title="De Finetti's representation theorem (predictive form)">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
De Finetti’s representation theorem (predictive form)
</div>
</div>
<div class="callout-body-container callout-body">
<p>Let <span class="math inline">(Y_n)_{n \ge 1}</span> be an exchangeable sequence with predictive distributions <span class="math inline">(P_n)_{n \ge 1}</span>. Then <span class="math inline">P_n</span> converges weakly (a.s. <span class="math inline">\mathbb{P}</span>) to a random probability measure <span class="math inline">P</span> distributed according to <span class="math inline">\Pi</span> as <span class="math inline">n \to \infty</span>.</p>
</div>
</div>
</section>
<section id="connecting-inference-and-prediction-ii" class="level2">
<h2 class="anchored" data-anchor-id="connecting-inference-and-prediction-ii">Connecting inference and prediction II</h2>
<ul>
<li>In other words, the sequence of predictive distributions <span class="math inline">P_n</span> converges to a random probability measure <span class="math inline">P</span> with <span class="orange">prior</span> distribution <span class="math inline">\Pi</span>. The <span class="blue">source of randomness</span> is the data<br>
<span class="math display">
Y_1, Y_2, Y_3, \dots
</span> Intuitively, before observing the data, our predictions eventually reflect the prior.</li>
</ul>
<ul>
<li><p>Given <span class="math inline">y_{1:n}</span>, the sequence <span class="math inline">P_{n+m}</span> converges weakly (a.s. <span class="math inline">\mathbb{P}</span>) as <span class="math inline">m \to \infty</span> to a random probability measure with <span class="orange">posterior</span> distribution <span class="math inline">\Pi(\cdot \mid y_{1:n})</span>. The source of randomness is the <span class="blue">future data</span><br>
<span class="math display">
Y_{n+1}, Y_{n+2}, Y_{n+3}, \dots .
</span></p></li>
<li><p>This provides a natural <span class="blue">alternative interpretation</span> of the posterior distribution <span class="math inline">P \mid y_{1:n}</span> and a <span class="orange">practical algorithm</span> for sampling from it, called <span class="grey">predictive resampling</span>.</p></li>
<li><p>Intuitively, posterior uncertainty arises from <span class="orange">lack of knowledge</span> about future observations. If we knew them, the posterior would collapse to a point mass (Bayesian consistency).</p></li>
</ul>
<ul>
<li>This reasoning is at the heart of <span class="blue">martingale posteriors</span> <span class="citation" data-cites="fong_holmes_2023">(<a href="#ref-fong_holmes_2023" role="doc-biblioref">Fong et al. 2023</a>)</span>.</li>
</ul>
</section>
<section id="generalizations-of-the-predictive-approach" class="level2">
<h2 class="anchored" data-anchor-id="generalizations-of-the-predictive-approach">Generalizations of the predictive approach</h2>
<ul>
<li>Defining a sequence of predictive laws <span class="math inline">P_n</span> that guarantees exchangeability—i.e., satisfies the two-step-ahead conditions of <span class="citation" data-cites="fortini2000exchangeability">Fortini et al. (<a href="#ref-fortini2000exchangeability" role="doc-biblioref">2000</a>)</span>—is a <span class="orange">difficult</span> task in practice.</li>
</ul>
<ul>
<li><p>Solution: replace exchangeability with the weaker requirement that <span class="math inline">(Y_n)_{n\ge1}</span> is <span class="blue">conditionally identically distributed</span> (CID), also known as <span class="orange">martingale posteriors</span>.</p></li>
<li><p>The CID condition requires <span class="math display">
\mathbb{P}(Y_{n+k} \in \cdot \mid y_{1:n}) = \mathbb{P}(Y_{n+1} \in \cdot \mid y_{1:n}) = P_n(\cdot), \qquad \text{for all} \quad  k \ge 1, \; n \ge 1.
</span> It is sufficient to verify this condition for <span class="math inline">k = 1</span> in order to ensure its validity for all <span class="math inline">k  \ge 1</span>.</p></li>
<li><p>Equivalently, <span class="citation" data-cites="fong_holmes_2023">Fong et al. (<a href="#ref-fong_holmes_2023" role="doc-biblioref">2023</a>)</span> express the above condition in a way that emphasizes the <span class="orange">martingale property</span> of the predictive distributions <span class="math display">
\mathbb{E}\{P_{n+1}(\cdot) \mid y_{1:n}\} = P_n(\cdot), \qquad n \ge 1.
</span></p></li>
</ul>
<ul>
<li>This is another <span class="blue">lively recent topic</span>: <span class="citation" data-cites="fortini_petrone_2020">Fortini and Petrone (<a href="#ref-fortini_petrone_2020" role="doc-biblioref">2020</a>)</span>; <span class="citation" data-cites="berti_pratelli_2021">Berti et al. (<a href="#ref-berti_pratelli_2021" role="doc-biblioref">2021</a>)</span>; <span class="citation" data-cites="fortini2021predictive">Fortini et al. (<a href="#ref-fortini2021predictive" role="doc-biblioref">2021</a>)</span>; <span class="citation" data-cites="berti2023without">Berti et al. (<a href="#ref-berti2023without" role="doc-biblioref">2023a</a>)</span>; <span class="citation" data-cites="berti2023kernel">Berti et al. (<a href="#ref-berti2023kernel" role="doc-biblioref">2023b</a>)</span>; <span class="citation" data-cites="fong_holmes_2023">Fong et al. (<a href="#ref-fong_holmes_2023" role="doc-biblioref">2023</a>)</span>; <span class="citation" data-cites="fong2024asymptotics">Fong and Yiu (<a href="#ref-fong2024asymptotics" role="doc-biblioref">2024a</a>)</span>; <span class="citation" data-cites="fong2024bayesian">Fong and Yiu (<a href="#ref-fong2024bayesian" role="doc-biblioref">2024b</a>)</span>; <span class="citation" data-cites="cui2024martingale">Cui and Walker (<a href="#ref-cui2024martingale" role="doc-biblioref">2024</a>)</span>; <span class="citation" data-cites="fortini2024exchangeability">Fortini and Petrone (<a href="#ref-fortini2024exchangeability" role="doc-biblioref">2024</a>)</span> and more…</li>
</ul>
</section>
<section id="nonparametric-modelling-of-count-data" class="level2">
<h2 class="anchored" data-anchor-id="nonparametric-modelling-of-count-data">Nonparametric modelling of count data</h2>
<ul>
<li>Bayesian nonparametric modeling of <span class="blue">counts distributions</span> is a challenging task. Nonparametric mixtures of discrete kernels <span class="citation" data-cites="canale2011bayesian">(<a href="#ref-canale2011bayesian" role="doc-biblioref">Canale and Dunson 2011</a>)</span> can be cumbersome in practice.</li>
</ul>
<ul>
<li>Alternatively, one could directly specify a <span class="orange">DP prior</span> on the data generator as <span class="math display">
  Y_i\mid P \overset{\textup{iid}}{\sim} P,\quad P\sim\mathrm{DP}(\alpha, P_0),
</span> for <span class="math inline">Y_i\in\mathcal{Y}=\{0,1,\ldots\}</span>, <span class="math inline">i=1,\ldots,n</span>, where <span class="math inline">\alpha</span> is the precision parameter and <span class="math inline">P_0</span> a base parametric distribution, such as a Poisson.</li>
</ul>
<ul>
<li><p>The Dirichlet process is mathematically convenient. However, the corresponding posterior <span class="orange">lacks smoothing</span>, which can lead to poor performance.</p></li>
<li><p>Within the hypothetical approach, it is unclear how to specify a nonparametric process with the same simplicity and flexibility as the DP prior while allowing for smoothing.</p></li>
</ul>
<ul>
<li><span class="blue">Our proposal</span>: a predictive sequence tailored to count data inspired by kernel density estimators.</li>
</ul>
</section>
<section id="illustrative-example-i" class="level2">
<h2 class="anchored" data-anchor-id="illustrative-example-i">Illustrative example I</h2>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="img/example_plot.jpg" class="nostretch img-fluid quarto-figure quarto-figure-center figure-img" width="700"></p>
</figure>
</div>
<ul>
<li><span class="orange">Left plot</span>: posterior mean of a DP. <span class="blue">Right plot</span>: posterior mean of the proposed MAD sequence.</li>
</ul>
</section>
<section id="a-recursive-predictive-rule" class="level2">
<h2 class="anchored" data-anchor-id="a-recursive-predictive-rule">A recursive predictive rule</h2>
<ul>
<li>Intuitively, a better estimator would be obtained by replacing the indicator <span class="math inline">\mathbb{1}(\cdot)</span> of the DP predictive scheme with a <span class="orange">kernel</span> that allows the <span class="blue">borrowing of information</span> between nearby values.</li>
</ul>
<ul>
<li><p>Let <span class="math inline">Y_1\sim P_0</span> and <span class="math inline">Y_{n+1} \mid y_{1:n} \sim P_n</span> for <span class="math inline">n\ge 1</span>, and let <span class="math inline">K_n(\cdot \mid y_n)</span> be a sequence of transition kernels. We define the predictive distribution <span class="orange">recursively</span>: <span class="math display">
  P_n(\cdot) = \mathbb{P}(Y_{n+1}\in \cdot\mid y_{1:n}) = (1-w_n) P_{n-1}(\cdot) + w_n K_{n}(\cdot\mid y_n), \qquad n \ge 1,
</span> where <span class="math inline">(w_n)_{n\ge1}</span> are decreasing weights such that <span class="math inline">w_n\in(0,1)</span>, <span class="math inline">\sum_{n\ge1}w_n = \infty</span>, and <span class="math inline">\sum_{n\ge1}w_n^2&lt;\infty</span>.</p></li>
<li><p>The choice of weights <span class="math inline">w_n = (\alpha +n)^{-1}</span> gives the following DP-like predictive rule <span class="math display">
P_n(\cdot) = \frac{\alpha}{\alpha + n} P_0(\cdot) + \frac{1}{\alpha + n}\sum_{i=1}^nK_i(\cdot\mid y_i).
</span> Hence, the predictive law of a DP is a special case whenever <span class="math inline">K_i(\cdot \mid y_i) = \delta_{y_i}(\cdot)</span>.</p></li>
</ul>
<div class="callout callout-style-simple callout-tip no-icon">
<div class="callout-body d-flex">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-body-container">
<p>The above sequence, beyond the DP special case, is <span class="orange">not exchangeable</span> and it will <span class="blue">depend on the order</span> of the data. Moreover, without further restrictions, is not necessarily CID!</p>
</div>
</div>
</div>
</section>
<section id="metropolis-adjusted-dirichlet-mad-sequences" class="level2">
<h2 class="anchored" data-anchor-id="metropolis-adjusted-dirichlet-mad-sequences">Metropolis-adjusted Dirichlet (MAD) sequences</h2>
<ul>
<li><p>We assume that <span class="math inline">K_{n}(\cdot\mid y_n)</span> is a <span class="blue">Metropolis-Hastings kernel</span> centered in <span class="math inline">y_n</span> having pmf: <span class="math display">
  k_{n}(y\mid y_n) = \gamma_{n}(y,y_n) k_*(y\mid y_n) + \mathbb{1}(y=y_n)\Big[\sum_{z\in\mathcal{Y}}\big\{1-\gamma_{n}(z,y_n)\big\}k_*(z\mid y_n)\Big],
</span> with acceptance probability <span class="math display">
  \gamma_{n}(y,y_n) = \gamma(y, y_n, P_{n-1}) =
  \min\left\{1,\frac{p_{n-1}(y) k_*(y_n\mid y)}{p_{n-1}(y_n) k_*(y\mid y_n)}\right\},
</span> where <span class="math inline">p_{n-1}</span> is the probability mass functions associated to <span class="math inline">P_{n-1}</span> and <span class="math inline">k_*(\cdot\mid y_n)</span> is the pmf of a <span class="orange">discrete base kernel</span> centered at <span class="math inline">y_n</span>.</p></li>
<li><p>We refer to <span class="math inline">P_n</span> above as the <span class="blue">Metropolis-adjusted Dirichlet</span> (MAD) distribution with weights <span class="math inline">(w_n)_{n\ge1}</span>, base kernel <span class="math inline">k_*</span> and initial distribution <span class="math inline">P_0</span>. We call <span class="math inline">(Y_n)_{n \ge 1}</span> a MAD sequence.</p></li>
</ul>
<div class="callout callout-style-simple callout-warning no-icon callout-titled" title="Theorem (Agnoletto, R. and Dunson, 2025)">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Theorem (Agnoletto, R. and Dunson, 2025)
</div>
</div>
<div class="callout-body-container callout-body">
<p>Let <span class="math inline">(Y_n)_{n\ge1}</span> be a MAD sequence. Then, for every set of weights <span class="math inline">(w_n)_{n \ge 1}</span>, discrete base kernel <span class="math inline">k_*</span>, and initial distribution <span class="math inline">P_0</span>, the sequence <span class="math inline">(Y_n)_{n\ge1}</span> is <span class="blue">conditionally identically distributed (CID)</span>.</p>
</div>
</div>
</section>
<section id="bayesian-properties-of-cid-sequences-i" class="level2">
<h2 class="anchored" data-anchor-id="bayesian-properties-of-cid-sequences-i">Bayesian properties of CID sequences I</h2>
<div class="callout callout-style-simple callout-warning no-icon callout-titled" title="Corollary [@aldous1985exchangeability; @berti2004limit]">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Corollary <span class="citation" data-cites="aldous1985exchangeability berti2004limit">(<a href="#ref-aldous1985exchangeability" role="doc-biblioref">Aldous 1985</a>; <a href="#ref-berti2004limit" role="doc-biblioref">Berti et al. 2004</a>)</span>
</div>
</div>
<div class="callout-body-container callout-body">
<p>Consider a MAD sequence <span class="math inline">(Y_n)_{n\ge1}</span>. Then, <span class="math inline">\mathbb{P}</span>-a.s.,</p>
<p><span class="math inline">(a)</span> The sequence is asymptotically exchangeable, that is <span class="math display">
    (Y_{n+1}, Y_{n+2}, \ldots) \overset{\textup{d}}{\longrightarrow} (Z_1, Z_2, \ldots), \qquad n \rightarrow \infty,
</span> where <span class="math inline">(Z_1,Z_2,\ldots)</span> is an exchangeable sequence with directing random probability measure <span class="math inline">P</span>;</p>
<p><span class="math inline">(b)</span> the corresponding sequence of predictive distributions <span class="math inline">(P_n)_{n\ge1}</span> weakly converge to a random probability measures <span class="math inline">P</span> (a.s. <span class="math inline">\mathbb{P}</span>).</p>
</div>
</div>
<ul>
<li><p>An <span class="blue">asymptotic</span> equivalent of <span class="blue">de Finetti</span>’s theorem holds: each MAD sequence has a corresponding unique prior on <span class="math inline">P</span>.</p></li>
<li><p>The ordering dependence will vanish asymptotically and, informally, <span class="math inline">Y_i \mid P \overset{\mathrm{iid}}{\sim} P</span> <span class="blue">for large <span class="math inline">n</span></span>.</p></li>
<li><p>The random probability measure <span class="math inline">P</span> exists and is defined as the limit of the predictive distributions. However, it is <span class="orange">not available explicitly</span>.</p></li>
</ul>
</section>
<section id="bayesian-properties-of-cid-sequences-ii" class="level2">
<h2 class="anchored" data-anchor-id="bayesian-properties-of-cid-sequences-ii">Bayesian properties of CID sequences II</h2>
<div class="callout callout-style-simple callout-warning no-icon callout-titled" title="Corollary [@aldous1985exchangeability; @berti2004limit]">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Corollary <span class="citation" data-cites="aldous1985exchangeability berti2004limit">(<a href="#ref-aldous1985exchangeability" role="doc-biblioref">Aldous 1985</a>; <a href="#ref-berti2004limit" role="doc-biblioref">Berti et al. 2004</a>)</span>
</div>
</div>
<div class="callout-body-container callout-body">
<p>Let <span class="math inline">\theta = P(f) = \sum_{y \in \mathcal{Y}} f(y) p(y)</span> and analogously <span class="math inline">\theta_n = P_n(f) = \sum_{y \in \mathcal{Y}} f(y) p_n(y)</span> be any functional of interest. Consider a MAD sequence <span class="math inline">(Y_n)_{n\ge1}</span>.</p>
<p>Then, <span class="math inline">\mathbb{P}</span>-a.s., for every <span class="math inline">n\ge1</span> and every integrable function <span class="math inline">f:\mathcal{Y}\rightarrow\mathbb{R}</span>, we have <span class="math display">
\mathbb{E}(\theta \mid y_{1:n}) = \mathbb{E}\{P(f) \mid y_{1:n}\} = P_n(f) = \theta_n
</span></p>
</div>
</div>
<ul>
<li><p>Broadly speaking, the posterior mean of any functional of interest of <span class="math inline">P</span> coincides with the functional of the predictive.</p></li>
<li><p>Moreover, <span class="math inline">\mathbb{E}\{P(f)\}= P_0(f) = \theta_0</span> for every integrable function <span class="math inline">f</span>, so that <span class="math inline">P_0</span> retains the role of a <span class="blue">base measure</span> as for standard Dirichlet sequences, providing an initial guess at <span class="math inline">P</span>.</p></li>
</ul>
<ul>
<li>Uncertainty quantification for <span class="math inline">\theta=P(f)</span> is carried out by <span class="blue">predictive resampling</span> <span class="citation" data-cites="fong_holmes_2023">(<a href="#ref-fong_holmes_2023" role="doc-biblioref">Fong et al. 2023</a>)</span>.</li>
</ul>
</section>
<section id="predictive-resampling-for-mad-sequences" class="level2">
<h2 class="anchored" data-anchor-id="predictive-resampling-for-mad-sequences">Predictive resampling for MAD sequences</h2>
<p><strong>Algorithm <span class="citation" data-cites="fortini_petrone_2020">(<a href="#ref-fortini_petrone_2020" role="doc-biblioref">Fortini and Petrone 2020</a>)</span>:</strong></p>
<ol type="1">
<li>Compute <span class="math inline">P_n(\cdot)</span> from the observed data <span class="math inline">y_{1:n}</span></li>
<li>Set <span class="math inline">N\gg n</span></li>
<li>For <span class="math inline">j = 1,\ldots,B</span>
<ol type="a">
<li>For <span class="math inline">i=n+1,\ldots,N</span>
<ol type="i">
<li>Sample <span class="math inline">Y_i\mid y_{1:i-1}\sim P_{i-1}</span></li>
<li>Update <span class="math inline">P_i(\cdot) = (1-w_i)P_{i-1}(\cdot) + w_i K_{i-1}(\cdot\mid y_i)</span></li>
</ol></li>
<li>End For</li>
</ol></li>
<li>End For</li>
<li>Return <span class="math inline">P_N^{(1)}(\cdot),\ldots,P_N^{(B)}(\cdot)</span>, an iid sample from the distribution of <span class="math inline">P_N(\cdot)\mid y_{1:n}</span></li>
</ol>
</section>
<section id="on-the-choice-of-the-base-kernel" class="level2">
<h2 class="anchored" data-anchor-id="on-the-choice-of-the-base-kernel">On the choice of the base kernel</h2>
<ul>
<li>In principle, any discrete distribution can be chosen as the base kernel <span class="math inline">k_*</span>. However, it is natural to consider choices that allow the kernel to be centered at <span class="math inline">y_n</span> while permitting control over the variance.</li>
</ul>
<ul>
<li>We consider a <span class="blue">rounded Gaussian</span> distribution centered in <span class="math inline">y_n</span>, with pmf <span class="math display">
  k_*(y\mid y_n, \sigma) = \frac{\int_{y-1/2}^{y+1/2}\mathcal{N}(t\mid y_n, \sigma^2) \mathrm{d}t}{\sum_{z\in\mathcal{Y}} \int_{z-1/2}^{z+1/2}\mathcal{N}(t\mid y_n, \sigma^2) \mathrm{d}t},
</span> for <span class="math inline">n\ge1</span>, where <span class="math inline">\mathcal{N}(\cdot\mid y_n,\sigma^2)</span> denotes a normal density function with mean <span class="math inline">y_n</span> and variance <span class="math inline">\sigma^2</span>.</li>
</ul>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="img/ker_plot_slide.jpg" class="nostretch img-fluid quarto-figure quarto-figure-center figure-img" width="1500"></p>
</figure>
</div>
</section>
<section id="uncertainty-quantification-and-calibration" class="level2">
<h2 class="anchored" data-anchor-id="uncertainty-quantification-and-calibration">Uncertainty quantification and calibration</h2>
<ul>
<li>It can be shown that the <span class="blue">asymptotic distribution</span> of <span class="math inline">P(A)\mid y_{1:n}</span> is <span class="math inline">\mathcal{N}(P_n(A), \Sigma_n r_n^{-1})</span> for <span class="math inline">n</span> large, where the <span class="orange">variance</span> is <span class="math display">
\Sigma_n r_n^{-1} \approx \mathbb{E}\{[P_{n+1}(A)-P_n(A)]^2\mid y_{1:n}\}\sum_{k&gt;n+1}w_k^2.
</span></li>
</ul>
<ul>
<li><p>Weights that decay to zero quickly induce fast learning and convergence to the asymptotic exchangeable regime.</p></li>
<li><p>However, small values of <span class="math inline">w_n</span> leads to <span class="orange">underestimation</span> of the <span class="orange">posterior variability</span>.</p></li>
</ul>
<ul>
<li><p>In practice, possible choices are <span class="math inline">w_n=(\alpha+n)^{-1}</span> (i.e.&nbsp;DP-like sequence), <span class="math inline">w_n=(\alpha+n)^{-2/3}</span> <span class="citation" data-cites="martin2009asymptotic">(<a href="#ref-martin2009asymptotic" role="doc-biblioref">Martin and Tokdar 2009</a>)</span>, and <span class="math inline">w_n=(2 - n^{-1})(n+1)^{-1}</span> <span class="citation" data-cites="fong_holmes_2023">(<a href="#ref-fong_holmes_2023" role="doc-biblioref">Fong et al. 2023</a>)</span>.</p></li>
<li><p>We consider <span class="blue">adaptive weights</span> that we found have good empirical properties: <span class="math display">
  w_n=(\alpha+n)^{-\lambda_n}, \qquad \lambda_n=\lambda+(1+\lambda)\exp\bigg\{-\frac{1}{N_*}n\bigg\},
</span> with <span class="math inline">\lambda\in(0.5,1]</span>, <span class="math inline">N_*&gt;0</span>.</p></li>
</ul>
</section>
<section id="illustrative-examples-ii" class="level2">
<h2 class="anchored" data-anchor-id="illustrative-examples-ii">Illustrative examples II</h2>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="img/example_complete.jpg" class="nostretch img-fluid quarto-figure quarto-figure-center figure-img" width="850"></p>
</figure>
</div>
</section>
<section id="multivariate-count-and-binary-data" class="level2">
<h2 class="anchored" data-anchor-id="multivariate-count-and-binary-data">Multivariate count and binary data</h2>
<ul>
<li><p>Extending MAD sequences for <span class="blue">multivariate</span> data is straightforward using a <span class="blue">factorized</span> base kernel <span class="math display">
k_*(\bm y\mid\bm y_n) = \prod_{j=1}^d k_*(y_j\mid y_{n,j}),
</span> with <span class="math inline">\bm y=(y_1,\ldots,y_d)</span> and <span class="math inline">\bm y_n=(y_{n,1},\ldots,y_{n,d})</span>.</p></li>
<li><p>In particular, MAD sequences can be employed for modeling <span class="orange">multivariate binary data</span> using an appropriate base kernel.</p></li>
<li><p>A natural step further is to use MAD sequences for <span class="blue">nonparametric regression</span> and <span class="orange">classification</span>.</p></li>
</ul>
<div class="callout callout-style-simple callout-note no-icon">
<div class="callout-body d-flex">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-body-container">
<p>Multivariate MAD sequences do not depend on the ordering of the variables, unlike most other martingale posteriors.</p>
</div>
</div>
</div>
</section>
<section id="simulations-i" class="level2">
<h2 class="anchored" data-anchor-id="simulations-i">Simulations I</h2>
<p>Out-of-sample prediction accuracy evaluated in terms of MSE and AUC for <span class="blue">regression</span> and <span class="orange">classification</span>, respectively, in specific simulation studies.</p>
<table class="caption-top table">
<colgroup>
<col style="width: 14%">
<col style="width: 21%">
<col style="width: 19%">
<col style="width: 2%">
<col style="width: 21%">
<col style="width: 19%">
</colgroup>
<thead>
<tr class="header">
<th></th>
<th>Regress. (MSE)</th>
<th></th>
<th></th>
<th>Classific. (AUC)</th>
<th></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td></td>
<td><strong><span class="math inline">n=40</span></strong></td>
<td><strong><span class="math inline">n=80</span></strong></td>
<td></td>
<td><strong><span class="math inline">n=150</span></strong></td>
<td><strong><span class="math inline">n=300</span></strong></td>
</tr>
<tr class="even">
<td>GLM</td>
<td>120.77 [51.51]</td>
<td>94.93 [8.37]</td>
<td></td>
<td>0.796 [0.014]</td>
<td>0.809 [0.007]</td>
</tr>
<tr class="odd">
<td>BART</td>
<td>101.17 [12.69]</td>
<td><strong>74.17</strong> [10.00]</td>
<td></td>
<td>0.863 [0.026]</td>
<td><strong>0.932</strong> [0.009]</td>
</tr>
<tr class="even">
<td>RF</td>
<td><strong>99.98</strong> [7.45]</td>
<td>87.75 [6.53]</td>
<td></td>
<td><strong>0.882</strong> [0.025]</td>
<td>0.913 [0.015]</td>
</tr>
<tr class="odd">
<td>DP</td>
<td>1450.21 [5.53]</td>
<td>1395.61 [8.72]</td>
<td></td>
<td>0.644 [0.011]</td>
<td>0.724 [0.012]</td>
</tr>
<tr class="even">
<td>MAD-1</td>
<td>91.07 [10.35]</td>
<td>73.96 [7.60]</td>
<td></td>
<td>0.873 [0.014]</td>
<td>0.899 [0.008]</td>
</tr>
<tr class="odd">
<td>MAD-2/3</td>
<td>88.83 [13.00]</td>
<td>73.18 [9.58]</td>
<td></td>
<td>0.869 [0.015]</td>
<td>0.899 [0.009]</td>
</tr>
<tr class="even">
<td>MAD-DPM</td>
<td><strong>87.41</strong> [12.36]</td>
<td><strong>72.07</strong> [9.48]</td>
<td></td>
<td>0.872 [0.014]</td>
<td><strong>0.901</strong> [0.008]</td>
</tr>
<tr class="odd">
<td>MAD-ADA</td>
<td>90.61 [10.28]</td>
<td>73.45 [7.69]</td>
<td></td>
<td><strong>0.874</strong> [0.014]</td>
<td>0.900 [0.008]</td>
</tr>
</tbody>
</table>
</section>
<section id="application" class="level2">
<h2 class="anchored" data-anchor-id="application">Application</h2>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="img/pl_appl_slide_1.jpg" class="img-fluid quarto-figure quarto-figure-center figure-img"></p>
</figure>
</div>
<ul>
<li>We analyze the <span class="blue">occurrence</span> rates of 4 species corvids in Finland in year 2009 across different temperatures and habitats.</li>
</ul>
</section>
<section id="application-ii" class="level2">
<h2 class="anchored" data-anchor-id="application-ii">Application II</h2>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="img/pl_appl_slide_2.jpg" class="img-fluid quarto-figure quarto-figure-center figure-img"></p>
</figure>
</div>
</section>
<section id="thank-you" class="level2">
<h2 class="anchored" data-anchor-id="thank-you">Thank you!</h2>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="img/QR.png" class="img-fluid quarto-figure quarto-figure-center figure-img" style="width:2.5in"></p>
</figure>
</div>
<p>The <span class="orange">main paper</span> is:</p>
<p>Agnoletto, D., Rigon, T., and Dunson D.B. (2025+). Nonparametric predictive inference for discrete data via Metropolis-adjusted Dirichlet sequences. <em>arXiv:2507.08629</em></p>
</section>
<section id="references" class="level2 unnumbered smaller">
<h2 class="unnumbered smaller anchored" data-anchor-id="references">References</h2>
<div id="refs" class="references csl-bib-body hanging-indent" data-entry-spacing="0" role="list">
<div id="ref-Agnoletto2025" class="csl-entry" role="listitem">
Agnoletto, D., Rigon, T., and Dunson, D. B. (2025), <span>“Bayesian inference for generalized linear models via quasi-posteriors,”</span> <em>Biometrika</em>, 112.
</div>
<div id="ref-aldous1985exchangeability" class="csl-entry" role="listitem">
Aldous, D. J. (1985), <span>“Exchangeability and related topics, ecole d’et<span>é</span> de saint-flour XIII, lectures notes n 1117,”</span> Springer Verlag.
</div>
<div id="ref-berti2023kernel" class="csl-entry" role="listitem">
Berti, P., Dreassi, E., Leisen, F., Pratelli, L., and Rigo, P. (2023b), <span>“Kernel based dirichlet sequences,”</span> <em>Bernoulli</em>, Bernoulli Society for Mathematical Statistics; Probability, 29, 1321–1342.
</div>
<div id="ref-berti2023without" class="csl-entry" role="listitem">
Berti, P., Dreassi, E., Leisen, F., Pratelli, L., and Rigo, P. (2023a), <span>“Bayesian predictive inference without a prior,”</span> <em>Statistica Sinica</em>, Academia Sinica, Institute of Statistical Science, 34, 2405–2429.
</div>
<div id="ref-berti_pratelli_2021" class="csl-entry" role="listitem">
Berti, P., Dreassi, E., Pratelli, L., and Rigo, P. (2021), <span>“A class of models for <span>B</span>ayesian predictive inference,”</span> <em>Bernoulli</em>, 27, 702–726.
</div>
<div id="ref-berti2004limit" class="csl-entry" role="listitem">
Berti, P., Pratelli, L., and Rigo, P. (2004), <span>“Limit theorems for a class of identically distributed random variables,”</span> 32, 2029–2052.
</div>
<div id="ref-bissiri2016general" class="csl-entry" role="listitem">
Bissiri, P. G., Holmes, C. C., and Walker, S. G. (2016), <span>“A general framework for updating belief distributions,”</span> <em>Journal of the Royal Statistical Society: Series B (Statistical Methodology)</em>, Wiley Online Library, 78, 1103–1130.
</div>
<div id="ref-canale2011bayesian" class="csl-entry" role="listitem">
Canale, A., and Dunson, D. B. (2011), <span>“Bayesian kernel mixtures for counts,”</span> <em>Journal of the American Statistical Association</em>, Taylor &amp; Francis, 106, 1528–1539.
</div>
<div id="ref-chernozhukov2003mcmc" class="csl-entry" role="listitem">
Chernozhukov, V., and Hong, H. (2003), <span>“An <span>MCMC</span> approach to classical estimation,”</span> <em>Journal of econometrics</em>, Elsevier, 115, 293–346.
</div>
<div id="ref-cui2024martingale" class="csl-entry" role="listitem">
Cui, F., and Walker, S. G. (2024), <span>“Martingale posterior distributions for log-concave density functions,”</span> <em>arXiv preprint arXiv:2401.14515</em>.
</div>
<div id="ref-de1937prevision" class="csl-entry" role="listitem">
De Finetti, B. (1937), <span>“La pr<span>é</span>vision: Ses lois logiques, ses sources subjectives,”</span> in <em>Annales de l’institut henri poincar<span>é</span></em>, pp. 1–68.
</div>
<div id="ref-fong_holmes_2023" class="csl-entry" role="listitem">
Fong, E., Holmes, C. C., and Walker, S. G. (2023), <span>“Martingale posterior distributions,”</span> <em>Journal of the Royal Statistical Society Series B: Statistical Methodology</em>, 85, 1357–1391.
</div>
<div id="ref-fong2024bayesian" class="csl-entry" role="listitem">
Fong, E., and Yiu, A. (2024b), <span>“Bayesian quantile estimation and regression with martingale posteriors,”</span> <em>arXiv preprint arXiv:2406.03358</em>.
</div>
<div id="ref-fong2024asymptotics" class="csl-entry" role="listitem">
Fong, E., and Yiu, A. (2024a), <span>“Asymptotics for parametric martingale posteriors,”</span> <em>arXiv preprint arXiv:2410.17692</em>.
</div>
<div id="ref-fortini2000exchangeability" class="csl-entry" role="listitem">
Fortini, S., Ladelli, L., and Regazzini, E. (2000), <span>“Exchangeability, predictive distributions and parametric models,”</span> <em>Sankhya: The Indian Journal of Statistics, Series A</em>, 62, 86–109.
</div>
<div id="ref-fortini2012predictive" class="csl-entry" role="listitem">
Fortini, S., and Petrone, S. (2012), <span>“Predictive construction of priors in bayesian nonparametrics,”</span> <em>Brazilian Journal of Probability and Statistics</em>, 26, 423–449.
</div>
<div id="ref-fortini_petrone_2020" class="csl-entry" role="listitem">
Fortini, S., and Petrone, S. (2020), <span>“Quasi-<span>B</span>ayes properties of a procedure for sequential learning in mixture models,”</span> <em>Journal of the Royal Statistical Society Series B: Statistical Methodology</em>, Oxford University Press, 82, 1087–1114.
</div>
<div id="ref-fortini2024exchangeability" class="csl-entry" role="listitem">
Fortini, S., and Petrone, S. (2024), <span>“Exchangeability, prediction and predictive modeling in bayesian statistics,”</span> <em>arXiv preprint arXiv:2402.10126</em>.
</div>
<div id="ref-fortini2021predictive" class="csl-entry" role="listitem">
Fortini, S., Petrone, S., and Sariev, H. (2021), <span>“Predictive constructions based on measure-valued p<span>ó</span>lya urn processes,”</span> <em>Mathematics</em>, MDPI, 9, 2845.
</div>
<div id="ref-grunwald2020fast" class="csl-entry" role="listitem">
Grünwald, P. D., and Mehta, N. A. (2020), <span>“<span class="nocase">Fast rates for general unbounded loss functions: from ERM to generalized Bayes</span>,”</span> <em>The Journal of Machine Learning Research</em>, JMLRORG, 21, 2040–2119.
</div>
<div id="ref-heide2020safe" class="csl-entry" role="listitem">
Heide, R. de, Kirichenko, A., Grunwald, P., and Mehta, N. (2020), <span>“Safe-bayesian generalized linear regression,”</span> in <em>Proceedings of the twenty third international conference on artificial intelligence and statistics</em>, PMLR, pp. 2623–2633.
</div>
<div id="ref-holmes2017assigning" class="csl-entry" role="listitem">
Holmes, C. C., and Walker, S. G. (2017), <span>“Assigning a value to a power likelihood in a general <span>B</span>ayesian model,”</span> <em>Biometrika</em>, Oxford University Press, 104, 497–503.
</div>
<div id="ref-jewson_rossell_2022" class="csl-entry" role="listitem">
Jewson, J., and Rossell, D. (2022), <span>“General bayesian loss function selection and the use of improper models,”</span> <em>Journal of the Royal Statistical Society Series B: Statistical Methodology</em>, Oxford University Press, 84, 1640–1665.
</div>
<div id="ref-knoblauch2022optimization" class="csl-entry" role="listitem">
Knoblauch, J., Jewson, J., and Damoulas, T. (2022), <span>“An optimization-centric view on bayes’ rule: Reviewing and generalizing variational inference,”</span> <em>Journal of Machine Learning Research</em>, 23, 1–109.
</div>
<div id="ref-lyddon2019general" class="csl-entry" role="listitem">
Lyddon, S. P., Holmes, C. C., and Walker, S. G. (2019), <span>“General <span>B</span>ayesian updating and the loss-likelihood bootstrap,”</span> <em>Biometrika</em>, Oxford University Press, 106, 465–478.
</div>
<div id="ref-martin2009asymptotic" class="csl-entry" role="listitem">
Martin, R., and Tokdar, S. T. (2009), <span>“Asymptotic properties of predictive recursion: Robustness and rate of convergence,”</span> <em>Electornic Journal of Statistics</em>, 3, 1455–1472.
</div>
<div id="ref-matsubara2022robust" class="csl-entry" role="listitem">
Matsubara, T., Knoblauch, J., Briol, F.-X., and Oates, C. J. (2022), <span>“Robust generalised bayesian inference for intractable likelihoods,”</span> <em>Journal of the Royal Statistical Society Series B: Statistical Methodology</em>, Oxford University Press, 84, 997–1022.
</div>
<div id="ref-matsubara2023generalized" class="csl-entry" role="listitem">
Matsubara, T., Knoblauch, J., Briol, F.-X., and Oates, C. J. (2023), <span>“Generalized bayesian inference for discrete intractable likelihood,”</span> <em>Journal of the American Statistical Association</em>, Taylor &amp; Francis, 1–11.
</div>
<div id="ref-rigon2023generalized" class="csl-entry" role="listitem">
Rigon, T., Herring, A. H., and Dunson, D. B. (2023), <span>“A generalized <span>B</span>ayes framework for probabilistic clustering,”</span> <em>Biometrika</em>, Oxford University Press, 10, 559–578.
</div>
<div id="ref-syring2019calibrating" class="csl-entry" role="listitem">
Syring, N., and Martin, R. (2019), <span>“Calibrating general posterior credible regions,”</span> <em>Biometrika</em>, Oxford University Press, 106, 479–486.
</div>
</div>


</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
  window.document.addEventListener("DOMContentLoaded", function (event) {
    const icon = "";
    const anchorJS = new window.AnchorJS();
    anchorJS.options = {
      placement: 'right',
      icon: icon
    };
    anchorJS.add('.anchored');
    const isCodeAnnotation = (el) => {
      for (const clz of el.classList) {
        if (clz.startsWith('code-annotation-')) {                     
          return true;
        }
      }
      return false;
    }
    const onCopySuccess = function(e) {
      // button target
      const button = e.trigger;
      // don't keep focus
      button.blur();
      // flash "checked"
      button.classList.add('code-copy-button-checked');
      var currentTitle = button.getAttribute("title");
      button.setAttribute("title", "Copied!");
      let tooltip;
      if (window.bootstrap) {
        button.setAttribute("data-bs-toggle", "tooltip");
        button.setAttribute("data-bs-placement", "left");
        button.setAttribute("data-bs-title", "Copied!");
        tooltip = new bootstrap.Tooltip(button, 
          { trigger: "manual", 
            customClass: "code-copy-button-tooltip",
            offset: [0, -8]});
        tooltip.show();    
      }
      setTimeout(function() {
        if (tooltip) {
          tooltip.hide();
          button.removeAttribute("data-bs-title");
          button.removeAttribute("data-bs-toggle");
          button.removeAttribute("data-bs-placement");
        }
        button.setAttribute("title", currentTitle);
        button.classList.remove('code-copy-button-checked');
      }, 1000);
      // clear code selection
      e.clearSelection();
    }
    const getTextToCopy = function(trigger) {
        const codeEl = trigger.previousElementSibling.cloneNode(true);
        for (const childEl of codeEl.children) {
          if (isCodeAnnotation(childEl)) {
            childEl.remove();
          }
        }
        return codeEl.innerText;
    }
    const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
      text: getTextToCopy
    });
    clipboard.on('success', onCopySuccess);
    if (window.document.getElementById('quarto-embedded-source-code-modal')) {
      const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
        text: getTextToCopy,
        container: window.document.getElementById('quarto-embedded-source-code-modal')
      });
      clipboardModal.on('success', onCopySuccess);
    }
      var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
      var mailtoRegex = new RegExp(/^mailto:/);
        var filterRegex = new RegExp('/' + window.location.host + '/');
      var isInternal = (href) => {
          return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
      }
      // Inspect non-navigation links and adorn them if external
     var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
      for (var i=0; i<links.length; i++) {
        const link = links[i];
        if (!isInternal(link.href)) {
          // undo the damage that might have been done by quarto-nav.js in the case of
          // links that we want to consider external
          if (link.dataset.originalHref !== undefined) {
            link.href = link.dataset.originalHref;
          }
        }
      }
    function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
      const config = {
        allowHTML: true,
        maxWidth: 500,
        delay: 100,
        arrow: false,
        appendTo: function(el) {
            return el.parentElement;
        },
        interactive: true,
        interactiveBorder: 10,
        theme: 'quarto',
        placement: 'bottom-start',
      };
      if (contentFn) {
        config.content = contentFn;
      }
      if (onTriggerFn) {
        config.onTrigger = onTriggerFn;
      }
      if (onUntriggerFn) {
        config.onUntrigger = onUntriggerFn;
      }
      window.tippy(el, config); 
    }
    const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
    for (var i=0; i<noterefs.length; i++) {
      const ref = noterefs[i];
      tippyHover(ref, function() {
        // use id or data attribute instead here
        let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
        try { href = new URL(href).hash; } catch {}
        const id = href.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note) {
          return note.innerHTML;
        } else {
          return "";
        }
      });
    }
    const xrefs = window.document.querySelectorAll('a.quarto-xref');
    const processXRef = (id, note) => {
      // Strip column container classes
      const stripColumnClz = (el) => {
        el.classList.remove("page-full", "page-columns");
        if (el.children) {
          for (const child of el.children) {
            stripColumnClz(child);
          }
        }
      }
      stripColumnClz(note)
      if (id === null || id.startsWith('sec-')) {
        // Special case sections, only their first couple elements
        const container = document.createElement("div");
        if (note.children && note.children.length > 2) {
          container.appendChild(note.children[0].cloneNode(true));
          for (let i = 1; i < note.children.length; i++) {
            const child = note.children[i];
            if (child.tagName === "P" && child.innerText === "") {
              continue;
            } else {
              container.appendChild(child.cloneNode(true));
              break;
            }
          }
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(container);
          }
          return container.innerHTML
        } else {
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(note);
          }
          return note.innerHTML;
        }
      } else {
        // Remove any anchor links if they are present
        const anchorLink = note.querySelector('a.anchorjs-link');
        if (anchorLink) {
          anchorLink.remove();
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        if (note.classList.contains("callout")) {
          return note.outerHTML;
        } else {
          return note.innerHTML;
        }
      }
    }
    for (var i=0; i<xrefs.length; i++) {
      const xref = xrefs[i];
      tippyHover(xref, undefined, function(instance) {
        instance.disable();
        let url = xref.getAttribute('href');
        let hash = undefined; 
        if (url.startsWith('#')) {
          hash = url;
        } else {
          try { hash = new URL(url).hash; } catch {}
        }
        if (hash) {
          const id = hash.replace(/^#\/?/, "");
          const note = window.document.getElementById(id);
          if (note !== null) {
            try {
              const html = processXRef(id, note.cloneNode(true));
              instance.setContent(html);
            } finally {
              instance.enable();
              instance.show();
            }
          } else {
            // See if we can fetch this
            fetch(url.split('#')[0])
            .then(res => res.text())
            .then(html => {
              const parser = new DOMParser();
              const htmlDoc = parser.parseFromString(html, "text/html");
              const note = htmlDoc.getElementById(id);
              if (note !== null) {
                const html = processXRef(id, note);
                instance.setContent(html);
              } 
            }).finally(() => {
              instance.enable();
              instance.show();
            });
          }
        } else {
          // See if we can fetch a full url (with no hash to target)
          // This is a special case and we should probably do some content thinning / targeting
          fetch(url)
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.querySelector('main.content');
            if (note !== null) {
              // This should only happen for chapter cross references
              // (since there is no id in the URL)
              // remove the first header
              if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
                note.children[0].remove();
              }
              const html = processXRef(null, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      }, function(instance) {
      });
    }
        let selectedAnnoteEl;
        const selectorForAnnotation = ( cell, annotation) => {
          let cellAttr = 'data-code-cell="' + cell + '"';
          let lineAttr = 'data-code-annotation="' +  annotation + '"';
          const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
          return selector;
        }
        const selectCodeLines = (annoteEl) => {
          const doc = window.document;
          const targetCell = annoteEl.getAttribute("data-target-cell");
          const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
          const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
          const lines = annoteSpan.getAttribute("data-code-lines").split(",");
          const lineIds = lines.map((line) => {
            return targetCell + "-" + line;
          })
          let top = null;
          let height = null;
          let parent = null;
          if (lineIds.length > 0) {
              //compute the position of the single el (top and bottom and make a div)
              const el = window.document.getElementById(lineIds[0]);
              top = el.offsetTop;
              height = el.offsetHeight;
              parent = el.parentElement.parentElement;
            if (lineIds.length > 1) {
              const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
              const bottom = lastEl.offsetTop + lastEl.offsetHeight;
              height = bottom - top;
            }
            if (top !== null && height !== null && parent !== null) {
              // cook up a div (if necessary) and position it 
              let div = window.document.getElementById("code-annotation-line-highlight");
              if (div === null) {
                div = window.document.createElement("div");
                div.setAttribute("id", "code-annotation-line-highlight");
                div.style.position = 'absolute';
                parent.appendChild(div);
              }
              div.style.top = top - 2 + "px";
              div.style.height = height + 4 + "px";
              div.style.left = 0;
              let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
              if (gutterDiv === null) {
                gutterDiv = window.document.createElement("div");
                gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
                gutterDiv.style.position = 'absolute';
                const codeCell = window.document.getElementById(targetCell);
                const gutter = codeCell.querySelector('.code-annotation-gutter');
                gutter.appendChild(gutterDiv);
              }
              gutterDiv.style.top = top - 2 + "px";
              gutterDiv.style.height = height + 4 + "px";
            }
            selectedAnnoteEl = annoteEl;
          }
        };
        const unselectCodeLines = () => {
          const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
          elementsIds.forEach((elId) => {
            const div = window.document.getElementById(elId);
            if (div) {
              div.remove();
            }
          });
          selectedAnnoteEl = undefined;
        };
          // Handle positioning of the toggle
      window.addEventListener(
        "resize",
        throttle(() => {
          elRect = undefined;
          if (selectedAnnoteEl) {
            selectCodeLines(selectedAnnoteEl);
          }
        }, 10)
      );
      function throttle(fn, ms) {
      let throttle = false;
      let timer;
        return (...args) => {
          if(!throttle) { // first call gets through
              fn.apply(this, args);
              throttle = true;
          } else { // all the others get throttled
              if(timer) clearTimeout(timer); // cancel #2
              timer = setTimeout(() => {
                fn.apply(this, args);
                timer = throttle = false;
              }, ms);
          }
        };
      }
        // Attach click handler to the DT
        const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
        for (const annoteDlNode of annoteDls) {
          annoteDlNode.addEventListener('click', (event) => {
            const clickedEl = event.target;
            if (clickedEl !== selectedAnnoteEl) {
              unselectCodeLines();
              const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
              if (activeEl) {
                activeEl.classList.remove('code-annotation-active');
              }
              selectCodeLines(clickedEl);
              clickedEl.classList.add('code-annotation-active');
            } else {
              // Unselect the line
              unselectCodeLines();
              clickedEl.classList.remove('code-annotation-active');
            }
          });
        }
    const findCites = (el) => {
      const parentEl = el.parentElement;
      if (parentEl) {
        const cites = parentEl.dataset.cites;
        if (cites) {
          return {
            el,
            cites: cites.split(' ')
          };
        } else {
          return findCites(el.parentElement)
        }
      } else {
        return undefined;
      }
    };
    var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
    for (var i=0; i<bibliorefs.length; i++) {
      const ref = bibliorefs[i];
      const citeInfo = findCites(ref);
      if (citeInfo) {
        tippyHover(citeInfo.el, function() {
          var popup = window.document.createElement('div');
          citeInfo.cites.forEach(function(cite) {
            var citeDiv = window.document.createElement('div');
            citeDiv.classList.add('hanging-indent');
            citeDiv.classList.add('csl-entry');
            var biblioDiv = window.document.getElementById('ref-' + cite);
            if (biblioDiv) {
              citeDiv.innerHTML = biblioDiv.innerHTML;
            }
            popup.appendChild(citeDiv);
          });
          return popup.innerHTML;
        });
      }
    }
  });
  </script>
</div> <!-- /content -->




<script src="../../site_libs/quarto-html/zenscroll-min.js"></script>
</body></html>