---
title: "Karmic dice for Baldur's Gate III"
subtitle: "A simple algorithm that accounts for karma"
author: "[Tommaso Rigon]{.orange}"
lang: en
date: 2023-08-20
execute:
  cache: true
format:
  html:
    html-math-method: katex
    echo: true
    callout-appearance: minimal
    toc: true
    embed-resources: false
    code-line-numbers: true
    fig-dpi: 250
editor: 
  markdown: 
    wrap: 72
editor_options: 
  chunk_output_type: console
---


## Karmic dice: what do they do?

I played the game "Baldur's Gate" when I was a kid, on my old-fashioned
computer and I thought it was a great game. When [Baldur's Gate
III](https://baldursgate3.game) was released a few weeks ago, it caught
my attention.

The central dynamic of the game is based on Dungeons & Dragons, meaning
that players need to roll the famous [20 sided dice]{.orange} several
times. On a laptop/console, these ["random" numbers]{.blue} (actually,
pseudo-random) are automatically generated by the software.

A specific feature of the game, the [karmic
dice](https://www.ign.com/wikis/baldurs-gate-3/Karmic_Dice_Explained),
was very much discussed by the online community.

The karmic dice [avoid subsequent]{.blue} successes and
[failures]{.blue}. In other terms, the results of the dice are [not
independent]{.orange} anymore.

I am an [Assistant Professor of Statistical Science](https://tommasorigon.github.io), so I had a few questions: how does this algorithm work, exactly? Will it preserve basic
properties that do not [screw up]{.blue} the [balance]{.blue} of the game? 

Perhaps more importantly: is it possible to find a simple way to generate "karmic" rolls?

#### Regular dice vs karmic dice

```{r}
#| echo: false
rm(list = ls())
karmic_score <- function(n, kappa) {
  sigma <- sqrt(1 / (1 - kappa^2))
  latent_score <- numeric(n)
  latent_score[1] <- rnorm(1, 0, sigma)
  for (i in 1:(n - 1)) {
    latent_score[i + 1] <- -kappa * latent_score[i] + rnorm(1, 0, 1)
  }
  latent_score
}
```

```{r}
#| echo: false
karmic_dice <- function(n, kappa) {
  sigma <- sqrt(1 / (1 - kappa^2))
  latent_score <- karmic_score(n, kappa)
  ceiling(20 * pnorm(latent_score, 0, sigma))
}
```

Let us step back for a second. Why do we need a karmic dice in the first
place?

Most people find "[true randomness]{.orange}" (independent trials, in
the language of probability) quite [counter-intuitive]{.orange}.

Suppose we toss a fair dice with 20 sides, say, 100 times. A
compatible sequence of results could be:

```{r}
#| echo: false
set.seed(50)
karmic_dice(n = 100, kappa = 0)
```

A surprising amount of people would find this sequence "non-random" or
would suspect that something is off with the random number generator
(RNG). There is indeed a sequence of bad rolls:
`7, 8, 9, 5, 3, 8, 8, 6`, and `2`. Is this some mistake?

Long story short: nothing fishy is going on, this is the normal
behavior of a regular dice. Yet, a non-trained eye would expect
something like the following [karmic sequence]{.blue} (dependent
trials):

```{r}
#| echo: false
set.seed(60)
karmic_dice(n = 100, kappa = 0.35)
```

In this sequence, there are fewer streaks of negative results, and many players find this kind of pattern more enjoyable.

#### The long-run behavior and weighted dice

There are many ways to obtain a sequence with fewer unlucky rolls. A simple idea could be using a [weighted dice]{.orange}, e.g. an
hypothetical dice with non-uniform probabilities.

Such a weighted dice accomplishes the goal of reducing extreme events,
but it is modifying a fundamental aspect of Dungeons & Dragons and
therefore affecting the [balance of the game]{.orange}.

Changing the rules is not necessarily bad, but it has
consequences. Indeed, most role players are well aware that there is a
difference between [1d20]{.blue} and [2d10]{.orange}.

#### Desiderata for a karmic dice

I think a more [gentle approach]{.blue} would be tweaking the algorithm
in such a way:

-   The [long-rung proportions]{.blue} are identical to those of a
    regular dice, i.e., each side of the dice has a $5\%$ probability of
    being picked if we were to roll the dice a considerable amount of times.

-   The rolls are [dependent]{.orange}, meaning the values obtained
    at the previous step influence future behavior.

Moreover, the algorithm should be [easy to implement]{.orange} and tunable to account for players' preferences (e.g., weak vs strong karma
effect).

## A simple algorithm for the karmic dice

I do not know which approach is implemented in Baldur's Gate III, I can only make
educated guesses. However, it turns out that a simple idea, based on the notion of [latent karma]{.blue}, has all the above desiderata.

Let us focus on the generic $t$th roll. The latent karma $Y_t$ of the is
a real number that identifies how lucky we have been, say the
[positive]{.blue} values $1.5$ or $3.2$ (lucky events) or the
[negative]{.orange} value $-4.3$ (unlucky event).

Let $0 < \kappa < 1$ be a constant determining the "karma effect."
The [first]{.blue} karma score $Y_1$ does not depend on the past
(because there is no past), and we generate it according to a Gaussian
random variable:

$$
Y_1 \sim \text{N}\left(0, \frac{1}{1 - \kappa^2}\right).
$$

The subsequent karma scores are obtained by adjusting the previous karma
score as follows:

$$
Y_t = - \kappa \:Y_{t-1} + \epsilon_t, \qquad \epsilon_t \sim \text{N}(0, 1),
$$ for $t = 2,\dots,n$. In other words, [if we were unlucky]{.orange} at
step $t$, we had [better chances to be lucky]{.blue} at step $t+1$ (and
vice-versa). The amount of this [effect]{.grey} is [regulated by the
parameter $\kappa$]{.grey}.

In **R / python code**, the generation of the latent karma scores is
straightforward:

::: panel-tabset
## R code

```{r}
karmic_score <- function(n, kappa) {
  sigma <- sqrt(1 / (1 - kappa^2))
  latent_score <- numeric(n)
  
  # First latent karma Y_1
  latent_score[1] <- rnorm(1, 0, sigma)
  
  # Generation of the latent karma values Y_2,...,Y_n
  for (t in 1:(n - 1)) {
    latent_score[t + 1] <- -kappa * latent_score[t] + rnorm(1, 0, 1)
  }
  
  return(latent_score)
}
```

## Python code

```{python}
import numpy as np

def karmic_score(n, kappa):
    sigma = np.sqrt(1 / (1 - kappa**2))
    latent_score = np.zeros(n)
    
    # First latent karma Y_1
    latent_score[0] = np.random.normal(0, sigma)
    
    # Generation of the latent karma values Y_2,...,Y_n
    for t in range(1, n):
        latent_score[t] = -kappa * latent_score[t - 1] + np.random.normal(0, 1)
    
    return latent_score
```
:::


Once we have obtained the karma scores $Y_t$, we simply
[convert]{.orange} them into integers $D_t$, belonging to $1,\dots,20$,
using the following formula

$$
D_t = \text{ceiling}\{20 \Phi_\kappa(Y_t)\}, \qquad t = 1,\dots,n,
$$ where $\Phi_\kappa(y)$ is the [cumulative distribution
function]{.blue} of a Gaussian with zero mean and variance
$1 / (1 - \kappa^2)$. 

The $\text{ceiling}$ operation rounds up its
argument, producing an integer. Once again, this operation can be
performed with a few lines of R / Python code.

::: panel-tabset

## R code
```{r}
karmic_dice <- function(n, kappa) {
  sigma <- sqrt(1 / (1 - kappa^2))
  
  # Generate the latent scores
  latent_score <- karmic_score(n, kappa)
  # Convert the latent scores into integers between 1 and 20
  dice_results <- ceiling(20 * pnorm(latent_score, 0, sigma))
  return(dice_results)
}
```

## Python code
```{python}
from scipy.stats import norm

def karmic_dice(n, kappa):
    sigma = np.sqrt(1 / (1 - kappa**2))
    
    # Generate the latent scores
    latent_score = karmic_score(n, kappa)
    # Convert the latent scores into integers between 1 and 20
    dice_results = np.ceil(20 * norm.cdf(latent_score, loc=0, scale=sigma))
    return dice_results
```

:::


The usage of this function is also straightforward. In R, we can get a bunch of values for the latent score and the dice as follows:

```{r}
karmic_dice(n = 30, kappa = 0.35)
```


## Analyzing the algorithm

The most interesting property of this karmic dice is that it [preserves]{.blue}
the [long-run proportions]{.blue}. If you have a solid background on the theory of
[auto-regressive processes]{.blue} and the so-called [inversion
theorem]{.orange} for generating random variables, this should be
immediately obvious to you.

If you are not familiar with probability theory, the following
[empirical demonstration]{.blue} should give you an intuition of why this idea works nicely. 

Suppose I were to get [one million rolls]{.orange}, then I could check how many times I got each of the values $1,2,\dots, 20$. With a computer, this can be quickly done (using $\kappa = 0.35$), and these are the [proportions]{.blue} we get:

```{r}
#| cache: true
#| echo: false
table(karmic_dice(10^6, kappa = 0.35)) / 10^6
```


Each number is roughly appearing in the sequence $5\%$ of the times, as
it should!

The take-home message is that this algorithm [does not affect the overall balance]{.blue} of the game because the long-term behavior (called stationary distribution) coincides with that of the regular dice. 

#### The karmic effect

Another aspect we would like to understand is the [dependence]{.orange} between the current roll and the following. These two rolls are independent in the case of a regular dice, but the situation is quite different in a karmic dice.

Let us start noticing that when $\kappa = 0$, the above karmic dice algorithm is just a convoluted way of sampling from a regular dice! It can be "easily" proved using standard probability tools. 

When $\kappa > 0$, the karmic dice tweaks the probability and induces a negative correlation between subsequent rolls. Let us visualize what happens when $\kappa = 0.35$. 

```{r}
#| cache: true
#| echo: false
n <- 5 * 10^6
kappa <- c(0, 0.35, 0.55)
sim1 <- karmic_dice(n = n, kappa = kappa[1])
sim2 <- karmic_dice(n = n, kappa = kappa[2])
sim3 <- karmic_dice(n = n, kappa = kappa[3])

tab1 <- table(sim1[1:(n - 1)], sim1[2:n])
tab1 <- tab1 / rowSums(tab1)

tab2 <- table(sim2[1:(n - 1)], sim2[2:n])
tab2 <- tab2 / rowSums(tab2)

tab3 <- table(sim3[1:(n - 1)], sim3[2:n])
tab3 <- tab3 / rowSums(tab3)
```

```{r}
#| echo: false
library(ggplot2)
df <- rbind(data.frame(reshape2::melt(tab2), kappa = "Karmic dice (kappa = 0.35)"))
ggplot(df, aes(x = Var1, y = Var2, fill = value)) +
  geom_tile(color = "black") +
  scale_fill_gradient2(low = "#fc7d0b", high = "#1170aa", midpoint = 0.05, mid = "white") +
  coord_fixed() +
  facet_grid(. ~ kappa) +
  scale_x_continuous(breaks = 1:20, minor_breaks = NULL) +
  scale_y_continuous(breaks = 1:20, minor_breaks = NULL) +
  theme_bw() +
  guides(fill = guide_colourbar(title = "Probability")) +
  xlab("Current roll") +
  ylab("Next roll")
```

[Blue squares]{.blue} are more likely values (compared to the regular dice), whereas [orange squares]{.orange} are less likely values. This graph reveals a few interesting facts:

  - If the current roll is a 1, then the most likely outcome of the following will be in the range of 15-20, with a peak in 20.
  - If the current roll is 20, then the most likely outcome of the following will be in the range of 1-5, with a peak in 1.
  - If the current roll is either 10 or 11, the following roll will roughly behave like a regular dice.
  
This version of the karmic effect therefore [compensate]{.orange} positive values with negative values in a balanced manner so that the long-run proportions are correct.

#### The choice of  $\kappa$

The amount of compensation is regulated by $\kappa$, a [crucial tuning parameter]{.blue} that can be used to tune the dynamic of the karma dice. 

If $\kappa$ is close to $0$, we get a regular dice. Vice versa, if $\kappa$ is close to $1$, i.e., the theoretical maximum, we would get sequences like this:

```{r}
karmic_dice(n = 100, kappa = 0.999)
```

Even though, in the (very) long run, the correct proportions are still preserved for any $\kappa < 1$, such an extreme choice of $\kappa = 0.999$ leads to almost [deterministic compensations]{.orange}, which are not appropriate for a game, because it would become [too predictable]{.blue}.

There is no "statistical" optimal choice for $\kappa$, which instead should be selected based on the [player preferences]{.blue}. Here I picked $\kappa = 0.35$ as it seems like a reasonable default, being a middle-ground solution between the independence case (regular dice) and an almost determinist pattern. 
