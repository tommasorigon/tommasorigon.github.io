<!DOCTYPE html>
<html lang="en"><head>
<script src="../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../site_libs/quarto-html/tabby.min.js"></script>
<script src="../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../site_libs/quarto-html/tippy.umd.min.js"></script>
<link href="../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../site_libs/quarto-html/light-border.css" rel="stylesheet">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting-234273d1456647dabc34a594ac50e507.css" rel="stylesheet" id="quarto-text-highlighting-styles"><meta charset="utf-8">
  <meta name="generator" content="quarto-1.7.32">

  <meta name="author" content="Tommaso Rigon">
  <meta name="dcterms.date" content="2026-02-06">
  <title>Tommaso Rigon – ABACO26</title>
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no, minimal-ui">
  <link rel="stylesheet" href="../../site_libs/revealjs/dist/reset.css">
  <link rel="stylesheet" href="../../site_libs/revealjs/dist/reveal.css">
  <style>
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    ul.task-list{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      width: 0.8em;
      margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
      vertical-align: middle;
    }
    /* CSS for citations */
    div.csl-bib-body { }
    div.csl-entry {
      clear: both;
      margin-bottom: 0em;
    }
    .hanging-indent div.csl-entry {
      margin-left:2em;
      text-indent:-2em;
    }
    div.csl-left-margin {
      min-width:2em;
      float:left;
    }
    div.csl-right-inline {
      margin-left:2em;
      padding-left:1em;
    }
    div.csl-indent {
      margin-left: 2em;
    }  </style>
  <link rel="stylesheet" href="../../site_libs/revealjs/dist/theme/quarto-a0c0cc41185901f99e5862491b9a6885.css">
  <script>window.backupDefine = window.define; window.define = undefined;</script><script src="https://cdn.jsdelivr.net/npm/katex@latest/dist/katex.min.js"></script>
  <script>document.addEventListener("DOMContentLoaded", function () {
 var mathElements = document.getElementsByClassName("math");
 var macros = [];
 for (var i = 0; i < mathElements.length; i++) {
  var texText = mathElements[i].firstChild;
  if (mathElements[i].tagName == "SPAN") {
   katex.render(texText.data, mathElements[i], {
    displayMode: mathElements[i].classList.contains('display'),
    throwOnError: false,
    macros: macros,
    fleqn: false
   });
}}});
  </script>
  <script>window.define = window.backupDefine; window.backupDefine = undefined;</script><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@latest/dist/katex.min.css">
  <link href="../../site_libs/revealjs/plugin/quarto-line-highlight/line-highlight.css" rel="stylesheet">
  <link href="../../site_libs/revealjs/plugin/reveal-menu/menu.css" rel="stylesheet">
  <link href="../../site_libs/revealjs/plugin/reveal-menu/quarto-menu.css" rel="stylesheet">
  <link href="../../site_libs/revealjs/plugin/quarto-support/footer.css" rel="stylesheet">
  <style type="text/css">
    .reveal div.sourceCode {
      margin: 0;
      overflow: auto;
    }
    .reveal div.hanging-indent {
      margin-left: 1em;
      text-indent: -1em;
    }
    .reveal .slide:not(.center) {
      height: 100%;
    }
    .reveal .slide.scrollable {
      overflow-y: auto;
    }
    .reveal .footnotes {
      height: 100%;
      overflow-y: auto;
    }
    .reveal .slide .absolute {
      position: absolute;
      display: block;
    }
    .reveal .footnotes ol {
      counter-reset: ol;
      list-style-type: none; 
      margin-left: 0;
    }
    .reveal .footnotes ol li:before {
      counter-increment: ol;
      content: counter(ol) ". "; 
    }
    .reveal .footnotes ol li > p:first-child {
      display: inline-block;
    }
    .reveal .slide ul,
    .reveal .slide ol {
      margin-bottom: 0.5em;
    }
    .reveal .slide ul li,
    .reveal .slide ol li {
      margin-top: 0.4em;
      margin-bottom: 0.2em;
    }
    .reveal .slide ul[role="tablist"] li {
      margin-bottom: 0;
    }
    .reveal .slide ul li > *:first-child,
    .reveal .slide ol li > *:first-child {
      margin-block-start: 0;
    }
    .reveal .slide ul li > *:last-child,
    .reveal .slide ol li > *:last-child {
      margin-block-end: 0;
    }
    .reveal .slide .columns:nth-child(3) {
      margin-block-start: 0.8em;
    }
    .reveal blockquote {
      box-shadow: none;
    }
    .reveal .tippy-content>* {
      margin-top: 0.2em;
      margin-bottom: 0.7em;
    }
    .reveal .tippy-content>*:last-child {
      margin-bottom: 0.2em;
    }
    .reveal .slide > img.stretch.quarto-figure-center,
    .reveal .slide > img.r-stretch.quarto-figure-center {
      display: block;
      margin-left: auto;
      margin-right: auto; 
    }
    .reveal .slide > img.stretch.quarto-figure-left,
    .reveal .slide > img.r-stretch.quarto-figure-left  {
      display: block;
      margin-left: 0;
      margin-right: auto; 
    }
    .reveal .slide > img.stretch.quarto-figure-right,
    .reveal .slide > img.r-stretch.quarto-figure-right  {
      display: block;
      margin-left: auto;
      margin-right: 0; 
    }
  </style>
</head>
<body class="quarto-light">
  <div class="reveal">
    <div class="slides">

<section id="title-slide" class="quarto-title-block center">
  <h1 class="title">ABACO26</h1>
  <p class="subtitle"><em>Nonparametric predictive inference for discrete data via Metropolis-adjusted Dirichlet sequences</em></p>

<div class="quarto-title-authors">
<div class="quarto-title-author">
<div class="quarto-title-author-name">
<span class="orange">Tommaso Rigon</span> 
</div>
        <p class="quarto-title-affiliation">
            <em>Università degli Studi di Milano-Bicocca</em>
          </p>
    </div>
</div>

  <p class="date">2026-02-06</p>
</section>
<section id="warm-thanks" class="slide level2 center">
<h2>Warm thanks</h2>
<div class="columns">
<div class="column" style="width:45%;">
<p><span class="orange">Davide Agnoletto</span> (Duke University)</p>
<div class="quarto-figure quarto-figure-left">
<figure>
<p><img data-src="img/davide.jpg" class="quarto-figure quarto-figure-left"></p>
</figure>
</div>
</div><div class="column" style="width:10%;">

</div><div class="column" style="width:45%;">
<p><span class="orange">David Dunson</span> (Duke University)</p>
<div class="quarto-figure quarto-figure-left">
<figure>
<p><img data-src="img/david.jpeg" class="quarto-figure quarto-figure-left"></p>
</figure>
</div>
</div></div>
</section>
<section id="foundations" class="slide level2 center">
<h2>Foundations</h2>
<ul>
<li>De Finetti’s representation Theorem <span class="citation" data-cites="de1937prevision">(<a href="#/references" role="doc-biblioref" onclick="">De Finetti 1937</a>)</span> it provides the fundamental justification to the <span class="blue">two approaches</span> to <span class="orange">Bayesian statistics</span>: the <span class="orange">hypothetical</span> approach and the <span class="blue">predictive</span> approach.</li>
</ul>
<div title="De Finetti's representation theorem">
<div class="callout callout-warning no-icon callout-titled callout-style-simple">
<div class="callout-body">
<div class="callout-title">
<p><strong>De Finetti’s representation theorem</strong></p>
</div>
<div class="callout-content">
<p>Let <span class="math inline">(Y_n)_{n\ge 1}</span>, be a sequence of <span class="blue">exchangeable</span> random variables. Then there exists a unique probability measure <span class="math inline">\Pi</span> such that, for any <span class="math inline">n\ge 1</span> and <span class="math inline">A_1,\dots,A_n</span> <span class="math display">
\mathbb{P}(Y_1 \in A_1,\ldots,Y_n \in A_n) = \int_{\mathcal{P}} \prod_{i=1}^n p(A_i)\,\Pi(\mathrm{d}p).
</span></p>
</div>
</div>
</div>
</div>
<ul>
<li>In a hierarchical formulation, we will say that <span class="math inline">(Y_n)_{n \ge 1}</span> is <span class="bl">exchangeable</span> if and only if <span class="math display">
\begin{aligned}
Y_i \mid P &amp;\overset{\textup{iid}}{\sim} P, \qquad i \ge 1, \\
P &amp;\sim \Pi,
\end{aligned}
</span> where <span class="math inline">P</span> is a <span class="blue">random probability measure</span> and <span class="math inline">\Pi</span> is the <span class="orange">prior law</span>.</li>
</ul>
</section>
<section id="hypothetical-approach" class="slide level2 center">
<h2>Hypothetical approach</h2>
<ul>
<li><p>The hypothetical approach represents the the <span class="blue">most common</span> way to operate within the Bayesian community.</p></li>
<li><p>In a <span class="orange">parametric</span> setting, <span class="math inline">\Pi</span> has support on a class <span class="math inline">\Theta\subseteq\mathbb{R}^p</span> with <span class="math inline">p&lt;\infty</span>, such that <span class="math inline">\boldsymbol{\theta}\in\Theta</span> indexes the class of distributions <span class="math inline">\mathcal{P}_{\boldsymbol{\theta}}=\{P_{\boldsymbol{\theta}} : \boldsymbol{\theta} \in \Theta\subseteq\mathbb{R}^p\}</span>.</p></li>
<li><p>Bayes’ rule takes the well-known formulation: <span class="math display">
\pi(\boldsymbol{\theta}\mid y_{1:n}) \propto \pi(\boldsymbol{\theta}) \prod_{i=1}^n p_{\boldsymbol{\theta}}(y_i),
</span> where <span class="math inline">\pi</span> and <span class="math inline">p_{\boldsymbol{\theta}}</span> denote the probability density functions associated with <span class="math inline">\Pi</span> and <span class="math inline">P_{\boldsymbol{\theta}}</span>, respectively.</p></li>
<li><p>However, when the link between observations and parameter of interest cannot be easily expressed through a distribution function, the traditional hypothetical approach fails.</p></li>
</ul>
<div class="fragment">
<ul>
<li><p>Solution: <span class="orange">generalized posterior distributions</span>, sometimes called <span class="blue">Gibbs-posteriors</span>.</p></li>
<li><p>This is a <span class="blue">lively recent topic</span>, see for instance: <span class="citation" data-cites="chernozhukov2003mcmc">Chernozhukov and Hong (<a href="#/references" role="doc-biblioref" onclick="">2003</a>)</span>; <span class="citation" data-cites="bissiri2016general">Bissiri et al. (<a href="#/references" role="doc-biblioref" onclick="">2016</a>)</span> <span class="citation" data-cites="heide2020safe">Heide et al. (<a href="#/references" role="doc-biblioref" onclick="">2020</a>)</span>; <span class="citation" data-cites="grunwald2020fast">Grünwald and Mehta (<a href="#/references" role="doc-biblioref" onclick="">2020</a>)</span>; <span class="citation" data-cites="knoblauch2022optimization">Knoblauch et al. (<a href="#/references" role="doc-biblioref" onclick="">2022</a>)</span>; <span class="citation" data-cites="matsubara2022robust">Matsubara et al. (<a href="#/references" role="doc-biblioref" onclick="">2022</a>)</span>; <span class="citation" data-cites="matsubara2023generalized">Matsubara et al. (<a href="#/references" role="doc-biblioref" onclick="">2023</a>)</span>; <span class="citation" data-cites="jewson_rossell_2022">Jewson and Rossell (<a href="#/references" role="doc-biblioref" onclick="">2022</a>)</span>; <span class="citation" data-cites="rigon2023generalized">Rigon et al. (<a href="#/references" role="doc-biblioref" onclick="">2023</a>)</span>; <span class="citation" data-cites="Agnoletto2025">Agnoletto et al. (<a href="#/references" role="doc-biblioref" onclick="">2025</a>)</span>.</p></li>
</ul>
</div>
</section>
<section id="generalizations-of-the-hypothetical-approach" class="slide level2 center">
<h2>Generalizations of the hypothetical approach</h2>
<ul>
<li><span class="citation" data-cites="bissiri2016general">Bissiri et al. (<a href="#/references" role="doc-biblioref" onclick="">2016</a>)</span> showed that the <span class="blue">generalized posterior</span> <span class="math display">
\pi_\omega(\boldsymbol{\theta} \mid y_{1:n}) \propto \pi(\boldsymbol{\theta}) \exp\left\{ - \omega \sum_{i=1}^n \ell(\boldsymbol{\theta}; y_i) \right\},
</span> is the only coherent update of the prior beliefs about <span class="math display">
\boldsymbol{\theta}^* = \arg\min_{\boldsymbol{\theta}\in\Theta} \int_{\mathcal{Y}} \ell(\boldsymbol{\theta}; y)\, F_0(\mathrm{d}y),
</span> where <span class="math inline">\ell(\boldsymbol{\theta}, y)</span> is a <span class="orange">loss function</span>, <span class="math inline">\omega</span> is the <span class="grey">loss-scale</span>, and <span class="math inline">F_0</span> is the unknown true sampling distribution.</li>
</ul>
<div class="fragment">
<ul>
<li><p>Learning the loss scale <span class="math inline">\omega</span> from the data is a <span class="orange">delicate</span> task. Assuming a prior for <span class="math inline">\omega</span> can lead to degenerate estimates if not accompanied by additional adjustments to the loss function.</p></li>
<li><p>However, there are several solutions for its calibration: <span class="citation" data-cites="holmes2017assigning">Holmes and Walker (<a href="#/references" role="doc-biblioref" onclick="">2017</a>)</span>; <span class="citation" data-cites="lyddon2019general">Lyddon et al. (<a href="#/references" role="doc-biblioref" onclick="">2019</a>)</span>; <span class="citation" data-cites="syring2019calibrating">Syring and Martin (<a href="#/references" role="doc-biblioref" onclick="">2019</a>)</span>; <span class="citation" data-cites="matsubara2023generalized">Matsubara et al. (<a href="#/references" role="doc-biblioref" onclick="">2023</a>)</span>.</p></li>
</ul>
</div>
</section>
<section id="predictive-approach" class="slide level2 center">
<h2>Predictive approach</h2>
<ul>
<li>Taking a <span class="blue">predictive approach</span>, one can implicitly <span class="orange">characterize the prior</span> via de Finetti theorem by specifying the sequence of predictive distributions of an exchangeable sequence: <span class="math display">
P_n(A) := \mathbb{P}(Y_{n+1}\in A\mid y_{1:n}), \qquad n \ge 1.
</span> This leads to an exchangeable sequences iff the conditions in <span class="citation" data-cites="fortini2000exchangeability">Fortini et al. (<a href="#/references" role="doc-biblioref" onclick="">2000</a>)</span> are satisfied.</li>
</ul>
<div class="fragment">
<ul>
<li><span class="orange">Example</span>: the predictive construction of a Dirichlet process prior is such that <span class="math inline">Y_1\sim P_0</span> and <span class="math inline">Y_{n+1}\mid y_{1:n}\sim P_n</span> for <span class="math inline">n\ge 1</span>, where <span class="math display">
P_n(A) = \frac{\alpha}{\alpha+n} P_0(A) + \frac{1}{\alpha+n}\sum_{i=1}^n\mathbb{1}(y_i\in A),
</span> for any measurable set <span class="math inline">A</span>.</li>
</ul>
</div>
<div class="fragment">
<ul>
<li>The possibility of specifying a sequence of one-step-ahead predictive distributions is appealing:
<ul>
<li>it bypasses direct elicitation of the prior;</li>
<li>it explicitly connects prediction and inference (see the next slide);</li>
</ul></li>
</ul>
</div>
</section>
<section id="connecting-inference-and-prediction-i" class="slide level2 center">
<h2>Connecting inference and prediction I</h2>
<ul>
<li><p>The posterior <span class="math inline">P \mid y_{1:n}</span> is usually obtained through Bayes theorem, but this is not the only way.</p></li>
<li><p>We can <span class="blue">characterize</span> both <span class="blue">prior</span> and <span class="orange">posterior</span> of <span class="math inline">P</span> through the predictive distributions <span class="math inline">P_n</span>, which indeed contains all the necessary information.</p></li>
</ul>
<div class="fragment">
<ul>
<li>If <span class="math inline">(Y_n)_{n \ge 1}</span> is exchangeable, then the <span class="blue">prior</span> and <span class="blue">posterior mean</span> of <span class="math inline">P</span> coincide with the predictive: <span class="math display">
P_0(A) = \mathbb{P}(Y_1\in A) = \mathbb{E}\{P(A)\}, \qquad P_n(A) = \mathbb{P}(Y_{n+1}\in A\mid y_{1:n}) = \mathbb{E}(P(A) \mid y_{1:n}), \qquad n \ge 1.
</span></li>
</ul>
</div>
<div class="fragment">
<ul>
<li>A deeper result holds, which is a corollary of Finetti theorem <span class="citation" data-cites="fortini2012predictive">(<a href="#/references" role="doc-biblioref" onclick="">Fortini and Petrone 2012</a>)</span>.</li>
</ul>
<div title="De Finetti's representation theorem (predictive form)">
<div class="callout callout-warning no-icon callout-titled callout-style-simple">
<div class="callout-body">
<div class="callout-title">
<p><strong>De Finetti’s representation theorem (predictive form)</strong></p>
</div>
<div class="callout-content">
<p>Let <span class="math inline">(Y_n)_{n \ge 1}</span> be an exchangeable sequence with predictive distributions <span class="math inline">(P_n)_{n \ge 1}</span>. Then <span class="math inline">P_n</span> converges weakly (a.s. <span class="math inline">\mathbb{P}</span>) to a random probability measure <span class="math inline">P</span> distributed according to <span class="math inline">\Pi</span> as <span class="math inline">n \to \infty</span>.</p>
</div>
</div>
</div>
</div>
</div>
</section>
<section id="connecting-inference-and-prediction-ii" class="slide level2 center">
<h2>Connecting inference and prediction II</h2>
<ul>
<li>In other words, the sequence of predictive distributions <span class="math inline">P_n</span> converges to a random probability measure <span class="math inline">P</span> with <span class="orange">prior</span> distribution <span class="math inline">\Pi</span>. The <span class="blue">source of randomness</span> is the data<br>
<span class="math display">
Y_1, Y_2, Y_3, \dots
</span> Intuitively, before observing the data, our predictions eventually reflect the prior.</li>
</ul>
<div class="fragment">
<ul>
<li><p>Given <span class="math inline">y_{1:n}</span>, the sequence <span class="math inline">P_{n+m}</span> converges weakly (a.s. <span class="math inline">\mathbb{P}</span>) as <span class="math inline">m \to \infty</span> to a random probability measure with <span class="orange">posterior</span> distribution <span class="math inline">\Pi(\cdot \mid y_{1:n})</span>. The source of randomness is the <span class="blue">future data</span><br>
<span class="math display">
Y_{n+1}, Y_{n+2}, Y_{n+3}, \dots .
</span></p></li>
<li><p>This provides a natural <span class="blue">alternative interpretation</span> of the posterior distribution <span class="math inline">P \mid y_{1:n}</span> and a <span class="orange">practical algorithm</span> for sampling from it, called <span class="grey">predictive resampling</span>.</p></li>
<li><p>Intuitively, posterior uncertainty arises from <span class="orange">lack of knowledge</span> about future observations. If we knew them, the posterior would collapse to a point mass (Bayesian consistency).</p></li>
</ul>
</div>
<div class="fragment">
<ul>
<li>This reasoning is at the heart of <span class="blue">martingale posteriors</span> <span class="citation" data-cites="fong_holmes_2023">(<a href="#/references" role="doc-biblioref" onclick="">Fong et al. 2023</a>)</span>.</li>
</ul>
</div>
</section>
<section id="generalizations-of-the-predictive-approach" class="slide level2 center">
<h2>Generalizations of the predictive approach</h2>
<ul>
<li>Defining a sequence of predictive laws <span class="math inline">P_n</span> that guarantees exchangeability—i.e., satisfies the two-step-ahead conditions of <span class="citation" data-cites="fortini2000exchangeability">Fortini et al. (<a href="#/references" role="doc-biblioref" onclick="">2000</a>)</span>—is a <span class="orange">difficult</span> task in practice.</li>
</ul>
<div class="fragment">
<ul>
<li><p>Solution: replace exchangeability with the weaker requirement that <span class="math inline">(Y_n)_{n\ge1}</span> is <span class="blue">conditionally identically distributed</span> (CID), also known as <span class="orange">martingale posteriors</span>.</p></li>
<li><p>The CID condition requires <span class="math display">
\mathbb{P}(Y_{n+k} \in \cdot \mid y_{1:n}) = \mathbb{P}(Y_{n+1} \in \cdot \mid y_{1:n}) = P_n(\cdot), \qquad \text{for all} \quad  k \ge 1, \; n \ge 1.
</span> It is sufficient to verify this condition for <span class="math inline">k = 1</span> in order to ensure its validity for all <span class="math inline">k  \ge 1</span>.</p></li>
<li><p>Equivalently, <span class="citation" data-cites="fong_holmes_2023">Fong et al. (<a href="#/references" role="doc-biblioref" onclick="">2023</a>)</span> express the above condition in a way that emphasizes the <span class="orange">martingale property</span> of the predictive distributions <span class="math display">
\mathbb{E}\{P_{n+1}(\cdot) \mid y_{1:n}\} = P_n(\cdot), \qquad n \ge 1.
</span></p></li>
</ul>
</div>
<div class="fragment">
<ul>
<li>This is another <span class="blue">lively recent topic</span>: <span class="citation" data-cites="fortini_petrone_2020">Fortini and Petrone (<a href="#/references" role="doc-biblioref" onclick="">2020</a>)</span>; <span class="citation" data-cites="berti_pratelli_2021">Berti et al. (<a href="#/references" role="doc-biblioref" onclick="">2021</a>)</span>; <span class="citation" data-cites="fortini2021predictive">Fortini et al. (<a href="#/references" role="doc-biblioref" onclick="">2021</a>)</span>; <span class="citation" data-cites="berti2023without">Berti et al. (<a href="#/references" role="doc-biblioref" onclick="">2023a</a>)</span>; <span class="citation" data-cites="berti2023kernel">Berti et al. (<a href="#/references" role="doc-biblioref" onclick="">2023b</a>)</span>; <span class="citation" data-cites="fong_holmes_2023">Fong et al. (<a href="#/references" role="doc-biblioref" onclick="">2023</a>)</span>; <span class="citation" data-cites="fong2024asymptotics">Fong and Yiu (<a href="#/references" role="doc-biblioref" onclick="">2024a</a>)</span>; <span class="citation" data-cites="fong2024bayesian">Fong and Yiu (<a href="#/references" role="doc-biblioref" onclick="">2024b</a>)</span>; <span class="citation" data-cites="cui2024martingale">Cui and Walker (<a href="#/references" role="doc-biblioref" onclick="">2024</a>)</span>; <span class="citation" data-cites="fortini2024exchangeability">Fortini and Petrone (<a href="#/references" role="doc-biblioref" onclick="">2024</a>)</span> and more…</li>
</ul>
</div>
</section>
<section id="nonparametric-modelling-of-count-data" class="slide level2 center">
<h2>Nonparametric modelling of count data</h2>
<ul>
<li>Bayesian nonparametric modeling of <span class="blue">counts distributions</span> is a challenging task. Nonparametric mixtures of discrete kernels <span class="citation" data-cites="canale2011bayesian">(<a href="#/references" role="doc-biblioref" onclick="">Canale and Dunson 2011</a>)</span> can be cumbersome in practice.</li>
</ul>
<div class="fragment">
<ul>
<li>Alternatively, one could directly specify a <span class="orange">DP prior</span> on the data generator as <span class="math display">
  Y_i\mid P \overset{\textup{iid}}{\sim} P,\quad P\sim\mathrm{DP}(\alpha, P_0),
</span> for <span class="math inline">Y_i\in\mathcal{Y}=\{0,1,\ldots\}</span>, <span class="math inline">i=1,\ldots,n</span>, where <span class="math inline">\alpha</span> is the precision parameter and <span class="math inline">P_0</span> a base parametric distribution, such as a Poisson.</li>
</ul>
</div>
<div class="fragment">
<ul>
<li><p>The Dirichlet process is mathematically convenient. However, the corresponding posterior <span class="orange">lacks smoothing</span>, which can lead to poor performance.</p></li>
<li><p>Within the hypothetical approach, it is unclear how to specify a nonparametric process with the same simplicity and flexibility as the DP prior while allowing for smoothing.</p></li>
</ul>
</div>
<div class="fragment">
<ul>
<li><span class="blue">Our proposal</span>: a predictive sequence tailored to count data inspired by kernel density estimators.</li>
</ul>
</div>
</section>
<section id="illustrative-example-i" class="slide level2 center">
<h2>Illustrative example I</h2>
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img data-src="img/example_plot.jpg" class="quarto-figure quarto-figure-center" width="700"></p>
</figure>
</div>
<ul>
<li><span class="orange">Left plot</span>: posterior mean of a DP. <span class="blue">Right plot</span>: posterior mean of the proposed MAD sequence.</li>
</ul>
</section>
<section id="a-recursive-predictive-rule" class="slide level2 center">
<h2>A recursive predictive rule</h2>
<ul>
<li>Intuitively, a better estimator would be obtained by replacing the indicator <span class="math inline">\mathbb{1}(\cdot)</span> of the DP predictive scheme with a <span class="orange">kernel</span> that allows the <span class="blue">borrowing of information</span> between nearby values.</li>
</ul>
<div class="fragment">
<ul>
<li><p>Let <span class="math inline">Y_1\sim P_0</span> and <span class="math inline">Y_{n+1} \mid y_{1:n} \sim P_n</span> for <span class="math inline">n\ge 1</span>, and let <span class="math inline">K_n(\cdot \mid y_n)</span> be a sequence of transition kernels. We define the predictive distribution <span class="orange">recursively</span>: <span class="math display">
  P_n(\cdot) = \mathbb{P}(Y_{n+1}\in \cdot\mid y_{1:n}) = (1-w_n) P_{n-1}(\cdot) + w_n K_{n}(\cdot\mid y_n), \qquad n \ge 1,
</span> where <span class="math inline">(w_n)_{n\ge1}</span> are decreasing weights such that <span class="math inline">w_n\in(0,1)</span>, <span class="math inline">\sum_{n\ge1}w_n = \infty</span>, and <span class="math inline">\sum_{n\ge1}w_n^2&lt;\infty</span>.</p></li>
<li><p>The choice of weights <span class="math inline">w_n = (\alpha +n)^{-1}</span> gives the following DP-like predictive rule <span class="math display">
P_n(\cdot) = \frac{\alpha}{\alpha + n} P_0(\cdot) + \frac{1}{\alpha + n}\sum_{i=1}^nK_i(\cdot\mid y_i).
</span> Hence, the predictive law of a DP is a special case whenever <span class="math inline">K_i(\cdot \mid y_i) = \delta_{y_i}(\cdot)</span>.</p></li>
</ul>
<div class="callout callout-tip no-icon callout-style-simple">
<div class="callout-body">
<div class="callout-content">
<p>The above sequence, beyond the DP special case, is <span class="orange">not exchangeable</span> and it will <span class="blue">depend on the order</span> of the data. Moreover, without further restrictions, is not necessarily CID!</p>
</div>
</div>
</div>
</div>
</section>
<section id="metropolis-adjusted-dirichlet-mad-sequences" class="slide level2 center">
<h2>Metropolis-adjusted Dirichlet (MAD) sequences</h2>
<ul>
<li><p>We assume that <span class="math inline">K_{n}(\cdot\mid y_n)</span> is a <span class="blue">Metropolis-Hastings kernel</span> centered in <span class="math inline">y_n</span> having pmf: <span class="math display">
  k_{n}(y\mid y_n) = \gamma_{n}(y,y_n) k_*(y\mid y_n) + \mathbb{1}(y=y_n)\Big[\sum_{z\in\mathcal{Y}}\big\{1-\gamma_{n}(z,y_n)\big\}k_*(z\mid y_n)\Big],
</span> with acceptance probability <span class="math display">
  \gamma_{n}(y,y_n) = \gamma(y, y_n, P_{n-1}) =
  \min\left\{1,\frac{p_{n-1}(y) k_*(y_n\mid y)}{p_{n-1}(y_n) k_*(y\mid y_n)}\right\},
</span> where <span class="math inline">p_{n-1}</span> is the probability mass functions associated to <span class="math inline">P_{n-1}</span> and <span class="math inline">k_*(\cdot\mid y_n)</span> is the pmf of a <span class="orange">discrete base kernel</span> centered at <span class="math inline">y_n</span>.</p></li>
<li><p>We refer to <span class="math inline">P_n</span> above as the <span class="blue">Metropolis-adjusted Dirichlet</span> (MAD) distribution with weights <span class="math inline">(w_n)_{n\ge1}</span>, base kernel <span class="math inline">k_*</span> and initial distribution <span class="math inline">P_0</span>. We call <span class="math inline">(Y_n)_{n \ge 1}</span> a MAD sequence.</p></li>
</ul>
<div class="fragment">
<div title="Theorem (Agnoletto, R. and Dunson, 2025)">
<div class="callout callout-warning no-icon callout-titled callout-style-simple">
<div class="callout-body">
<div class="callout-title">
<p><strong>Theorem (Agnoletto, R. and Dunson, 2025)</strong></p>
</div>
<div class="callout-content">
<p>Let <span class="math inline">(Y_n)_{n\ge1}</span> be a MAD sequence. Then, for every set of weights <span class="math inline">(w_n)_{n \ge 1}</span>, discrete base kernel <span class="math inline">k_*</span>, and initial distribution <span class="math inline">P_0</span>, the sequence <span class="math inline">(Y_n)_{n\ge1}</span> is <span class="blue">conditionally identically distributed (CID)</span>.</p>
</div>
</div>
</div>
</div>
</div>
</section>
<section id="bayesian-properties-of-cid-sequences-i" class="slide level2 center">
<h2>Bayesian properties of CID sequences I</h2>
<div title="Corollary [@aldous1985exchangeability; @berti2004limit]">
<div class="callout callout-warning no-icon callout-titled callout-style-simple">
<div class="callout-body">
<div class="callout-title">
<p><strong>Corollary <span class="citation" data-cites="aldous1985exchangeability berti2004limit">(<a href="#/references" role="doc-biblioref" onclick="">Aldous 1985</a>; <a href="#/references" role="doc-biblioref" onclick="">Berti et al. 2004</a>)</span></strong></p>
</div>
<div class="callout-content">
<p>Consider a MAD sequence <span class="math inline">(Y_n)_{n\ge1}</span>. Then, <span class="math inline">\mathbb{P}</span>-a.s.,</p>
<p><span class="math inline">(a)</span> The sequence is asymptotically exchangeable, that is <span class="math display">
    (Y_{n+1}, Y_{n+2}, \ldots) \overset{\textup{d}}{\longrightarrow} (Z_1, Z_2, \ldots), \qquad n \rightarrow \infty,
</span> where <span class="math inline">(Z_1,Z_2,\ldots)</span> is an exchangeable sequence with directing random probability measure <span class="math inline">P</span>;</p>
<p><span class="math inline">(b)</span> the corresponding sequence of predictive distributions <span class="math inline">(P_n)_{n\ge1}</span> weakly converge to a random probability measures <span class="math inline">P</span> (a.s. <span class="math inline">\mathbb{P}</span>).</p>
</div>
</div>
</div>
</div>
<ul>
<li><p>An <span class="blue">asymptotic</span> equivalent of <span class="blue">de Finetti</span>’s theorem holds: each MAD sequence has a corresponding unique prior on <span class="math inline">P</span>.</p></li>
<li><p>The ordering dependence will vanish asymptotically and, informally, <span class="math inline">Y_i \mid P \overset{\mathrm{iid}}{\sim} P</span> <span class="blue">for large <span class="math inline">n</span></span>.</p></li>
<li><p>The random probability measure <span class="math inline">P</span> exists and is defined as the limit of the predictive distributions. However, it is <span class="orange">not available explicitly</span>.</p></li>
</ul>
</section>
<section id="bayesian-properties-of-cid-sequences-ii" class="slide level2 center">
<h2>Bayesian properties of CID sequences II</h2>
<div title="Corollary [@aldous1985exchangeability; @berti2004limit]">
<div class="callout callout-warning no-icon callout-titled callout-style-simple">
<div class="callout-body">
<div class="callout-title">
<p><strong>Corollary <span class="citation" data-cites="aldous1985exchangeability berti2004limit">(<a href="#/references" role="doc-biblioref" onclick="">Aldous 1985</a>; <a href="#/references" role="doc-biblioref" onclick="">Berti et al. 2004</a>)</span></strong></p>
</div>
<div class="callout-content">
<p>Let <span class="math inline">\theta = P(f) = \sum_{y \in \mathcal{Y}} f(y) p(y)</span> and analogously <span class="math inline">\theta_n = P_n(f) = \sum_{y \in \mathcal{Y}} f(y) p_n(y)</span> be any functional of interest. Consider a MAD sequence <span class="math inline">(Y_n)_{n\ge1}</span>.</p>
<p>Then, <span class="math inline">\mathbb{P}</span>-a.s., for every <span class="math inline">n\ge1</span> and every integrable function <span class="math inline">f:\mathcal{Y}\rightarrow\mathbb{R}</span>, we have <span class="math display">
\mathbb{E}(\theta \mid y_{1:n}) = \mathbb{E}\{P(f) \mid y_{1:n}\} = P_n(f) = \theta_n
</span></p>
</div>
</div>
</div>
</div>
<ul>
<li><p>Broadly speaking, the posterior mean of any functional of interest of <span class="math inline">P</span> coincides with the functional of the predictive.</p></li>
<li><p>Moreover, <span class="math inline">\mathbb{E}\{P(f)\}= P_0(f) = \theta_0</span> for every integrable function <span class="math inline">f</span>, so that <span class="math inline">P_0</span> retains the role of a <span class="blue">base measure</span> as for standard Dirichlet sequences, providing an initial guess at <span class="math inline">P</span>.</p></li>
</ul>
<div class="fragment">
<ul>
<li>Uncertainty quantification for <span class="math inline">\theta=P(f)</span> is carried out by <span class="blue">predictive resampling</span> <span class="citation" data-cites="fong_holmes_2023">(<a href="#/references" role="doc-biblioref" onclick="">Fong et al. 2023</a>)</span>.</li>
</ul>
</div>
</section>
<section id="predictive-resampling-for-mad-sequences" class="slide level2 center">
<h2>Predictive resampling for MAD sequences</h2>
<p><strong>Algorithm <span class="citation" data-cites="fortini_petrone_2020">(<a href="#/references" role="doc-biblioref" onclick="">Fortini and Petrone 2020</a>)</span>:</strong></p>
<ol type="1">
<li>Compute <span class="math inline">P_n(\cdot)</span> from the observed data <span class="math inline">y_{1:n}</span></li>
<li>Set <span class="math inline">N\gg n</span></li>
<li>For <span class="math inline">j = 1,\ldots,B</span>
<ol type="a">
<li>For <span class="math inline">i=n+1,\ldots,N</span>
<ol type="i">
<li>Sample <span class="math inline">Y_i\mid y_{1:i-1}\sim P_{i-1}</span></li>
<li>Update <span class="math inline">P_i(\cdot) = (1-w_i)P_{i-1}(\cdot) + w_i K_{i-1}(\cdot\mid y_i)</span></li>
</ol></li>
<li>End For</li>
</ol></li>
<li>End For</li>
<li>Return <span class="math inline">P_N^{(1)}(\cdot),\ldots,P_N^{(B)}(\cdot)</span>, an iid sample from the distribution of <span class="math inline">P_N(\cdot)\mid y_{1:n}</span></li>
</ol>
</section>
<section id="on-the-choice-of-the-base-kernel" class="slide level2 center">
<h2>On the choice of the base kernel</h2>
<ul>
<li>In principle, any discrete distribution can be chosen as the base kernel <span class="math inline">k_*</span>. However, it is natural to consider choices that allow the kernel to be centered at <span class="math inline">y_n</span> while permitting control over the variance.</li>
</ul>
<div class="fragment">
<ul>
<li>We consider a <span class="blue">rounded Gaussian</span> distribution centered in <span class="math inline">y_n</span>, with pmf <span class="math display">
  k_*(y\mid y_n, \sigma) = \frac{\int_{y-1/2}^{y+1/2}\mathcal{N}(t\mid y_n, \sigma^2) \mathrm{d}t}{\sum_{z\in\mathcal{Y}} \int_{z-1/2}^{z+1/2}\mathcal{N}(t\mid y_n, \sigma^2) \mathrm{d}t},
</span> for <span class="math inline">n\ge1</span>, where <span class="math inline">\mathcal{N}(\cdot\mid y_n,\sigma^2)</span> denotes a normal density function with mean <span class="math inline">y_n</span> and variance <span class="math inline">\sigma^2</span>.</li>
</ul>
</div>
<div class="fragment">
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img data-src="img/ker_plot_slide.jpg" class="quarto-figure quarto-figure-center" width="850"></p>
</figure>
</div>
</div>
</section>
<section id="role-of-the-weights-in-controlling-posterior-variability" class="slide level2 center">
<h2>Role of the weights in controlling posterior variability</h2>
<ul>
<li>It can be shown that the distribution of <span class="math inline">P(A)\mid y_{1:n}</span> is approximated by <span class="math inline">\mathcal{N}(P_n(A), \Sigma_n r_n^{-1})</span> for <span class="math inline">n</span> large, where the variance is <span class="math display">
  \Sigma_n r_n^{-1}
  \approx \mathbb{E}\{[P_{n+1}(A)-P_n(A)]^2\mid y_{1:n}\}\sum_{k&gt;n+1}w_k^2.
</span></li>
</ul>
<div class="fragment">
<ul>
<li><p>Weights that <span class="blue">decay to zero quickly</span> induce <span class="blue">fast learning and convergence</span> to the asymptotic exchangeable regime.</p></li>
<li><p>But <span class="orange">small values of <span class="math inline">w_n</span></span> leads to <span class="orange">poor learning</span> and <span class="orange">underestimation</span> of the <span class="orange">posterior variability</span>.</p></li>
</ul>
</div>
<div class="fragment">
<ul>
<li><p>Possible choices are <span class="math inline">w_n=(\alpha+n)^{-1}</span>, <span class="math inline">w_n=(\alpha+n)^{-2/3}</span> <span class="citation" data-cites="martin2009asymptotic">(<a href="#/references" role="doc-biblioref" onclick="">Martin and Tokdar 2009</a>)</span>, and <span class="math inline">w_n=(2 - n^{-1})(n+1)^{-1}</span> <span class="citation" data-cites="fong_holmes_2023">(<a href="#/references" role="doc-biblioref" onclick="">Fong et al. 2023</a>)</span>.</p></li>
<li><p>We consider <span class="blue">adaptive weights</span> <span class="math display">
  w_n=(\alpha+n)^{-\lambda_n}, \qquad \lambda_n=\lambda+(1+\lambda)\exp\bigg\{-\frac{1}{N_*}n\bigg\},
</span> with <span class="math inline">\lambda\in(0.5,1]</span>, <span class="math inline">N_*&gt;0</span>.</p></li>
</ul>
</div>
</section>
<section id="illustrative-example-ii" class="slide level2 center">
<h2>Illustrative example II</h2>
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img data-src="img/example_complete.jpg" class="quarto-figure quarto-figure-center" width="850"></p>
</figure>
</div>
</section>
<section id="multivariate-count-and-binary-data" class="slide level2 center">
<h2>Multivariate count and binary data</h2>
<ul>
<li>Extending MAD sequences for <span class="blue">multivariate</span> data is straightforward using a <span class="blue">factorized base kernel</span> <span class="math display">
k_*(\bm y\mid\bm y_n) = \prod_{j=1}^d k_*(y_j\mid y_{n,j}),
</span> with <span class="math inline">\bm y=(y_1,\ldots,y_d)</span> and <span class="math inline">\bm y_n=(y_{n,1},\ldots,y_{n,d})</span>.</li>
</ul>
<div class="fragment">
<ul>
<li>MAD sequences can be employed for modeling <span class="orange">multivariate binary data</span> using an appropriate base kernel.</li>
</ul>
</div>
<div class="fragment">
<ul>
<li>A natural step further is to use MAD sequences for <span class="orange">nonparametric regression and classification</span>.</li>
</ul>
</div>
</section>
<section id="simulations-i" class="slide level2 center">
<h2>Simulations I</h2>
<p>Out-of-sample prediction accuracy evaluated in terms of MSE and AUC for regression and classification, respectively.</p>
<table class="caption-top" style="width:100%;">
<colgroup>
<col style="width: 16%">
<col style="width: 16%">
<col style="width: 16%">
<col style="width: 16%">
<col style="width: 16%">
<col style="width: 16%">
</colgroup>
<thead>
<tr class="header">
<th></th>
<th>Regression (MSE)</th>
<th></th>
<th></th>
<th>Classification (AUC)</th>
<th></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td></td>
<td><strong><span class="math inline">n=40</span></strong></td>
<td><strong><span class="math inline">n=80</span></strong></td>
<td></td>
<td><strong><span class="math inline">n=150</span></strong></td>
<td><strong><span class="math inline">n=300</span></strong></td>
</tr>
<tr class="even">
<td>GLM</td>
<td>120.77 [51.51]</td>
<td>94.93 [8.37]</td>
<td></td>
<td>0.796 [0.014]</td>
<td>0.809 [0.007]</td>
</tr>
<tr class="odd">
<td>BART</td>
<td>101.17 [12.69]</td>
<td>74.17 [10.00]</td>
<td></td>
<td>0.863 [0.026]</td>
<td>0.932 [0.009]</td>
</tr>
<tr class="even">
<td>RF</td>
<td>99.98 [7.45]</td>
<td>87.75 [6.53]</td>
<td></td>
<td>0.882 [0.025]</td>
<td>0.913 [0.015]</td>
</tr>
<tr class="odd">
<td>DP</td>
<td>1450.21 [5.53]</td>
<td>1395.61 [8.72]</td>
<td></td>
<td>0.644 [0.011]</td>
<td>0.724 [0.012]</td>
</tr>
<tr class="even">
<td><strong>MAD-1</strong></td>
<td>91.07 [10.35]</td>
<td>73.96 [7.60]</td>
<td></td>
<td>0.873 [0.014]</td>
<td>0.899 [0.008]</td>
</tr>
<tr class="odd">
<td><strong>MAD-2/3</strong></td>
<td>88.83 [13.00]</td>
<td>73.18 [9.58]</td>
<td></td>
<td>0.869 [0.015]</td>
<td>0.899 [0.009]</td>
</tr>
<tr class="even">
<td><strong>MAD-dpm</strong></td>
<td>87.41 [12.36]</td>
<td>72.07 [9.48]</td>
<td></td>
<td>0.872 [0.014]</td>
<td>0.901 [0.008]</td>
</tr>
<tr class="odd">
<td><strong>MAD-ada</strong></td>
<td><strong>90.61 [10.28]</strong></td>
<td><strong>73.45 [7.69]</strong></td>
<td></td>
<td><strong>0.874 [0.014]</strong></td>
<td><strong>0.900 [0.008]</strong></td>
</tr>
</tbody>
</table>
</section>
<section id="simulations-ii" class="slide level2 center">
<h2>Simulations II</h2>

<img data-src="img/pl_cov_sim.jpg" class="quarto-figure quarto-figure-center r-stretch" style="width:85.0%"></section>
<section id="application" class="slide level2 center">
<h2>Application</h2>
<ul>
<li>We analyze the occurrence rates of 4 species corvids in Finland in year 2009 across different temperatures and habitats.</li>
</ul>

<img data-src="img/pl_appl_slide_1.jpg" class="quarto-figure quarto-figure-center r-stretch" style="width:65.0%"></section>
<section id="application-ii" class="slide level2 center">
<h2>Application II</h2>

<img data-src="img/pl_appl_slide_2.jpg" class="quarto-figure quarto-figure-center r-stretch" style="width:85.0%"></section>
<section id="thank-you" class="slide level2 center">
<h2>Thank you!</h2>

<img data-src="img/QR.png" class="quarto-figure quarto-figure-center r-stretch" style="width:2.5in"><p>The <span class="orange">main paper</span> is:</p>
<p>Agnoletto, D., Rigon, T., and Dunson D.B. (2025+). Nonparametric predictive inference for discrete data via Metropolis-adjusted Dirichlet sequences. <em>arXiv:2507.08629</em></p>
</section>
<section id="references" class="slide level2 unnumbered smaller scrollable">
<h2>References</h2>
<div id="refs" class="references csl-bib-body hanging-indent" data-entry-spacing="0" role="list">
<div id="ref-Agnoletto2025" class="csl-entry" role="listitem">
Agnoletto, D., Rigon, T., and Dunson, D. B. (2025), <span>“Bayesian inference for generalized linear models via quasi-posteriors,”</span> <em>Biometrika</em>, 112.
</div>
<div id="ref-aldous1985exchangeability" class="csl-entry" role="listitem">
Aldous, D. J. (1985), <span>“Exchangeability and related topics, ecole d’et<span>é</span> de saint-flour XIII, lectures notes n 1117,”</span> Springer Verlag.
</div>
<div id="ref-berti2023kernel" class="csl-entry" role="listitem">
Berti, P., Dreassi, E., Leisen, F., Pratelli, L., and Rigo, P. (2023b), <span>“Kernel based dirichlet sequences,”</span> <em>Bernoulli</em>, Bernoulli Society for Mathematical Statistics; Probability, 29, 1321–1342.
</div>
<div id="ref-berti2023without" class="csl-entry" role="listitem">
Berti, P., Dreassi, E., Leisen, F., Pratelli, L., and Rigo, P. (2023a), <span>“Bayesian predictive inference without a prior,”</span> <em>Statistica Sinica</em>, Academia Sinica, Institute of Statistical Science, 34, 2405–2429.
</div>
<div id="ref-berti_pratelli_2021" class="csl-entry" role="listitem">
Berti, P., Dreassi, E., Pratelli, L., and Rigo, P. (2021), <span>“A class of models for <span>B</span>ayesian predictive inference,”</span> <em>Bernoulli</em>, 27, 702–726.
</div>
<div id="ref-berti2004limit" class="csl-entry" role="listitem">
Berti, P., Pratelli, L., and Rigo, P. (2004), <span>“Limit theorems for a class of identically distributed random variables,”</span> 32, 2029–2052.
</div>
<div id="ref-bissiri2016general" class="csl-entry" role="listitem">
Bissiri, P. G., Holmes, C. C., and Walker, S. G. (2016), <span>“A general framework for updating belief distributions,”</span> <em>Journal of the Royal Statistical Society: Series B (Statistical Methodology)</em>, Wiley Online Library, 78, 1103–1130.
</div>
<div id="ref-canale2011bayesian" class="csl-entry" role="listitem">
Canale, A., and Dunson, D. B. (2011), <span>“Bayesian kernel mixtures for counts,”</span> <em>Journal of the American Statistical Association</em>, Taylor &amp; Francis, 106, 1528–1539.
</div>
<div id="ref-chernozhukov2003mcmc" class="csl-entry" role="listitem">
Chernozhukov, V., and Hong, H. (2003), <span>“An <span>MCMC</span> approach to classical estimation,”</span> <em>Journal of econometrics</em>, Elsevier, 115, 293–346.
</div>
<div id="ref-cui2024martingale" class="csl-entry" role="listitem">
Cui, F., and Walker, S. G. (2024), <span>“Martingale posterior distributions for log-concave density functions,”</span> <em>arXiv preprint arXiv:2401.14515</em>.
</div>
<div id="ref-de1937prevision" class="csl-entry" role="listitem">
De Finetti, B. (1937), <span>“La pr<span>é</span>vision: Ses lois logiques, ses sources subjectives,”</span> in <em>Annales de l’institut henri poincar<span>é</span></em>, pp. 1–68.
</div>
<div id="ref-fong_holmes_2023" class="csl-entry" role="listitem">
Fong, E., Holmes, C. C., and Walker, S. G. (2023), <span>“Martingale posterior distributions,”</span> <em>Journal of the Royal Statistical Society Series B: Statistical Methodology</em>, 85, 1357–1391.
</div>
<div id="ref-fong2024bayesian" class="csl-entry" role="listitem">
Fong, E., and Yiu, A. (2024b), <span>“Bayesian quantile estimation and regression with martingale posteriors,”</span> <em>arXiv preprint arXiv:2406.03358</em>.
</div>
<div id="ref-fong2024asymptotics" class="csl-entry" role="listitem">
Fong, E., and Yiu, A. (2024a), <span>“Asymptotics for parametric martingale posteriors,”</span> <em>arXiv preprint arXiv:2410.17692</em>.
</div>
<div id="ref-fortini2000exchangeability" class="csl-entry" role="listitem">
Fortini, S., Ladelli, L., and Regazzini, E. (2000), <span>“Exchangeability, predictive distributions and parametric models,”</span> <em>Sankhya: The Indian Journal of Statistics, Series A</em>, 62, 86–109.
</div>
<div id="ref-fortini2012predictive" class="csl-entry" role="listitem">
Fortini, S., and Petrone, S. (2012), <span>“Predictive construction of priors in bayesian nonparametrics,”</span> <em>Brazilian Journal of Probability and Statistics</em>, 26, 423–449.
</div>
<div id="ref-fortini_petrone_2020" class="csl-entry" role="listitem">
Fortini, S., and Petrone, S. (2020), <span>“Quasi-<span>B</span>ayes properties of a procedure for sequential learning in mixture models,”</span> <em>Journal of the Royal Statistical Society Series B: Statistical Methodology</em>, Oxford University Press, 82, 1087–1114.
</div>
<div id="ref-fortini2024exchangeability" class="csl-entry" role="listitem">
Fortini, S., and Petrone, S. (2024), <span>“Exchangeability, prediction and predictive modeling in bayesian statistics,”</span> <em>arXiv preprint arXiv:2402.10126</em>.
</div>
<div id="ref-fortini2021predictive" class="csl-entry" role="listitem">
Fortini, S., Petrone, S., and Sariev, H. (2021), <span>“Predictive constructions based on measure-valued p<span>ó</span>lya urn processes,”</span> <em>Mathematics</em>, MDPI, 9, 2845.
</div>
<div id="ref-grunwald2020fast" class="csl-entry" role="listitem">
Grünwald, P. D., and Mehta, N. A. (2020), <span>“<span class="nocase">Fast rates for general unbounded loss functions: from ERM to generalized Bayes</span>,”</span> <em>The Journal of Machine Learning Research</em>, JMLRORG, 21, 2040–2119.
</div>
<div id="ref-heide2020safe" class="csl-entry" role="listitem">
Heide, R. de, Kirichenko, A., Grunwald, P., and Mehta, N. (2020), <span>“Safe-bayesian generalized linear regression,”</span> in <em>Proceedings of the twenty third international conference on artificial intelligence and statistics</em>, PMLR, pp. 2623–2633.
</div>
<div id="ref-holmes2017assigning" class="csl-entry" role="listitem">
Holmes, C. C., and Walker, S. G. (2017), <span>“Assigning a value to a power likelihood in a general <span>B</span>ayesian model,”</span> <em>Biometrika</em>, Oxford University Press, 104, 497–503.
</div>
<div id="ref-jewson_rossell_2022" class="csl-entry" role="listitem">
Jewson, J., and Rossell, D. (2022), <span>“General bayesian loss function selection and the use of improper models,”</span> <em>Journal of the Royal Statistical Society Series B: Statistical Methodology</em>, Oxford University Press, 84, 1640–1665.
</div>
<div id="ref-knoblauch2022optimization" class="csl-entry" role="listitem">
Knoblauch, J., Jewson, J., and Damoulas, T. (2022), <span>“An optimization-centric view on bayes’ rule: Reviewing and generalizing variational inference,”</span> <em>Journal of Machine Learning Research</em>, 23, 1–109.
</div>
<div id="ref-lyddon2019general" class="csl-entry" role="listitem">
Lyddon, S. P., Holmes, C. C., and Walker, S. G. (2019), <span>“General <span>B</span>ayesian updating and the loss-likelihood bootstrap,”</span> <em>Biometrika</em>, Oxford University Press, 106, 465–478.
</div>
<div id="ref-martin2009asymptotic" class="csl-entry" role="listitem">
Martin, R., and Tokdar, S. T. (2009), <span>“Asymptotic properties of predictive recursion: Robustness and rate of convergence,”</span> <em>Electornic Journal of Statistics</em>, 3, 1455–1472.
</div>
<div id="ref-matsubara2022robust" class="csl-entry" role="listitem">
Matsubara, T., Knoblauch, J., Briol, F.-X., and Oates, C. J. (2022), <span>“Robust generalised bayesian inference for intractable likelihoods,”</span> <em>Journal of the Royal Statistical Society Series B: Statistical Methodology</em>, Oxford University Press, 84, 997–1022.
</div>
<div id="ref-matsubara2023generalized" class="csl-entry" role="listitem">
Matsubara, T., Knoblauch, J., Briol, F.-X., and Oates, C. J. (2023), <span>“Generalized bayesian inference for discrete intractable likelihood,”</span> <em>Journal of the American Statistical Association</em>, Taylor &amp; Francis, 1–11.
</div>
<div id="ref-rigon2023generalized" class="csl-entry" role="listitem">
Rigon, T., Herring, A. H., and Dunson, D. B. (2023), <span>“A generalized <span>B</span>ayes framework for probabilistic clustering,”</span> <em>Biometrika</em>, Oxford University Press, 10, 559–578.
</div>
<div id="ref-syring2019calibrating" class="csl-entry" role="listitem">
Syring, N., and Martin, R. (2019), <span>“Calibrating general posterior credible regions,”</span> <em>Biometrika</em>, Oxford University Press, 106, 479–486.
</div>
</div>


</section>
    </div>
  <div class="quarto-auto-generated-content" style="display: none;">
<p><img src="../../images/logoB.png" class="slide-logo"></p>
<div class="footer footer-default">
<p><a href="https://tommasorigon.github.io/">Home page</a></p>
</div>
</div></div>

  <script>window.backupDefine = window.define; window.define = undefined;</script>
  <script src="../../site_libs/revealjs/dist/reveal.js"></script>
  <!-- reveal.js plugins -->
  <script src="../../site_libs/revealjs/plugin/quarto-line-highlight/line-highlight.js"></script>
  <script src="../../site_libs/revealjs/plugin/pdf-export/pdfexport.js"></script>
  <script src="../../site_libs/revealjs/plugin/reveal-menu/menu.js"></script>
  <script src="../../site_libs/revealjs/plugin/reveal-menu/quarto-menu.js"></script>
  <script src="../../site_libs/revealjs/plugin/quarto-support/support.js"></script>
  

  <script src="../../site_libs/revealjs/plugin/notes/notes.js"></script>
  <script src="../../site_libs/revealjs/plugin/search/search.js"></script>
  <script src="../../site_libs/revealjs/plugin/zoom/zoom.js"></script>
  <script>window.define = window.backupDefine; window.backupDefine = undefined;</script>

  <script>

      // Full list of configuration options available at:
      // https://revealjs.com/config/
      Reveal.initialize({
'controlsAuto': true,
'previewLinksAuto': false,
'pdfSeparateFragments': false,
'autoAnimateEasing': "ease",
'autoAnimateDuration': 1,
'autoAnimateUnmatched': true,
'jumpToSlide': true,
'menu': {"side":"left","useTextContentForMissingTitles":true,"markers":false,"loadIcons":false,"custom":[{"title":"Tools","icon":"<i class=\"fas fa-gear\"></i>","content":"<ul class=\"slide-menu-items\">\n<li class=\"slide-tool-item active\" data-item=\"0\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.fullscreen(event)\"><kbd>f</kbd> Fullscreen</a></li>\n<li class=\"slide-tool-item\" data-item=\"1\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.speakerMode(event)\"><kbd>s</kbd> Speaker View</a></li>\n<li class=\"slide-tool-item\" data-item=\"2\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.overview(event)\"><kbd>o</kbd> Slide Overview</a></li>\n<li class=\"slide-tool-item\" data-item=\"3\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.togglePdfExport(event)\"><kbd>e</kbd> PDF Export Mode</a></li>\n<li class=\"slide-tool-item\" data-item=\"4\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.toggleScrollView(event)\"><kbd>r</kbd> Scroll View Mode</a></li>\n<li class=\"slide-tool-item\" data-item=\"5\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.keyboardHelp(event)\"><kbd>?</kbd> Keyboard Help</a></li>\n</ul>"}],"openButton":true},
'smaller': false,
 
        // Display controls in the bottom right corner
        controls: false,

        // Help the user learn the controls by providing hints, for example by
        // bouncing the down arrow when they first encounter a vertical slide
        controlsTutorial: false,

        // Determines where controls appear, "edges" or "bottom-right"
        controlsLayout: 'edges',

        // Visibility rule for backwards navigation arrows; "faded", "hidden"
        // or "visible"
        controlsBackArrows: 'faded',

        // Display a presentation progress bar
        progress: true,

        // Display the page number of the current slide
        slideNumber: 'c/t',

        // 'all', 'print', or 'speaker'
        showSlideNumber: 'all',

        // Add the current slide number to the URL hash so that reloading the
        // page/copying the URL will return you to the same slide
        hash: true,

        // Start with 1 for the hash rather than 0
        hashOneBasedIndex: false,

        // Flags if we should monitor the hash and change slides accordingly
        respondToHashChanges: true,

        // Push each slide change to the browser history
        history: true,

        // Enable keyboard shortcuts for navigation
        keyboard: true,

        // Enable the slide overview mode
        overview: true,

        // Disables the default reveal.js slide layout (scaling and centering)
        // so that you can use custom CSS layout
        disableLayout: false,

        // Vertical centering of slides
        center: true,

        // Enables touch navigation on devices with touch input
        touch: true,

        // Loop the presentation
        loop: false,

        // Change the presentation direction to be RTL
        rtl: false,

        // see https://revealjs.com/vertical-slides/#navigation-mode
        navigationMode: 'linear',

        // Randomizes the order of slides each time the presentation loads
        shuffle: false,

        // Turns fragments on and off globally
        fragments: true,

        // Flags whether to include the current fragment in the URL,
        // so that reloading brings you to the same fragment position
        fragmentInURL: false,

        // Flags if the presentation is running in an embedded mode,
        // i.e. contained within a limited portion of the screen
        embedded: false,

        // Flags if we should show a help overlay when the questionmark
        // key is pressed
        help: true,

        // Flags if it should be possible to pause the presentation (blackout)
        pause: true,

        // Flags if speaker notes should be visible to all viewers
        showNotes: false,

        // Global override for autoplaying embedded media (null/true/false)
        autoPlayMedia: null,

        // Global override for preloading lazy-loaded iframes (null/true/false)
        preloadIframes: null,

        // Number of milliseconds between automatically proceeding to the
        // next slide, disabled when set to 0, this value can be overwritten
        // by using a data-autoslide attribute on your slides
        autoSlide: 0,

        // Stop auto-sliding after user input
        autoSlideStoppable: true,

        // Use this method for navigation when auto-sliding
        autoSlideMethod: null,

        // Specify the average time in seconds that you think you will spend
        // presenting each slide. This is used to show a pacing timer in the
        // speaker view
        defaultTiming: null,

        // Enable slide navigation via mouse wheel
        mouseWheel: false,

        // The display mode that will be used to show slides
        display: 'block',

        // Hide cursor if inactive
        hideInactiveCursor: true,

        // Time before the cursor is hidden (in ms)
        hideCursorTime: 5000,

        // Opens links in an iframe preview overlay
        previewLinks: false,

        // Transition style (none/fade/slide/convex/concave/zoom)
        transition: 'none',

        // Transition speed (default/fast/slow)
        transitionSpeed: 'default',

        // Transition style for full page slide backgrounds
        // (none/fade/slide/convex/concave/zoom)
        backgroundTransition: 'none',

        // Number of slides away from the current that are visible
        viewDistance: 3,

        // Number of slides away from the current that are visible on mobile
        // devices. It is advisable to set this to a lower number than
        // viewDistance in order to save resources.
        mobileViewDistance: 2,

        // The "normal" size of the presentation, aspect ratio will be preserved
        // when the presentation is scaled to fit different resolutions. Can be
        // specified using percentage units.
        width: 1050,

        height: 700,

        // Factor of the display size that should remain empty around the content
        margin: 0.1,

        // reveal.js plugins
        plugins: [QuartoLineHighlight, PdfExport, RevealMenu, QuartoSupport,

          RevealNotes,
          RevealSearch,
          RevealZoom
        ]
      });
    </script>
    <script id="quarto-html-after-body" type="application/javascript">
      window.document.addEventListener("DOMContentLoaded", function (event) {
        const tabsets =  window.document.querySelectorAll(".panel-tabset-tabby")
        tabsets.forEach(function(tabset) {
          const tabby = new Tabby('#' + tabset.id);
        });
        const isCodeAnnotation = (el) => {
          for (const clz of el.classList) {
            if (clz.startsWith('code-annotation-')) {                     
              return true;
            }
          }
          return false;
        }
        const onCopySuccess = function(e) {
          // button target
          const button = e.trigger;
          // don't keep focus
          button.blur();
          // flash "checked"
          button.classList.add('code-copy-button-checked');
          var currentTitle = button.getAttribute("title");
          button.setAttribute("title", "Copied!");
          let tooltip;
          if (window.bootstrap) {
            button.setAttribute("data-bs-toggle", "tooltip");
            button.setAttribute("data-bs-placement", "left");
            button.setAttribute("data-bs-title", "Copied!");
            tooltip = new bootstrap.Tooltip(button, 
              { trigger: "manual", 
                customClass: "code-copy-button-tooltip",
                offset: [0, -8]});
            tooltip.show();    
          }
          setTimeout(function() {
            if (tooltip) {
              tooltip.hide();
              button.removeAttribute("data-bs-title");
              button.removeAttribute("data-bs-toggle");
              button.removeAttribute("data-bs-placement");
            }
            button.setAttribute("title", currentTitle);
            button.classList.remove('code-copy-button-checked');
          }, 1000);
          // clear code selection
          e.clearSelection();
        }
        const getTextToCopy = function(trigger) {
            const codeEl = trigger.previousElementSibling.cloneNode(true);
            for (const childEl of codeEl.children) {
              if (isCodeAnnotation(childEl)) {
                childEl.remove();
              }
            }
            return codeEl.innerText;
        }
        const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
          text: getTextToCopy
        });
        clipboard.on('success', onCopySuccess);
        if (window.document.getElementById('quarto-embedded-source-code-modal')) {
          const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
            text: getTextToCopy,
            container: window.document.getElementById('quarto-embedded-source-code-modal')
          });
          clipboardModal.on('success', onCopySuccess);
        }
          var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
          var mailtoRegex = new RegExp(/^mailto:/);
            var filterRegex = new RegExp('/' + window.location.host + '/');
          var isInternal = (href) => {
              return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
          }
          // Inspect non-navigation links and adorn them if external
         var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
          for (var i=0; i<links.length; i++) {
            const link = links[i];
            if (!isInternal(link.href)) {
              // undo the damage that might have been done by quarto-nav.js in the case of
              // links that we want to consider external
              if (link.dataset.originalHref !== undefined) {
                link.href = link.dataset.originalHref;
              }
            }
          }
        function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
          const config = {
            allowHTML: true,
            maxWidth: 500,
            delay: 100,
            arrow: false,
            appendTo: function(el) {
                return el.closest('section.slide') || el.parentElement;
            },
            interactive: true,
            interactiveBorder: 10,
            theme: 'light-border',
            placement: 'bottom-start',
          };
          if (contentFn) {
            config.content = contentFn;
          }
          if (onTriggerFn) {
            config.onTrigger = onTriggerFn;
          }
          if (onUntriggerFn) {
            config.onUntrigger = onUntriggerFn;
          }
            config['offset'] = [0,0];
            config['maxWidth'] = 700;
          window.tippy(el, config); 
        }
        const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
        for (var i=0; i<noterefs.length; i++) {
          const ref = noterefs[i];
          tippyHover(ref, function() {
            // use id or data attribute instead here
            let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
            try { href = new URL(href).hash; } catch {}
            const id = href.replace(/^#\/?/, "");
            const note = window.document.getElementById(id);
            if (note) {
              return note.innerHTML;
            } else {
              return "";
            }
          });
        }
        const findCites = (el) => {
          const parentEl = el.parentElement;
          if (parentEl) {
            const cites = parentEl.dataset.cites;
            if (cites) {
              return {
                el,
                cites: cites.split(' ')
              };
            } else {
              return findCites(el.parentElement)
            }
          } else {
            return undefined;
          }
        };
        var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
        for (var i=0; i<bibliorefs.length; i++) {
          const ref = bibliorefs[i];
          const citeInfo = findCites(ref);
          if (citeInfo) {
            tippyHover(citeInfo.el, function() {
              var popup = window.document.createElement('div');
              citeInfo.cites.forEach(function(cite) {
                var citeDiv = window.document.createElement('div');
                citeDiv.classList.add('hanging-indent');
                citeDiv.classList.add('csl-entry');
                var biblioDiv = window.document.getElementById('ref-' + cite);
                if (biblioDiv) {
                  citeDiv.innerHTML = biblioDiv.innerHTML;
                }
                popup.appendChild(citeDiv);
              });
              return popup.innerHTML;
            });
          }
        }
      });
      </script>
    

</body></html>