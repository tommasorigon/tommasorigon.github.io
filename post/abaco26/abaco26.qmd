---
title: "ABACO26"
subtitle: "*Nonparametric predictive inference for discrete data via Metropolis-adjusted Dirichlet sequences*"
date: "2026-02-06"
description: "A joint work with [Davide Agnoletto]{.orange} and [David Dunson]{.blue} presented at the Workshop on *Advances in BAyesian COmputation and modeling organized* by Emanuele Aliverti at the University of Padova." 
lang: en
author: "[Tommaso Rigon]{.orange}"
institute: "_Università degli Studi di Milano-Bicocca_"
page-layout: full
bibliography: biblio.bib
citeproc: true
csl: https://www.zotero.org/styles/journal-of-the-american-statistical-association
reference-location: margin
execute:
  cache: false
format:
  html:
    toc: true
  revealjs:
    output-file: abaco26_slides.html
format-links: true
categories: talk
---

## Warm thanks

::: columns

::: {.column width="45%"}

[Davide Agnoletto]{.orange} (Duke University)

![](img/davide.jpg){fig-align="left"}
:::

::: {.column width="10%"}
:::

::: {.column width="45%"}
[David Dunson]{.orange} (Duke University)

![](img/david.jpeg){fig-align="left"}


:::


:::

## Foundations

- De Finetti’s representation Theorem [@de1937prevision] it provides the fundamental justification to the [two approaches]{.blue} to [Bayesian statistics]{.orange}: the [hypothetical]{.orange} approach and the [predictive]{.blue} approach.

::: {.callout-warning title="De Finetti's representation theorem"}
Let $(Y_n)_{n\ge 1}$, be a sequence of [exchangeable]{.blue} random variables. Then there exists a unique probability measure $\Pi$ such that, for any $n\ge 1$ and $A_1,\dots,A_n$
$$
\mathbb{P}(Y_1 \in A_1,\ldots,Y_n \in A_n) = \int_{\mathcal{P}} \prod_{i=1}^n p(A_i)\,\Pi(\mathrm{d}p).
$$
:::

- In a hierarchical formulation, we will say that $(Y_n)_{n \ge 1}$ is [exchangeable]{.bl} if and only if
$$
\begin{aligned}
Y_i \mid P &\overset{\textup{iid}}{\sim} P, \qquad i \ge 1, \\
P &\sim \Pi,
\end{aligned}
$$
where $P$ is a [random probability measure]{.blue} and $\Pi$ is the [prior law]{.orange}. 



## Hypothetical approach

- The hypothetical approach represents the the [most common]{.blue} way to operate within the Bayesian community.

- In a [parametric]{.orange} setting, $\Pi$ has support on a class $\Theta\subseteq\mathbb{R}^p$ with $p<\infty$, such that $\boldsymbol{\theta}\in\Theta$ indexes the class of distributions $\mathcal{P}_{\boldsymbol{\theta}}=\{P_{\boldsymbol{\theta}} : \boldsymbol{\theta} \in \Theta\subseteq\mathbb{R}^p\}$.

- Bayes' rule takes the well-known formulation:
$$
\pi(\boldsymbol{\theta}\mid y_{1:n}) \propto \pi(\boldsymbol{\theta}) \prod_{i=1}^n p_{\boldsymbol{\theta}}(y_i),
$$
where $\pi$ and $p_{\boldsymbol{\theta}}$ denote the probability density functions associated with $\Pi$ and $P_{\boldsymbol{\theta}}$, respectively.

- However, when the link between observations and parameter of interest cannot be easily expressed through a distribution function, the traditional hypothetical approach fails.

. . .

- Solution: [generalized posterior distributions]{.orange}, sometimes called [Gibbs-posteriors]{.blue}. 

- This is a [lively recent topic]{.blue}, see for instance: @chernozhukov2003mcmc; @bissiri2016general @heide2020safe; @grunwald2020fast; @knoblauch2022optimization; @matsubara2022robust; @matsubara2023generalized; @jewson_rossell_2022; @rigon2023generalized; @Agnoletto2025.


## Generalizations of the hypothetical approach

- @bissiri2016general showed that the [generalized posterior]{.blue} 
$$
\pi_\omega(\boldsymbol{\theta} \mid y_{1:n}) \propto \pi(\boldsymbol{\theta}) \exp\left\{ - \omega \sum_{i=1}^n \ell(\boldsymbol{\theta}; y_i) \right\},
$$
is the only coherent update of the prior beliefs about
$$
\boldsymbol{\theta}^* = \arg\min_{\boldsymbol{\theta}\in\Theta} \int_{\mathcal{Y}} \ell(\boldsymbol{\theta}; y)\, F_0(\mathrm{d}y),
$$
where $\ell(\boldsymbol{\theta}, y)$ is a [loss function]{.orange}, $\omega$ is the [loss-scale]{.grey}, and $F_0$ is the unknown true sampling distribution.

. . .

- Learning the loss scale $\omega$ from the data is a [delicate]{.orange} task. Assuming a prior for $\omega$ can lead to degenerate estimates if not accompanied by additional adjustments to the loss function.  

- However, there are several solutions for its calibration: @holmes2017assigning; @lyddon2019general; @syring2019calibrating; @matsubara2023generalized.

## Predictive approach

- Taking a [predictive approach]{.blue}, one can implicitly [characterize the prior]{.orange} via de Finetti theorem by specifying the sequence of predictive distributions:
$$
P_n(A) := \mathbb{P}(Y_{n+1}\in A\mid y_{1:n}), \qquad n \ge 1.
$$
This leads to an exchangeable sequences iff the conditions in @fortini2000exchangeability are satisfied. 

. . .

- [Example]{.orange}: the predictive construction of a Dirichlet process prior is such that $Y_1\sim P_0$ and $Y_{n+1}\mid y_{1:n}\sim P_n$ for $n\ge 1$, where
$$
P_n(A) = \frac{\alpha}{\alpha+n} P_0(A) + \frac{1}{\alpha+n}\sum_{i=1}^n\mathbb{1}(y_i\in A),
$$
for any measurable set $A$.

. . .

- The possibility of specifying a sequence of one-step-ahead predictive distributions is appealing:
  - $\rightarrow$ bypass direct elicitation of the prior;
  - $\rightarrow$ explicitly connects prediction and inference (wait for the next slide);

## Connecting inference and prediction I

- The posterior $P \mid y_{1:n}$ is usually obtained through Bayes theorem, but this is not the only way. 

- We can [characterize]{.blue} both [prior]{.blue} and [posterior]{.orange} of $P$ through the predictive distributions $P_n$, which indeed contains all the necessary information. 

. . .

- If $(Y_n)_{n \ge 1}$ is exchangeable, then the [prior]{.blue} and [posterior mean]{.blue} of $P$ coincide with the predictive:
$$
P_0(A) = \mathbb{P}(Y_1\in A) = \mathbb{E}\{P(A)\}, \qquad P_n(A) = \mathbb{P}(Y_{n+1}\in A\mid y_{1:n}) = \mathbb{E}(P(A) \mid y_{1:n}), \qquad n \ge 1.
$$

. . .


- A deeper result holds, which is a corollary of Finetti theorem [@fortini2012predictive]. 

::: {.callout-warning title="De Finetti's representation theorem (predictive form)"}
Let $(Y_n)_{n \ge 1}$ be an exchangeable sequence with predictive distributions $(P_n)_{n \ge 1}$. Then $P_n$ converges weakly (a.s. $\mathbb{P}$) to a random probability measure $P$ distributed according to $\Pi$ as $n \to \infty$.
:::


## Connecting inference and prediction II

- In other words, the sequence of predictive distributions $P_n$ converges to a random probability measure $P$ with [prior]{.orange} distribution $\Pi$. The [source of randomness]{.blue} are the data 
$$
Y_1, Y_2, Y_3, \dots
$$ 
This is intuitive: before observing the data, our predictions eventually reflect the prior. 

. . .

- Given $y_{1:n}$, the sequence $P_{n+m}$ converges weakly (a.s. $\mathbb{P}$) for $m \rightarrow \infty$ to a random probability measure with [posterior]{.orange} distribution $\Pi(\cdot\mid y_{1:n})$. The source of randomness are [future data]{.blue}
$$
Y_{n+1}, Y_{n+2}, Y_{n+3}, \dots.$$ 

- This gives a nice [alternative interpretation]{.blue} to the posterior distribution and a [practical algorithm]{.orange} for sampling from it called predictive resampling. 

- Intuitively, the posterior uncertainty comes from [lack of knowledge]{.orange} of future observations. If we knew them, the posterior would collapse to a point mass (Bayesian consistency). 

. . .

- This reasoning is at the hearth of [martingale posteriors]{.blue} [@fong_holmes_2023]. 

## Generalizations of the predictive approach

- However, for an arbitrary kernel, the sequence $(Y_n)_{n\ge1}$ generated by the sequence of predictive distributions is typically not exchangeable.

. . .

- Solution: replace exchangeability with the weaker requirement that $(Y_n)_{n\ge1}$ is [conditionally identically distributed]{.orange}.
The corresponding implied posterior distributions are known as [martingale posteriors]{.blue}.

. . .

- Very lively recent topic: @fortini_petrone_2020; @berti_pratelli_2021; @fortini2021predictive; @berti2023without; @berti2023kernel; @fong_holmes_2023; @fong2024asymptotics; @fong2024bayesian; @cui2024martingale; @fortini2024exchangeability and more...

- Our contribution: [Nonparametric predictive inference for discrete data via Metropolis-adjusted Dirichlet sequences]{.blue}.


## Motivation

- A better estimator for counts distributions would be obtained by [replacing $\mathbb{1}(\cdot)$ with a kernel]{.orange} that allows the borrowing of information between nearby values.


- Bayesian nonparametric modeling of [counts distributions]{.blue} is a challenging task.

. . .

- Estimating nonparametric mixtures of discrete kernels [@canale2011bayesian; @Canale2017] can be cumbersome in practice.

. . .

- Alternatively, one can directly specify a [DP prior]{.orange} on the data generator as
$$
    Y_i\mid P \sim P,\quad P\sim\mathrm{DP}(\alpha,P_0),
$$
for $Y_i\in\mathcal{Y}=\{0,1,\ldots\}$, $i=1,\ldots,n$, where $\alpha$ is the precision parameter and $P_0$ a base parametric distribution, such as a Poisson.

. . .

- However, the corresponding posterior [lacks smoothing]{.orange}, leading to poor performance.

. . .

- Within the hypothetical approach, it is not clear how to specify a nonparametric process having the same simplicity and flexibility as the DP prior while allowing smoothing.




## Metropolis-adjusted Dirichlet (MAD) sequences

- We assume that $Y_1\sim P_0$ and $Y_{n+1}\mid y_{1:n}\sim P_n$ for $n\ge 1$, with
$$
    P_n(\cdot) = \mathbb{P}\{Y_{n+1}\in \cdot\mid y_{1:n}\} = (1-w_n) P_{n-1}(\cdot) + w_n K_{n}(\cdot\mid y_n),
$$
where $(w_n)_{n\ge1}$ is a sequence of decreasing weights such that $w_n\in(0,1)$ for every $n\ge1$, $\sum_{n\ge1}w_n = \infty$, and $\sum_{n\ge1}w_n^2<\infty$.

. . .

- [$K_{n}(\cdot\mid y_n)$]{.blue} denotes a [Metropolis-Hastings kernel]{.blue} centered in $y_n$ having pmf:
$$
    k_{n}(y\mid y_n) = \gamma_{n}(y,y_n) k_*(y\mid y_n) + \mathbb{1}(y=y_n)\Big[\sum_{z\in\mathcal{Y}}\big\{1-\gamma_{n}(z,y_n)\big\}k_*(z\mid y_n)\Big],
$$
where
$$
    \gamma_{n}(y,y_n) = \gamma(y, y_n, P_{n-1}) =
    \min\left\{1,\frac{p_{n-1}(y) k_*(y_n\mid y)}{p_{n-1}(y_n) k_*(y\mid y_n)}\right\},
$$
$p_{n-1}$ is the probability mass functions associated to $P_{n-1}$ and [$k_*(\cdot\mid y_n)$]{.orange} is a [discrete base kernel]{.orange} centered in $y_n$.

. . .

- We refer to $P_n$ as the [Metropolis-adjusted Dirichlet]{.blue} (MAD) distribution with weights $(w_n)_{n\ge1}$, base kernel $k_*$ and initial distribution $P_0$.
The corresponding sequence $(Y_n)_{n\ge1}$ is a MAD sequence.

## MAD sequences are cid

- If $w_n = (\alpha +n)^{-1}$, then the MAD distribution becomes
$$
    P_n(\cdot) = \frac{\alpha}{\alpha + n}P_0(\cdot) + \frac{1}{\alpha + n}\sum_{i=1}^nK_i(\cdot\mid y_i).
$$

. . .

::: {.callout-warning title="Theorem"}
Let $(Y_n)_{n\ge1}$ be a MAD sequence. Then, for every set of weights $(w_n)_{n \ge 1}$, discrete base kernel $k_*$, and initial distribution $P_0$, the sequence $(Y_n)_{n\ge1}$ is [conditionally identically distributed]{.blue}.
:::

- A sequence $(Y_n)_{n\ge1}$ is conditionally identically distributed (cid) if, $\mathbb{P}$-a.s.,
$$
    \mathbb{P}\{Y_{n+k}\in\cdot\mid y_{1:n}\} = \mathbb{P}\{Y_{n+1}\in\cdot\mid y_{1:n}\}=P_n(\cdot)
    \qquad k\ge1.
$$

- This is equivalent to the martingale condition
$$
    \mathbb{E}\{P_{n+1}(\cdot)\mid y_{1:n}\} = P_{n}(\cdot).
$$

## Bayesian properties of cid sequences

::: {.callout-note title="Corollary (Aldous 1985; Berti et al. 2004)"}
Consider a MAD sequence $(Y_n)_{n\ge1}$. Then, $\mathbb{P}$-a.s.,

$(a)$ The sequence is asymptotically exchangeable, that is 
$$
    (Y_{n+1}, Y_{n+2}, \ldots) \overset{\textup{d}}{\longrightarrow} (Z_1, Z_2, \ldots), \qquad n \rightarrow \infty,
$$
where $(Z_1,Z_2,\ldots)$ is an exchangeable sequence with directing random probability measure $P$;

$(b)$ the corresponding sequences of MAD predictive distributions $(P_n)_{n\ge1}$ and empirical distributions $(\hat{P}_n)_{n\ge1}$, where $\hat{P}_n := n^{-1}\sum_{i=1}^n \delta_{Y_i}$, converge to $P$ weakly.
:::

- The ordering dependence will vanish asymptotically and, informally, $Y_i\mid P \overset{\mathrm{iid}}{\sim} P$ for large $n$.
- $P$ exists and is defined as the limit of the sequence of MAD distributions.

## Predictive inference

- Thus, an asymptotic equivalent of de Finetti’s theorem holds: each MAD sequence has a corresponding unique prior on $P$.

- The prior and posterior on $P$ [exists]{.orange} but are [not available explicitly]{.orange}.

. . .

::: {.callout-note title="Corollary"}
Let [$\theta = P(f) = \sum_{y \in \mathcal{Y}} f(y) p(y)$]{.blue} and analogously [$\theta_n = P_n(f) = \sum_{y \in \mathcal{Y}} f(y) p_n(y)$]{.blue} be any functional of interest.
Consider a MAD sequence $(Y_n)_{n\ge1}$. Then, $\mathbb{P}$-a.s., for every $n\ge1$ and every integrable function $f:\mathcal{Y}\rightarrow\mathbb{R}$, we have 
$$
\mathbb{E}(\theta \mid y_{1:n}) = \mathbb{E}\{P(f) \mid y_{1:n}\} = P_n(f) = \theta_n
$$
:::

- Moreover, we also have that $\theta = \mathbb{E}\{P(f)\}=P_0(f) = \theta_0$ for every integrable function $f$, so that [$P_0$]{.blue} retains the role of a [base measure]{.blue} as for standard Dirichlet sequences, providing an initial guess at $P$.

. . .

- Uncertainty quantification for $\theta=P(f)$ is carried out by [predictive resampling]{.blue} [@fong_holmes_2023].

## Illustrative example

![](img/pl_ill_slide.jpg)

## On the choice of the base kernel

- [Any discrete distribution can be chosen as base kernel $k_*$]{.orange}.

. . .

- To improve flexibility, we consider a [rounded Gaussian]{.blue} distribution centered in $y_n$, with pmf
$$
    k_*(y\mid y_n, \sigma) = \frac{\int_{y-1/2}^{y+1/2}\mathcal{N}(t\mid y_n, \sigma^2) \mathrm{d}t}{\sum_{z\in\mathcal{Y}} \int_{z-1/2}^{z+1/2}\mathcal{N}(t\mid y_n, \sigma^2) \mathrm{d}t},
$$
for $n\ge1$, where $\mathcal{N}(\cdot\mid y_n,\sigma^2)$ denotes a normal density function with mean $y_n$ and variance $\sigma^2$.

. . .

![](img/ker_plot_slide.jpg){fig-align="center" width="100%"}

## Role of the weights in controlling posterior variability

- The distribution of $P(A)\mid y_{1:n}$ is approximated by $\mathcal{N}(P_n(A), \Sigma_nr_n^{-1})$ for $n$ large, where
$$
    \Sigma_n r_n^{-1} 
    \overset{\cdot}{=} \mathbb{E}\{[P_{n+1}(A)-P_n(A)]^2\mid y_{1:n}\}\sum_{k>n+1}w_k^2.
$$

. . .

- Weights that [decay to zero quickly]{.blue} induce [fast learning and convergence]{.blue} to the asymptotic exchangeable regime.
- But [small values of $w_n$]{.orange} leads to [poor learning]{.orange} and [underestimation]{.orange} of the [posterior variability]{.orange}.

. . .

- Possible choices are $w_n=(\alpha+n)^{-1}$, $w_n=(\alpha+n)^{-2/3}$ [@martin2009asymptotic], and 
$w_n=(2 - n^{-1})(n+1)^{-1}$ [@fong_holmes_2023].

- We consider [adaptive weights]{.blue}
$$
    w_n=(\alpha+n)^{-\lambda_n}, \qquad \lambda_n=\lambda+(1+\lambda)\exp\bigg\{-\frac{1}{N_*}n\bigg\},
$$
with $\lambda\in(0.5,1]$, $N_*>0$.

## Multivariate count and binary data

- Extending MAD sequences for [multivariate]{.blue} data is straightforward using a [factorized base kernel]{.blue}
$$
k_*(\bm y\mid\bm y_n) = \prod_{j=1}^d k_*(y_j\mid y_{n,j}),
$$
with $\bm y=(y_1,\ldots,y_d)$ and $\bm y_n=(y_{n,1},\ldots,y_{n,d})$.

. . .

- MAD sequences can be employed for modeling [multivariate binary data]{.orange} using an appropriate base kernel.

. . .

- A natural step further is to use MAD sequences for [nonparametric regression and classification]{.orange}.

## Simulations I

Out-of-sample prediction accuracy evaluated in terms of MSE and AUC for regression and classification, respectively.

| | Regression (MSE) | | | Classification (AUC) | |
|---|---|---|---|---|---|
| | **$n=40$** | **$n=80$** | | **$n=150$** | **$n=300$** |
| GLM | 120.77 [51.51] | 94.93 [8.37] | | 0.796 [0.014] | 0.809 [0.007] |
| BART | 101.17 [12.69] | 74.17 [10.00] | | 0.863 [0.026] | 0.932 [0.009] |
| RF | 99.98 [7.45] | 87.75 [6.53] | | 0.882 [0.025] | 0.913 [0.015] |
| DP | 1450.21 [5.53] | 1395.61 [8.72] | | 0.644 [0.011] | 0.724 [0.012] |
| **MAD-1** | 91.07 [10.35] | 73.96 [7.60] | | 0.873 [0.014] | 0.899 [0.008] |
| **MAD-2/3** | 88.83 [13.00] | 73.18 [9.58] | | 0.869 [0.015] | 0.899 [0.009] |
| **MAD-dpm** | 87.41 [12.36] | 72.07 [9.48] | | 0.872 [0.014] | 0.901 [0.008] |
| **MAD-ada** | **90.61 [10.28]** | **73.45 [7.69]** | | **0.874 [0.014]** | **0.900 [0.008]** |

## Simulations II

![](img/pl_cov_sim.jpg){fig-align="center" width="85%"}

## Application

- We analyze the occurrence rates of 4 species corvids in Finland in year 2009 across different temperatures and habitats.

![](img/pl_appl_slide_1.jpg){fig-align="center" width="65%"}

## Application II

![](img/pl_appl_slide_2.jpg){fig-align="center" width="85%"}

## Thank you!

![](img/QR.png){width=3in fig-align="center"}

The [main paper]{.orange} is: 

Agnoletto, D., Rigon, T., and Dunson D.B. (2025+). Bayesian inference for generalized linear models via quasi-posteriors. *Biometrika*, to appear.

## References {.unnumbered .smaller}

::: {#refs}
:::


## Appendix



## Empirical consistency

![Illustrative example. Hellinger distance of MAD-ada (red), MAD-1 (blue), and DP (black) predictive distributions with respect to the density of the true data generator.](img/emp_cons.jpg){fig-align="center" width="85%"}

## Predictive resampling for MAD sequences

**Algorithm:**

1. Compute $P_n(A)$ from the observed data $y_{1:n}$
2. Set $N\gg n$
3. For $j = 1,\ldots,B$
    - For $i=n+1,\ldots,N$
        - Sample $Y_i\mid y_{1:i-1}\sim P_{i-1}$
        - Update $P_i(A) = (1-w_i)P_{i-1}(A) + w_i K_{i-1}(A\mid y_i)$
    - End For
4. End For
5. Return $\big\{P_N^{(1)}(A),\ldots,P_N^{(B)}(A)\big\}$, an i.i.d. sample from the distribution of $P_N(A)\mid y_{1:n}$

## On the choice of the base kernel

![](img/cor_var_plot.jpg){fig-align="center" width="85%"}

$$
\mathrm{var}\{P_{n+1}(y)\mid y_{1:n}\}, \text{ with } y\in\{20,40,60,80\}
$$
$$
\mathrm{cor}\{P_{n+1}(40),P_{n+1}(y)\mid y_{1:n}\}, \text{ with } y\in\{10, 41, 80\}
$$

## Asymptotic posterior distributions I

::: {.callout-warning title="Theorem"}
Let $(Y_n)_{n\ge1}$ be a MAD sequence and let $(r_n)_{n\ge1}$ be a monotone sequence of positive numbers such that $r_n\overset{\cdot}{=}(\sum_{k>n}w_k^2)^{-1}$ as $n\rightarrow\infty$.
Assume $\sqrt{r_n}\sup_{k\ge n}w_k\rightarrow 0$, $\sum_{k\ge1}r_k^2w_{k+1}^4<\infty$, and that, for every $H\ge1$ and all measurable sets $A_1,\ldots,A_H$, the random matrix $\bm\Sigma = \bm\Sigma(A_1,\ldots,A_H) = [\Sigma_{jt}]$ is positive definite.
Then, for every $H\ge1$ and every $A_1,\ldots,A_H$, 
$$
    \sqrt{r_n}\left[\begin{array}{c}
     P(A_1)-P_n(A_1)\\
     \cdots\\
     P(A_H)-P_n(A_H)
    \end{array}\right] \Big|\;
Y_{1:n}
    \overset{d}{\longrightarrow} \mathcal{N}_H(\bm0,\bm\Sigma)
$$
$\mathbb{P}$-a.s. for $n\rightarrow\infty$.
:::

## Asymptotic posterior distributions II

::: {.callout-warning title="Theorem"}
For every $n\ge1$ and all measurable sets $A_1,\ldots,A_H$, let $\bm\Sigma_n=\bm\Sigma_n(A_1,\ldots,A_H) = [\Sigma_{n,jt}]$.
Then, under the assumptions of previous Theorem, $\mathbb{P}$-a.s., $\bm\Sigma_n\rightarrow\bm\Sigma$ and
$$
    \sqrt{r_n}\bm\Sigma_n^{-1/2}\left[\begin{array}{c}
     P(A_1)-P_n(A_1)\\
     \cdots\\
     P(A_H)-P_n(A_H)
    \end{array}\right] \Big|\;
y_{1:n}
    \overset{d}{\longrightarrow} \mathcal{N}_H(\bm0,\bm I)
$$
for $n\rightarrow\infty$, where $\bm I$ denotes the identity matrix.
:::


